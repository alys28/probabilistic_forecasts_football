{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble training (1 year of testing, 2 years of validation, the rest is training)\n",
        "    # Learned weights on validation data (Constrained optimization)\n",
        "    # Meta-learning using an other ML model\n",
        "\n",
        "# Models used:\n",
        "# - XGBoost\n",
        "# - Neural Network\n",
        "# - Logistic Regression\n",
        "# - LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML\n"
          ]
        }
      ],
      "source": [
        "# Set up paths and load data\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "sys.path.append(parent_dir)\n",
        "print(parent_dir)\n",
        "\n",
        "interpolated_dir = os.path.join(parent_dir, \"dataset_interpolated_fixed\")\n",
        "features = [\"score_difference\", \"timestep\", \"type.id\", \"relative_strength\", \"home_has_possession\", \"end.down\", \"end.yardsToEndzone\", \"end.distance\", \"field_position_shift\", \"home_timeouts_left\", \"away_timeouts_left\"]\n",
        "\n",
        "# Import necessary modules\n",
        "from sklearn.metrics import brier_score_loss, accuracy_score, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.optimize import minimize\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for 2022\n",
            "skipping  2022\n",
            "Loading data for 2024\n",
            "skipping  2024\n",
            "Loading data for 2023\n",
            "skipping  2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "  Processing 254 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  Completed processing 2017\n",
            "Loading data for 2019\n",
            "  Processing 256 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  Completed processing 2019\n",
            "Loading data for 2021\n",
            "  Processing 272 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2021/game_401326405.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2021/game_401326412.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2021/game_401326412.csv\n",
            "  Completed processing 2021\n",
            "Loading data for 2020\n",
            "  Processing 255 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  Completed processing 2020\n",
            "Loading data for 2018\n",
            "  Processing 255 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  Completed processing 2018\n",
            "Loading data for 2016\n",
            "  Processing 254 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  Completed processing 2016\n",
            "Loading data for 2022\n",
            "  Processing 271 CSV files in parallel with 8 workers...\n",
            "  Completed processing 2022\n",
            "Loading data for 2024\n",
            "skipping  2024\n",
            "Loading data for 2023\n",
            "  Processing 272 CSV files in parallel with 8 workers...\n",
            "  Completed processing 2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "skipping  2017\n",
            "Loading data for 2019\n",
            "skipping  2019\n",
            "Loading data for 2021\n",
            "skipping  2021\n",
            "Loading data for 2020\n",
            "skipping  2020\n",
            "Loading data for 2018\n",
            "skipping  2018\n",
            "Loading data for 2016\n",
            "skipping  2016\n",
            "Loading data for 2022\n",
            "skipping  2022\n",
            "Loading data for 2024\n",
            "  Processing 272 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  Completed processing 2024\n",
            "Loading data for 2023\n",
            "skipping  2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "skipping  2017\n",
            "Loading data for 2019\n",
            "skipping  2019\n",
            "Loading data for 2021\n",
            "skipping  2021\n",
            "Loading data for 2020\n",
            "skipping  2020\n",
            "Loading data for 2018\n",
            "skipping  2018\n",
            "Loading data for 2016\n",
            "skipping  2016\n",
            "Loading data for 2022\n",
            "skipping  2022\n",
            "Loading data for 2024\n",
            "skipping  2024\n",
            "Loading data for 2023\n",
            "skipping  2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "  Processing 254 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2017/game_400951752.csv\n",
            "  Completed processing 2017\n",
            "Loading data for 2019\n",
            "  Processing 256 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127989.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2019/game_401127963.csv\n",
            "  Completed processing 2019\n",
            "Loading data for 2021\n",
            "skipping  2021\n",
            "Loading data for 2020\n",
            "  Processing 255 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220254.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2020/game_401220161.csv\n",
            "  Completed processing 2020\n",
            "Loading data for 2018\n",
            "  Processing 255 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030954.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030831.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030856.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2018/game_401030931.csv\n",
            "  Completed processing 2018\n",
            "Loading data for 2016\n",
            "  Processing 254 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2016/game_400874612.csv\n",
            "  Completed processing 2016\n",
            "Loading data for 2022\n",
            "  Processing 271 CSV files in parallel with 8 workers...\n",
            "  Completed processing 2022\n",
            "Loading data for 2024\n",
            "skipping  2024\n",
            "Loading data for 2023\n",
            "  Processing 272 CSV files in parallel with 8 workers...\n",
            "  Completed processing 2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "skipping  2017\n",
            "Loading data for 2019\n",
            "skipping  2019\n",
            "Loading data for 2021\n",
            "skipping  2021\n",
            "Loading data for 2020\n",
            "skipping  2020\n",
            "Loading data for 2018\n",
            "skipping  2018\n",
            "Loading data for 2016\n",
            "skipping  2016\n",
            "Loading data for 2022\n",
            "skipping  2022\n",
            "Loading data for 2024\n",
            "  Processing 272 CSV files in parallel with 8 workers...\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  NaN found in file: /Users/aly/Documents/University_of_Waterloo/Winter 2025/Research/code/NFL/ML/dataset_interpolated_fixed/2024/game_401671770.csv\n",
            "  Completed processing 2024\n",
            "Loading data for 2023\n",
            "skipping  2023\n",
            "Loading data for .DS_Store\n",
            "Loading data for 2017\n",
            "skipping  2017\n",
            "Loading data for 2019\n",
            "skipping  2019\n",
            "Loading data for 2021\n",
            "skipping  2021\n",
            "Loading data for 2020\n",
            "skipping  2020\n",
            "Loading data for 2018\n",
            "skipping  2018\n",
            "Loading data for 2016\n",
            "skipping  2016\n"
          ]
        }
      ],
      "source": [
        "# Load data for ensemble training\n",
        "import process_data\n",
        "training_data = process_data.load_data(interpolated_dir, \n",
        "                                       years = [2016, 2017, 2018, 2019, 2020, 2021], \n",
        "                                       history_length = 0, \n",
        "                                       features = features, \n",
        "                                       label_feature = \"home_win\")\n",
        "\n",
        "ensemble_data = process_data.load_data(interpolated_dir, \n",
        "                                         years = [2022, 2023], \n",
        "                                         history_length = 0, \n",
        "                                         features = features, \n",
        "                                         label_feature = \"home_win\",\n",
        "                                         train = True\n",
        "                                         )\n",
        "\n",
        "test_data = process_data.load_data(interpolated_dir, \n",
        "                                   years = [2024],\n",
        "                                   history_length = 0, \n",
        "                                   features = features, \n",
        "                                   label_feature = \"home_win\",\n",
        "                                   train = False\n",
        "                                   )\n",
        "\n",
        "training_data_seq = process_data.load_data(interpolated_dir, \n",
        "                                       years = [2016, 2017, 2018, 2019, 2020], \n",
        "                                       history_length = 4, \n",
        "                                       features = features, \n",
        "                                       label_feature = \"home_win\",\n",
        "                                       train = True\n",
        "                                       )\n",
        "\n",
        "ensemble_data_seq = process_data.load_data(interpolated_dir, \n",
        "                                         years = [2022, 2023], \n",
        "                                         history_length = 4, \n",
        "                                         features = features, \n",
        "                                         label_feature = \"home_win\",\n",
        "                                         train = True\n",
        "                                         )\n",
        "\n",
        "test_data_seq = process_data.load_data(interpolated_dir, \n",
        "                                   years = [2024],\n",
        "                                   history_length = 4, \n",
        "                                   features = features, \n",
        "                                   label_feature = \"home_win\",\n",
        "                                   train = False\n",
        "                                   )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "class EnsemblePredictor:\n",
        "    \"\"\"\n",
        "    Ensemble predictor class, one per timestep\n",
        "    \"\"\"\n",
        "    def __init__(self, all_models, all_models_order, strategy='meta_model', meta_model=None):\n",
        "        self.all_models = all_models\n",
        "        self.all_models_order = all_models_order\n",
        "        self.strategy = strategy\n",
        "        if self.strategy != 'meta_model' and self.strategy != 'weighted_average':\n",
        "            raise ValueError(\"Invalid strategy\")\n",
        "        if meta_model is None and self.strategy == 'meta_model':\n",
        "            raise ValueError(\"Meta model is required for meta_model strategy\")\n",
        "        self.meta_model = meta_model\n",
        "        self.ensemble_weights = None # Will be a 1D array of shape (n_models,) once trained\n",
        " \n",
        "    def train_ensemble(self, x_train, y_train, objective='brier'):\n",
        "        \"\"\"\n",
        "        Train the ensemble for a single timestep using validation data.\n",
        "        \"\"\"\n",
        "        print(f\"Training ensemble for this timestep...\")\n",
        "        if self.strategy == 'weighted_average':\n",
        "            self.optimize_ensemble_weights(x_train, y_train, objective)\n",
        "        elif self.strategy == 'meta_model':\n",
        "            self.train_meta_model(x_train, y_train)\n",
        "\n",
        "    def optimize_ensemble_weights(self, x_train, y_train, objective='brier'):\n",
        "        \"\"\"\n",
        "        Optimize ensemble weights for a single timestep using validation data.\n",
        "        \"\"\"\n",
        "        print(f\"Optimizing ensemble weights for this timestep...\")\n",
        "\n",
        "        n_models = x_train.shape[1]\n",
        "\n",
        "        def objective_function(weights):\n",
        "            weights = weights / np.sum(weights)  # Normalize weights\n",
        "            ensemble_preds = np.dot(x_train, weights)\n",
        "\n",
        "            if objective == 'brier':\n",
        "                return brier_score_loss(y_train, ensemble_preds)\n",
        "            elif objective == 'logloss':\n",
        "                # Clip predictions to avoid log(0)\n",
        "                ensemble_preds = np.clip(ensemble_preds, 1e-15, 1-1e-15)\n",
        "                return log_loss(y_train, ensemble_preds)\n",
        "            elif objective == 'accuracy':\n",
        "                return -accuracy_score(y_train, ensemble_preds > 0.5)  # Negative for minimization\n",
        "\n",
        "        # Constraints: weights sum to 1 and are non-negative\n",
        "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
        "        bounds = [(0, 1) for _ in range(n_models)]\n",
        "\n",
        "        # Initialize with equal weights\n",
        "        initial_weights = np.ones(n_models) / n_models\n",
        "\n",
        "        result = minimize(objective_function, initial_weights,\n",
        "                          method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "        if result.success:\n",
        "            self.ensemble_weights = result.x\n",
        "            print(f\"  Optimized weights: {dict(zip(self.all_models_order, result.x.round(4)))} (score: {result.fun:.6f})\")\n",
        "        else:\n",
        "            print(f\"  Optimization failed, using equal weights\")\n",
        "            self.ensemble_weights = initial_weights\n",
        "\n",
        "        return self.ensemble_weights\n",
        "\n",
        "    def train_meta_model(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Train a meta-model for a single timestep to predict based on base model outputs.\n",
        "        \"\"\"\n",
        "        X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "        # Train meta-model: input=base_model_predictions, output=final_prediction\n",
        "        self.meta_model.fit(X_train, y_train.reshape(-1, 1))\n",
        "\n",
        "        # Test the meta-model's prediction capability\n",
        "        meta_predictions = self.meta_model.predict_proba(X_val)[:, 1]\n",
        "        meta_accuracy = accuracy_score(y_val, meta_predictions > 0.5)\n",
        "        meta_brier = brier_score_loss(y_val, meta_predictions)\n",
        "\n",
        "        print(f\"  Meta-model trained on {len(X_train)} samples\")\n",
        "        print(f\"    Validation Meta-model accuracy: {meta_accuracy:.4f}, Validation Brier score: {meta_brier:.4f}\")\n",
        "\n",
        "        return self.meta_model\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of the positive class for a given input\n",
        "        \"\"\"\n",
        "        # Generate predictions from each individual model\n",
        "        # X is a 3D array of shape (n_samples, n_history, n_features)\n",
        "        predictions = [] # Will be a 2D array of shape (n_samples, n_models)\n",
        "        for i, model_name in enumerate(self.all_models_order):\n",
        "            model = self.all_models[model_name]\n",
        "            if model_name == \"lstm\":\n",
        "                predictions.append(model.predict_proba(X)[:, 1]) # Will be a 1D array of shape (n_samples,) for each model\n",
        "            else:\n",
        "                x = np.array([X[i][-1] for i in range(X.shape[0])])\n",
        "                predictions.append(model.predict_proba(x)[:, 1]) # Will be a 1D array of shape (n_samples,) for each model\n",
        "        predictions = np.array(predictions) # Will be a 2D array of shape (n_models, n_samples)\n",
        "        predictions = predictions.T # Reshape to be a 2D array of shape (n_samples, n_models)\n",
        "        if self.strategy == 'weighted_average':\n",
        "            return np.dot(predictions, self.ensemble_weights)\n",
        "        elif self.strategy == 'meta_model':\n",
        "            return self.meta_model.predict_proba(predictions)[:, 1]\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of the positive class for a given input\n",
        "        \"\"\"\n",
        "        pred = self.predict(X).flatten()\n",
        "        return np.column_stack([1 - pred, pred])\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Score the ensemble for a given input\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "        return np.mean(y_pred_labels == y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload modules\n",
        "modules_to_reload = [\n",
        "    'models.xg_boost',\n",
        "    'models.direct_prediction_network_lstm',\n",
        "    'models.direct_prediction_network',\n",
        "    'models.logistic_regression',\n",
        "]\n",
        "\n",
        "for module_name in modules_to_reload:\n",
        "    if module_name in sys.modules:\n",
        "        del sys.modules[module_name]\n",
        "from models.xg_boost import setup_xgboost_models\n",
        "from models.direct_prediction_network_lstm import setup_direct_lstm_models\n",
        "from models.direct_prediction_network import setup_direct_models\n",
        "from models.logistic_regression import setup_logistic_regression_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split training data: 2055 train, 229 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.0, 0.005]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (2055, 5, 11)\n",
            "Flattened training data shape: (2055, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 1/201 completed\n",
            "Split training data: 719 train, 80 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.005, 0.01]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (719, 5, 11)\n",
            "Flattened training data shape: (719, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.6871, Train Loss: 0.2074, Val Acc: 0.6250, Val Loss: 0.2084\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.208428\n",
            "NFL LSTM model 2/201 completed\n",
            "Split training data: 1364 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.01, 0.015]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1364, 5, 11)\n",
            "Flattened training data shape: (1364, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.6672, Train Loss: 0.2100, Val Acc: 0.7303, Val Loss: 0.1966\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.196598\n",
            "NFL LSTM model 3/201 completed\n",
            "Split training data: 1130 train, 126 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.015, 0.02]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1130, 5, 11)\n",
            "Flattened training data shape: (1130, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 4/201 completed\n",
            "Split training data: 1281 train, 143 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.02, 0.025]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1281, 5, 11)\n",
            "Flattened training data shape: (1281, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.6573, Train Loss: 0.2157, Val Acc: 0.7203, Val Loss: 0.1885\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.188535\n",
            "NFL LSTM model 5/201 completed\n",
            "Split training data: 1299 train, 145 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.025, 0.03]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1299, 5, 11)\n",
            "Flattened training data shape: (1299, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 9\n",
            "Best epoch: 4, Train Acc: 0.6220, Train Loss: 0.2264, Val Acc: 0.5931, Val Loss: 0.2317\n",
            "Restored LSTM model from best epoch 4 with val_loss: 0.231681\n",
            "NFL LSTM model 6/201 completed\n",
            "Split training data: 1261 train, 141 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.03, 0.035]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1261, 5, 11)\n",
            "Flattened training data shape: (1261, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.6701, Train Loss: 0.2153, Val Acc: 0.7234, Val Loss: 0.1965\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.196456\n",
            "NFL LSTM model 7/201 completed\n",
            "Split training data: 1348 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.035, 0.04]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1348, 5, 11)\n",
            "Flattened training data shape: (1348, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.6595, Train Loss: 0.2147, Val Acc: 0.6467, Val Loss: 0.2107\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.210665\n",
            "NFL LSTM model 8/201 completed\n",
            "Split training data: 1285 train, 143 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.04, 0.045]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1285, 5, 11)\n",
            "Flattened training data shape: (1285, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.6911, Train Loss: 0.2076, Val Acc: 0.7343, Val Loss: 0.1737\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.173708\n",
            "NFL LSTM model 9/201 completed\n",
            "Split training data: 1394 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.045, 0.05]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1394, 5, 11)\n",
            "Flattened training data shape: (1394, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.6514, Train Loss: 0.2178, Val Acc: 0.7226, Val Loss: 0.1947\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.194662\n",
            "NFL LSTM model 10/201 completed\n",
            "Split training data: 1338 train, 149 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.05, 0.055]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1338, 5, 11)\n",
            "Flattened training data shape: (1338, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.6861, Train Loss: 0.2055, Val Acc: 0.7047, Val Loss: 0.1981\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.198124\n",
            "NFL LSTM model 11/201 completed\n",
            "Split training data: 1293 train, 144 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.055, 0.06]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1293, 5, 11)\n",
            "Flattened training data shape: (1293, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 30\n",
            "Best epoch: 25, Train Acc: 0.6914, Train Loss: 0.2039, Val Acc: 0.6806, Val Loss: 0.2100\n",
            "Restored LSTM model from best epoch 25 with val_loss: 0.209988\n",
            "NFL LSTM model 12/201 completed\n",
            "Split training data: 1414 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.06, 0.065]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1414, 5, 11)\n",
            "Flattened training data shape: (1414, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.6818, Train Loss: 0.2171, Val Acc: 0.6962, Val Loss: 0.1987\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.198728\n",
            "NFL LSTM model 13/201 completed\n",
            "Split training data: 1353 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.065, 0.07]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1353, 5, 11)\n",
            "Flattened training data shape: (1353, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.6593, Train Loss: 0.2173, Val Acc: 0.6291, Val Loss: 0.2164\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.216380\n",
            "NFL LSTM model 14/201 completed\n",
            "Split training data: 1368 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.07, 0.075]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1368, 5, 11)\n",
            "Flattened training data shape: (1368, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.7025, Train Loss: 0.2052, Val Acc: 0.7303, Val Loss: 0.1899\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.189922\n",
            "NFL LSTM model 15/201 completed\n",
            "Split training data: 1340 train, 149 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.075, 0.08]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1340, 5, 11)\n",
            "Flattened training data shape: (1340, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 8\n",
            "Best epoch: 3, Train Acc: 0.6649, Train Loss: 0.2218, Val Acc: 0.6443, Val Loss: 0.2192\n",
            "Restored LSTM model from best epoch 3 with val_loss: 0.219224\n",
            "NFL LSTM model 16/201 completed\n",
            "Split training data: 1408 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.08, 0.085]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1408, 5, 11)\n",
            "Flattened training data shape: (1408, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 17/201 completed\n",
            "Split training data: 1410 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.085, 0.09]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1410, 5, 11)\n",
            "Flattened training data shape: (1410, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.6929, Train Loss: 0.1951, Val Acc: 0.7197, Val Loss: 0.1876\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.187588\n",
            "NFL LSTM model 18/201 completed\n",
            "Split training data: 1343 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.09, 0.095]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1343, 5, 11)\n",
            "Flattened training data shape: (1343, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 19/201 completed\n",
            "Split training data: 1388 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.095, 0.1]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1388, 5, 11)\n",
            "Flattened training data shape: (1388, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 30\n",
            "Best epoch: 25, Train Acc: 0.6909, Train Loss: 0.1995, Val Acc: 0.7097, Val Loss: 0.1924\n",
            "Restored LSTM model from best epoch 25 with val_loss: 0.192367\n",
            "NFL LSTM model 20/201 completed\n",
            "Split training data: 1391 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.1, 0.105]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1391, 5, 11)\n",
            "Flattened training data shape: (1391, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.7132, Train Loss: 0.1930, Val Acc: 0.6839, Val Loss: 0.2042\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.204238\n",
            "NFL LSTM model 21/201 completed\n",
            "Split training data: 1366 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.105, 0.11]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1366, 5, 11)\n",
            "Flattened training data shape: (1366, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 22/201 completed\n",
            "Split training data: 1399 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.11, 0.115]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1399, 5, 11)\n",
            "Flattened training data shape: (1399, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 16, Train Acc: 0.6948, Train Loss: 0.2011, Val Acc: 0.7179, Val Loss: 0.2064\n",
            "Restored LSTM model from best epoch 16 with val_loss: 0.206393\n",
            "NFL LSTM model 23/201 completed\n",
            "Split training data: 1330 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.115, 0.12]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1330, 5, 11)\n",
            "Flattened training data shape: (1330, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 19, Train Acc: 0.7226, Train Loss: 0.1876, Val Acc: 0.6959, Val Loss: 0.2252\n",
            "Restored LSTM model from best epoch 19 with val_loss: 0.225210\n",
            "NFL LSTM model 24/201 completed\n",
            "Split training data: 1414 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.12, 0.125]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1414, 5, 11)\n",
            "Flattened training data shape: (1414, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.7065, Train Loss: 0.1978, Val Acc: 0.6519, Val Loss: 0.2131\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.213147\n",
            "NFL LSTM model 25/201 completed\n",
            "Split training data: 1393 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.125, 0.13]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1393, 5, 11)\n",
            "Flattened training data shape: (1393, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 26/201 completed\n",
            "Split training data: 1406 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.13, 0.135]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1406, 5, 11)\n",
            "Flattened training data shape: (1406, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.6984, Train Loss: 0.1936, Val Acc: 0.7070, Val Loss: 0.1951\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.195121\n",
            "NFL LSTM model 27/201 completed\n",
            "Split training data: 1379 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.135, 0.14]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1379, 5, 11)\n",
            "Flattened training data shape: (1379, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 29\n",
            "Best epoch: 24, Train Acc: 0.7288, Train Loss: 0.1836, Val Acc: 0.7208, Val Loss: 0.1913\n",
            "Restored LSTM model from best epoch 24 with val_loss: 0.191308\n",
            "NFL LSTM model 28/201 completed\n",
            "Split training data: 1329 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.14, 0.145]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1329, 5, 11)\n",
            "Flattened training data shape: (1329, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.6930, Train Loss: 0.2072, Val Acc: 0.7027, Val Loss: 0.2140\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.214037\n",
            "NFL LSTM model 29/201 completed\n",
            "Split training data: 1377 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.145, 0.15]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1377, 5, 11)\n",
            "Flattened training data shape: (1377, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 30/201 completed\n",
            "Split training data: 1392 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.15, 0.155]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1392, 5, 11)\n",
            "Flattened training data shape: (1392, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 31/201 completed\n",
            "Split training data: 1305 train, 146 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.155, 0.16]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1305, 5, 11)\n",
            "Flattened training data shape: (1305, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.7234, Train Loss: 0.1958, Val Acc: 0.7055, Val Loss: 0.2035\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.203493\n",
            "NFL LSTM model 32/201 completed\n",
            "Split training data: 1435 train, 160 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.16, 0.165]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1435, 5, 11)\n",
            "Flattened training data shape: (1435, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 33/201 completed\n",
            "Split training data: 1342 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.165, 0.17]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1342, 5, 11)\n",
            "Flattened training data shape: (1342, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 30\n",
            "Best epoch: 25, Train Acc: 0.7288, Train Loss: 0.1808, Val Acc: 0.6733, Val Loss: 0.2191\n",
            "Restored LSTM model from best epoch 25 with val_loss: 0.219098\n",
            "NFL LSTM model 34/201 completed\n",
            "Split training data: 1382 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.17, 0.175]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1382, 5, 11)\n",
            "Flattened training data shape: (1382, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7308, Train Loss: 0.1922, Val Acc: 0.7143, Val Loss: 0.2042\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.204249\n",
            "NFL LSTM model 35/201 completed\n",
            "Split training data: 1413 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.175, 0.18]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1413, 5, 11)\n",
            "Flattened training data shape: (1413, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.7162, Train Loss: 0.1992, Val Acc: 0.6879, Val Loss: 0.2042\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.204247\n",
            "NFL LSTM model 36/201 completed\n",
            "Split training data: 1339 train, 149 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.18, 0.185]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1339, 5, 11)\n",
            "Flattened training data shape: (1339, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.7259, Train Loss: 0.1851, Val Acc: 0.7047, Val Loss: 0.1926\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.192582\n",
            "NFL LSTM model 37/201 completed\n",
            "Split training data: 1363 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.185, 0.19]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1363, 5, 11)\n",
            "Flattened training data shape: (1363, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.6985, Train Loss: 0.1980, Val Acc: 0.7171, Val Loss: 0.1960\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.196023\n",
            "NFL LSTM model 38/201 completed\n",
            "Split training data: 1357 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.19, 0.195]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1357, 5, 11)\n",
            "Flattened training data shape: (1357, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 39/201 completed\n",
            "Split training data: 1352 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.195, 0.2]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1352, 5, 11)\n",
            "Flattened training data shape: (1352, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 40/201 completed\n",
            "Split training data: 1350 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.2, 0.205]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1350, 5, 11)\n",
            "Flattened training data shape: (1350, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.6985, Train Loss: 0.1943, Val Acc: 0.7467, Val Loss: 0.1668\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.166826\n",
            "NFL LSTM model 41/201 completed\n",
            "Split training data: 1361 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.205, 0.21]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1361, 5, 11)\n",
            "Flattened training data shape: (1361, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7223, Train Loss: 0.1889, Val Acc: 0.6974, Val Loss: 0.1877\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.187669\n",
            "NFL LSTM model 42/201 completed\n",
            "Split training data: 1443 train, 161 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.21, 0.215]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1443, 5, 11)\n",
            "Flattened training data shape: (1443, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.6764, Train Loss: 0.2060, Val Acc: 0.7267, Val Loss: 0.2089\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.208853\n",
            "NFL LSTM model 43/201 completed\n",
            "Split training data: 1304 train, 145 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.215, 0.22]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1304, 5, 11)\n",
            "Flattened training data shape: (1304, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 44/201 completed\n",
            "Split training data: 1383 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.22, 0.225]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1383, 5, 11)\n",
            "Flattened training data shape: (1383, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 20, Train Acc: 0.7411, Train Loss: 0.1767, Val Acc: 0.7532, Val Loss: 0.1688\n",
            "Restored LSTM model from best epoch 20 with val_loss: 0.168776\n",
            "NFL LSTM model 45/201 completed\n",
            "Split training data: 1414 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.225, 0.23]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1414, 5, 11)\n",
            "Flattened training data shape: (1414, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 46/201 completed\n",
            "Split training data: 1332 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.23, 0.235]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1332, 5, 11)\n",
            "Flattened training data shape: (1332, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 29\n",
            "Best epoch: 24, Train Acc: 0.7568, Train Loss: 0.1673, Val Acc: 0.6824, Val Loss: 0.1964\n",
            "Restored LSTM model from best epoch 24 with val_loss: 0.196381\n",
            "NFL LSTM model 47/201 completed\n",
            "Split training data: 1418 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.235, 0.24]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1418, 5, 11)\n",
            "Flattened training data shape: (1418, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.7285, Train Loss: 0.1897, Val Acc: 0.7025, Val Loss: 0.2063\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.206294\n",
            "NFL LSTM model 48/201 completed\n",
            "Split training data: 1341 train, 149 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.24, 0.245]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1341, 5, 11)\n",
            "Flattened training data shape: (1341, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.7159, Train Loss: 0.1913, Val Acc: 0.6846, Val Loss: 0.1892\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.189192\n",
            "NFL LSTM model 49/201 completed\n",
            "Split training data: 1261 train, 141 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.245, 0.25]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1261, 5, 11)\n",
            "Flattened training data shape: (1261, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 22, Train Acc: 0.7431, Train Loss: 0.1800, Val Acc: 0.7518, Val Loss: 0.1719\n",
            "Restored LSTM model from best epoch 22 with val_loss: 0.171889\n",
            "NFL LSTM model 50/201 completed\n",
            "Split training data: 4087 train, 455 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.25, 0.255]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (4087, 5, 11)\n",
            "Flattened training data shape: (4087, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 20, Train Acc: 0.7497, Train Loss: 0.1694, Val Acc: 0.7429, Val Loss: 0.1734\n",
            "Restored LSTM model from best epoch 20 with val_loss: 0.173356\n",
            "NFL LSTM model 51/201 completed\n",
            "Split training data: 825 train, 92 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.255, 0.26]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (825, 5, 11)\n",
            "Flattened training data shape: (825, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 52/201 completed\n",
            "Split training data: 1090 train, 122 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.26, 0.265]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1090, 5, 11)\n",
            "Flattened training data shape: (1090, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7165, Train Loss: 0.1882, Val Acc: 0.7131, Val Loss: 0.1819\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.181862\n",
            "NFL LSTM model 53/201 completed\n",
            "Split training data: 1440 train, 160 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.265, 0.27]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1440, 5, 11)\n",
            "Flattened training data shape: (1440, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.7458, Train Loss: 0.1754, Val Acc: 0.7625, Val Loss: 0.1719\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.171866\n",
            "NFL LSTM model 54/201 completed\n",
            "Split training data: 1284 train, 143 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.27, 0.275]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1284, 5, 11)\n",
            "Flattened training data shape: (1284, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.7313, Train Loss: 0.1807, Val Acc: 0.7483, Val Loss: 0.1762\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.176172\n",
            "NFL LSTM model 55/201 completed\n",
            "Split training data: 1453 train, 162 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.275, 0.28]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1453, 5, 11)\n",
            "Flattened training data shape: (1453, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.7206, Train Loss: 0.1847, Val Acc: 0.7346, Val Loss: 0.1584\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.158375\n",
            "NFL LSTM model 56/201 completed\n",
            "Split training data: 1255 train, 140 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.28, 0.285]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1255, 5, 11)\n",
            "Flattened training data shape: (1255, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.7036, Train Loss: 0.1949, Val Acc: 0.7857, Val Loss: 0.1903\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.190283\n",
            "NFL LSTM model 57/201 completed\n",
            "Split training data: 1475 train, 164 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.285, 0.29]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1475, 5, 11)\n",
            "Flattened training data shape: (1475, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7525, Train Loss: 0.1685, Val Acc: 0.7134, Val Loss: 0.1852\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.185175\n",
            "NFL LSTM model 58/201 completed\n",
            "Split training data: 1329 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.29, 0.295]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1329, 5, 11)\n",
            "Flattened training data shape: (1329, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7329, Train Loss: 0.1856, Val Acc: 0.7162, Val Loss: 0.1771\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.177110\n",
            "NFL LSTM model 59/201 completed\n",
            "Split training data: 1377 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.295, 0.3]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1377, 5, 11)\n",
            "Flattened training data shape: (1377, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 9\n",
            "Best epoch: 4, Train Acc: 0.7081, Train Loss: 0.1945, Val Acc: 0.7078, Val Loss: 0.1868\n",
            "Restored LSTM model from best epoch 4 with val_loss: 0.186784\n",
            "NFL LSTM model 60/201 completed\n",
            "Split training data: 1426 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.3, 0.305]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1426, 5, 11)\n",
            "Flattened training data shape: (1426, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 61/201 completed\n",
            "Split training data: 1318 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.305, 0.31]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1318, 5, 11)\n",
            "Flattened training data shape: (1318, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 62/201 completed\n",
            "Split training data: 1430 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.31, 0.315]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1430, 5, 11)\n",
            "Flattened training data shape: (1430, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 63/201 completed\n",
            "Split training data: 1358 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.315, 0.32]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1358, 5, 11)\n",
            "Flattened training data shape: (1358, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 22, Train Acc: 0.7342, Train Loss: 0.1758, Val Acc: 0.7285, Val Loss: 0.1803\n",
            "Restored LSTM model from best epoch 22 with val_loss: 0.180324\n",
            "NFL LSTM model 64/201 completed\n",
            "Split training data: 1394 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.32, 0.325]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1394, 5, 11)\n",
            "Flattened training data shape: (1394, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 65/201 completed\n",
            "Split training data: 1326 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.325, 0.33]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1326, 5, 11)\n",
            "Flattened training data shape: (1326, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 66/201 completed\n",
            "Split training data: 1387 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.33, 0.335]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1387, 5, 11)\n",
            "Flattened training data shape: (1387, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.7520, Train Loss: 0.1734, Val Acc: 0.7548, Val Loss: 0.1607\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.160744\n",
            "NFL LSTM model 67/201 completed\n",
            "Split training data: 1407 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.335, 0.34]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1407, 5, 11)\n",
            "Flattened training data shape: (1407, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 68/201 completed\n",
            "Split training data: 1371 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.34, 0.345]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1371, 5, 11)\n",
            "Flattened training data shape: (1371, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 69/201 completed\n",
            "Split training data: 1444 train, 161 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.345, 0.35]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1444, 5, 11)\n",
            "Flattened training data shape: (1444, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 70/201 completed\n",
            "Split training data: 1404 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.35, 0.355]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1404, 5, 11)\n",
            "Flattened training data shape: (1404, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 71/201 completed\n",
            "Split training data: 1373 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.355, 0.36]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1373, 5, 11)\n",
            "Flattened training data shape: (1373, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.7429, Train Loss: 0.1749, Val Acc: 0.7712, Val Loss: 0.1576\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.157623\n",
            "NFL LSTM model 72/201 completed\n",
            "Split training data: 1359 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.36, 0.365]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1359, 5, 11)\n",
            "Flattened training data shape: (1359, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 18, Train Acc: 0.7483, Train Loss: 0.1692, Val Acc: 0.7351, Val Loss: 0.1752\n",
            "Restored LSTM model from best epoch 18 with val_loss: 0.175236\n",
            "NFL LSTM model 73/201 completed\n",
            "Split training data: 1397 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.365, 0.37]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1397, 5, 11)\n",
            "Flattened training data shape: (1397, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 74/201 completed\n",
            "Split training data: 1359 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.37, 0.375]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1359, 5, 11)\n",
            "Flattened training data shape: (1359, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.7167, Train Loss: 0.1826, Val Acc: 0.7285, Val Loss: 0.1924\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.192353\n",
            "NFL LSTM model 75/201 completed\n",
            "Split training data: 1389 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.375, 0.38]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1389, 5, 11)\n",
            "Flattened training data shape: (1389, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.7466, Train Loss: 0.1709, Val Acc: 0.7548, Val Loss: 0.1713\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.171292\n",
            "NFL LSTM model 76/201 completed\n",
            "Split training data: 1366 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.38, 0.385]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1366, 5, 11)\n",
            "Flattened training data shape: (1366, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 77/201 completed\n",
            "Split training data: 1390 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.385, 0.39]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1390, 5, 11)\n",
            "Flattened training data shape: (1390, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 20, Train Acc: 0.7784, Train Loss: 0.1543, Val Acc: 0.7548, Val Loss: 0.1637\n",
            "Restored LSTM model from best epoch 20 with val_loss: 0.163711\n",
            "NFL LSTM model 78/201 completed\n",
            "Split training data: 1369 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.39, 0.395]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1369, 5, 11)\n",
            "Flattened training data shape: (1369, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 79/201 completed\n",
            "Split training data: 1404 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.395, 0.4]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1404, 5, 11)\n",
            "Flattened training data shape: (1404, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 19, Train Acc: 0.7657, Train Loss: 0.1632, Val Acc: 0.7516, Val Loss: 0.1609\n",
            "Restored LSTM model from best epoch 19 with val_loss: 0.160944\n",
            "NFL LSTM model 80/201 completed\n",
            "Split training data: 1380 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.4, 0.405]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1380, 5, 11)\n",
            "Flattened training data shape: (1380, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 18, Train Acc: 0.7486, Train Loss: 0.1700, Val Acc: 0.7597, Val Loss: 0.1642\n",
            "Restored LSTM model from best epoch 18 with val_loss: 0.164188\n",
            "NFL LSTM model 81/201 completed\n",
            "Split training data: 1316 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.405, 0.41]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1316, 5, 11)\n",
            "Flattened training data shape: (1316, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.7637, Train Loss: 0.1611, Val Acc: 0.7959, Val Loss: 0.1474\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.147358\n",
            "NFL LSTM model 82/201 completed\n",
            "Split training data: 1414 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.41, 0.415]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1414, 5, 11)\n",
            "Flattened training data shape: (1414, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.7666, Train Loss: 0.1664, Val Acc: 0.7405, Val Loss: 0.1712\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.171206\n",
            "NFL LSTM model 83/201 completed\n",
            "Split training data: 1406 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.415, 0.42]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1406, 5, 11)\n",
            "Flattened training data shape: (1406, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.7760, Train Loss: 0.1557, Val Acc: 0.7325, Val Loss: 0.1870\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.186963\n",
            "NFL LSTM model 84/201 completed\n",
            "Split training data: 1429 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.42, 0.425]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1429, 5, 11)\n",
            "Flattened training data shape: (1429, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.7481, Train Loss: 0.1749, Val Acc: 0.7484, Val Loss: 0.1623\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.162348\n",
            "NFL LSTM model 85/201 completed\n",
            "Split training data: 1391 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.425, 0.43]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1391, 5, 11)\n",
            "Flattened training data shape: (1391, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.7815, Train Loss: 0.1510, Val Acc: 0.7226, Val Loss: 0.1674\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.167406\n",
            "NFL LSTM model 86/201 completed\n",
            "Split training data: 1369 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.43, 0.435]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1369, 5, 11)\n",
            "Flattened training data shape: (1369, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 87/201 completed\n",
            "Split training data: 1430 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.435, 0.44]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1430, 5, 11)\n",
            "Flattened training data shape: (1430, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 30\n",
            "Best epoch: 25, Train Acc: 0.7923, Train Loss: 0.1467, Val Acc: 0.7862, Val Loss: 0.1421\n",
            "Restored LSTM model from best epoch 25 with val_loss: 0.142067\n",
            "NFL LSTM model 88/201 completed\n",
            "Split training data: 1352 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.44, 0.445]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1352, 5, 11)\n",
            "Flattened training data shape: (1352, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.7507, Train Loss: 0.1627, Val Acc: 0.7616, Val Loss: 0.1501\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.150129\n",
            "NFL LSTM model 89/201 completed\n",
            "Split training data: 1393 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.445, 0.45]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1393, 5, 11)\n",
            "Flattened training data shape: (1393, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.7789, Train Loss: 0.1575, Val Acc: 0.8000, Val Loss: 0.1446\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.144625\n",
            "NFL LSTM model 90/201 completed\n",
            "Split training data: 1403 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.45, 0.455]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1403, 5, 11)\n",
            "Flattened training data shape: (1403, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.7648, Train Loss: 0.1616, Val Acc: 0.8077, Val Loss: 0.1425\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.142483\n",
            "NFL LSTM model 91/201 completed\n",
            "Split training data: 1383 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.455, 0.46]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1383, 5, 11)\n",
            "Flattened training data shape: (1383, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 92/201 completed\n",
            "Split training data: 1458 train, 162 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.46, 0.465]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1458, 5, 11)\n",
            "Flattened training data shape: (1458, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.7586, Train Loss: 0.1650, Val Acc: 0.8025, Val Loss: 0.1497\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.149697\n",
            "NFL LSTM model 93/201 completed\n",
            "Split training data: 1329 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.465, 0.47]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1329, 5, 11)\n",
            "Flattened training data shape: (1329, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.7713, Train Loss: 0.1631, Val Acc: 0.8108, Val Loss: 0.1451\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.145063\n",
            "NFL LSTM model 94/201 completed\n",
            "Split training data: 3807 train, 424 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.47, 0.475]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (3807, 5, 11)\n",
            "Flattened training data shape: (3807, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 95/201 completed\n",
            "Split training data: 1929 train, 215 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.475, 0.48]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1929, 5, 11)\n",
            "Flattened training data shape: (1929, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 96/201 completed\n",
            "Split training data: 2111 train, 235 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.48, 0.485]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (2111, 5, 11)\n",
            "Flattened training data shape: (2111, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.7968, Train Loss: 0.1474, Val Acc: 0.8085, Val Loss: 0.1404\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.140376\n",
            "NFL LSTM model 97/201 completed\n",
            "Split training data: 2361 train, 263 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.485, 0.49]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (2361, 5, 11)\n",
            "Flattened training data shape: (2361, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 98/201 completed\n",
            "Split training data: 2878 train, 320 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.49, 0.495]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (2878, 5, 11)\n",
            "Flattened training data shape: (2878, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 99/201 completed\n",
            "Split training data: 3364 train, 374 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.495, 0.5]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (3364, 5, 11)\n",
            "Flattened training data shape: (3364, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8089, Train Loss: 0.1421, Val Acc: 0.7995, Val Loss: 0.1349\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.134894\n",
            "NFL LSTM model 100/201 completed\n",
            "Split training data: 6236 train, 693 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.5, 0.505]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (6236, 5, 11)\n",
            "Flattened training data shape: (6236, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 101/201 completed\n",
            "Split training data: 806 train, 90 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.505, 0.51]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (806, 5, 11)\n",
            "Flattened training data shape: (806, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.7940, Train Loss: 0.1475, Val Acc: 0.7778, Val Loss: 0.1606\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.160605\n",
            "NFL LSTM model 102/201 completed\n",
            "Split training data: 1320 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.51, 0.515]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1320, 5, 11)\n",
            "Flattened training data shape: (1320, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.8000, Train Loss: 0.1455, Val Acc: 0.8367, Val Loss: 0.1255\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.125526\n",
            "NFL LSTM model 103/201 completed\n",
            "Split training data: 1143 train, 127 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.515, 0.52]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1143, 5, 11)\n",
            "Flattened training data shape: (1143, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 104/201 completed\n",
            "Split training data: 1282 train, 143 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.52, 0.525]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1282, 5, 11)\n",
            "Flattened training data shape: (1282, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.8003, Train Loss: 0.1397, Val Acc: 0.8252, Val Loss: 0.1299\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.129865\n",
            "NFL LSTM model 105/201 completed\n",
            "Split training data: 1367 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.525, 0.53]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1367, 5, 11)\n",
            "Flattened training data shape: (1367, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8083, Train Loss: 0.1404, Val Acc: 0.7895, Val Loss: 0.1447\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.144726\n",
            "NFL LSTM model 106/201 completed\n",
            "Split training data: 1316 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.53, 0.535]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1316, 5, 11)\n",
            "Flattened training data shape: (1316, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 107/201 completed\n",
            "Split training data: 1368 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.535, 0.54]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1368, 5, 11)\n",
            "Flattened training data shape: (1368, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.8275, Train Loss: 0.1286, Val Acc: 0.8235, Val Loss: 0.1219\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.121934\n",
            "NFL LSTM model 108/201 completed\n",
            "Split training data: 1317 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.54, 0.545]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1317, 5, 11)\n",
            "Flattened training data shape: (1317, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.7798, Train Loss: 0.1561, Val Acc: 0.7211, Val Loss: 0.1789\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.178923\n",
            "NFL LSTM model 109/201 completed\n",
            "Split training data: 1316 train, 147 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.545, 0.55]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1316, 5, 11)\n",
            "Flattened training data shape: (1316, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8176, Train Loss: 0.1303, Val Acc: 0.7959, Val Loss: 0.1459\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.145879\n",
            "NFL LSTM model 110/201 completed\n",
            "Split training data: 1356 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.55, 0.555]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1356, 5, 11)\n",
            "Flattened training data shape: (1356, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.8156, Train Loss: 0.1340, Val Acc: 0.8543, Val Loss: 0.1175\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.117454\n",
            "NFL LSTM model 111/201 completed\n",
            "Split training data: 1332 train, 149 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.555, 0.56]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1332, 5, 11)\n",
            "Flattened training data shape: (1332, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8116, Train Loss: 0.1472, Val Acc: 0.7584, Val Loss: 0.1661\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.166083\n",
            "NFL LSTM model 112/201 completed\n",
            "Split training data: 1371 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.56, 0.565]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1371, 5, 11)\n",
            "Flattened training data shape: (1371, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.8074, Train Loss: 0.1381, Val Acc: 0.8562, Val Loss: 0.1198\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.119790\n",
            "NFL LSTM model 113/201 completed\n",
            "Split training data: 1371 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.565, 0.57]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1371, 5, 11)\n",
            "Flattened training data shape: (1371, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.8016, Train Loss: 0.1378, Val Acc: 0.7778, Val Loss: 0.1436\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.143630\n",
            "NFL LSTM model 114/201 completed\n",
            "Split training data: 1386 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.57, 0.575]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1386, 5, 11)\n",
            "Flattened training data shape: (1386, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.8038, Train Loss: 0.1456, Val Acc: 0.7727, Val Loss: 0.1647\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.164672\n",
            "NFL LSTM model 115/201 completed\n",
            "Split training data: 1421 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.575, 0.58]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1421, 5, 11)\n",
            "Flattened training data shape: (1421, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.8191, Train Loss: 0.1323, Val Acc: 0.8038, Val Loss: 0.1345\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.134477\n",
            "NFL LSTM model 116/201 completed\n",
            "Split training data: 1347 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.58, 0.585]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1347, 5, 11)\n",
            "Flattened training data shape: (1347, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 117/201 completed\n",
            "Split training data: 1359 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.585, 0.59]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1359, 5, 11)\n",
            "Flattened training data shape: (1359, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.8094, Train Loss: 0.1388, Val Acc: 0.7616, Val Loss: 0.1610\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.160983\n",
            "NFL LSTM model 118/201 completed\n",
            "Split training data: 1441 train, 161 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.59, 0.595]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1441, 5, 11)\n",
            "Flattened training data shape: (1441, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 16, Train Acc: 0.8189, Train Loss: 0.1351, Val Acc: 0.8137, Val Loss: 0.1188\n",
            "Restored LSTM model from best epoch 16 with val_loss: 0.118831\n",
            "NFL LSTM model 119/201 completed\n",
            "Split training data: 1371 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.595, 0.6]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1371, 5, 11)\n",
            "Flattened training data shape: (1371, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8133, Train Loss: 0.1338, Val Acc: 0.7516, Val Loss: 0.1694\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.169433\n",
            "NFL LSTM model 120/201 completed\n",
            "Split training data: 1384 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.6, 0.605]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1384, 5, 11)\n",
            "Flattened training data shape: (1384, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.8092, Train Loss: 0.1320, Val Acc: 0.7987, Val Loss: 0.1362\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.136173\n",
            "NFL LSTM model 121/201 completed\n",
            "Split training data: 1398 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.605, 0.61]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1398, 5, 11)\n",
            "Flattened training data shape: (1398, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8205, Train Loss: 0.1336, Val Acc: 0.8782, Val Loss: 0.1089\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.108852\n",
            "NFL LSTM model 122/201 completed\n",
            "Split training data: 1437 train, 160 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.61, 0.615]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1437, 5, 11)\n",
            "Flattened training data shape: (1437, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.8191, Train Loss: 0.1422, Val Acc: 0.8000, Val Loss: 0.1421\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.142085\n",
            "NFL LSTM model 123/201 completed\n",
            "Split training data: 1392 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.615, 0.62]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1392, 5, 11)\n",
            "Flattened training data shape: (1392, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8197, Train Loss: 0.1287, Val Acc: 0.8065, Val Loss: 0.1403\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.140266\n",
            "NFL LSTM model 124/201 completed\n",
            "Split training data: 1360 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.62, 0.625]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1360, 5, 11)\n",
            "Flattened training data shape: (1360, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 125/201 completed\n",
            "Split training data: 1455 train, 162 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.625, 0.63]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1455, 5, 11)\n",
            "Flattened training data shape: (1455, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 126/201 completed\n",
            "Split training data: 1372 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.63, 0.635]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1372, 5, 11)\n",
            "Flattened training data shape: (1372, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8222, Train Loss: 0.1333, Val Acc: 0.8235, Val Loss: 0.1209\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.120921\n",
            "NFL LSTM model 127/201 completed\n",
            "Split training data: 1395 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.635, 0.64]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1395, 5, 11)\n",
            "Flattened training data shape: (1395, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8014, Train Loss: 0.1412, Val Acc: 0.8065, Val Loss: 0.1381\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.138148\n",
            "NFL LSTM model 128/201 completed\n",
            "Split training data: 1304 train, 145 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.64, 0.645]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1304, 5, 11)\n",
            "Flattened training data shape: (1304, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.8275, Train Loss: 0.1317, Val Acc: 0.8483, Val Loss: 0.1088\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.108816\n",
            "NFL LSTM model 129/201 completed\n",
            "Split training data: 1423 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.645, 0.65]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1423, 5, 11)\n",
            "Flattened training data shape: (1423, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.8152, Train Loss: 0.1340, Val Acc: 0.8176, Val Loss: 0.1297\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.129697\n",
            "NFL LSTM model 130/201 completed\n",
            "Split training data: 1384 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.65, 0.655]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1384, 5, 11)\n",
            "Flattened training data shape: (1384, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 9, Train Acc: 0.7991, Train Loss: 0.1430, Val Acc: 0.8052, Val Loss: 0.1319\n",
            "Restored LSTM model from best epoch 9 with val_loss: 0.131873\n",
            "NFL LSTM model 131/201 completed\n",
            "Split training data: 1404 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.655, 0.66]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1404, 5, 11)\n",
            "Flattened training data shape: (1404, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 132/201 completed\n",
            "Split training data: 1422 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.66, 0.665]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1422, 5, 11)\n",
            "Flattened training data shape: (1422, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.8361, Train Loss: 0.1226, Val Acc: 0.8418, Val Loss: 0.1196\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.119555\n",
            "NFL LSTM model 133/201 completed\n",
            "Split training data: 1398 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.665, 0.67]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1398, 5, 11)\n",
            "Flattened training data shape: (1398, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 20, Train Acc: 0.8512, Train Loss: 0.1169, Val Acc: 0.8077, Val Loss: 0.1243\n",
            "Restored LSTM model from best epoch 20 with val_loss: 0.124295\n",
            "NFL LSTM model 134/201 completed\n",
            "Split training data: 1382 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.67, 0.675]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1382, 5, 11)\n",
            "Flattened training data shape: (1382, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.8314, Train Loss: 0.1313, Val Acc: 0.7727, Val Loss: 0.1500\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.149957\n",
            "NFL LSTM model 135/201 completed\n",
            "Split training data: 1360 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.675, 0.68]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1360, 5, 11)\n",
            "Flattened training data shape: (1360, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 22, Train Acc: 0.8412, Train Loss: 0.1166, Val Acc: 0.8355, Val Loss: 0.1254\n",
            "Restored LSTM model from best epoch 22 with val_loss: 0.125373\n",
            "NFL LSTM model 136/201 completed\n",
            "Split training data: 1393 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.68, 0.685]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1393, 5, 11)\n",
            "Flattened training data shape: (1393, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 18, Train Acc: 0.8320, Train Loss: 0.1231, Val Acc: 0.8774, Val Loss: 0.1000\n",
            "Restored LSTM model from best epoch 18 with val_loss: 0.099961\n",
            "NFL LSTM model 137/201 completed\n",
            "Split training data: 1421 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.685, 0.69]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1421, 5, 11)\n",
            "Flattened training data shape: (1421, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 138/201 completed\n",
            "Split training data: 1379 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.69, 0.695]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1379, 5, 11)\n",
            "Flattened training data shape: (1379, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 139/201 completed\n",
            "Split training data: 1377 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.695, 0.7]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1377, 5, 11)\n",
            "Flattened training data shape: (1377, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.8439, Train Loss: 0.1146, Val Acc: 0.8377, Val Loss: 0.1331\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.133089\n",
            "NFL LSTM model 140/201 completed\n",
            "Split training data: 1426 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.7, 0.705]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1426, 5, 11)\n",
            "Flattened training data shape: (1426, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 141/201 completed\n",
            "Split training data: 1350 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.705, 0.71]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1350, 5, 11)\n",
            "Flattened training data shape: (1350, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 18, Train Acc: 0.8496, Train Loss: 0.1107, Val Acc: 0.8278, Val Loss: 0.1071\n",
            "Restored LSTM model from best epoch 18 with val_loss: 0.107073\n",
            "NFL LSTM model 142/201 completed\n",
            "Split training data: 1357 train, 151 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.71, 0.715]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1357, 5, 11)\n",
            "Flattened training data shape: (1357, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 143/201 completed\n",
            "Split training data: 1399 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.715, 0.72]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1399, 5, 11)\n",
            "Flattened training data shape: (1399, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 144/201 completed\n",
            "Split training data: 1395 train, 155 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.72, 0.725]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1395, 5, 11)\n",
            "Flattened training data shape: (1395, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.8301, Train Loss: 0.1247, Val Acc: 0.8000, Val Loss: 0.1300\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.129984\n",
            "NFL LSTM model 145/201 completed\n",
            "Split training data: 1440 train, 160 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.725, 0.73]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1440, 5, 11)\n",
            "Flattened training data shape: (1440, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8396, Train Loss: 0.1208, Val Acc: 0.8750, Val Loss: 0.1016\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.101556\n",
            "NFL LSTM model 146/201 completed\n",
            "Split training data: 1402 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.73, 0.735]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1402, 5, 11)\n",
            "Flattened training data shape: (1402, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 147/201 completed\n",
            "Split training data: 1369 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.735, 0.74]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1369, 5, 11)\n",
            "Flattened training data shape: (1369, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 19, Train Acc: 0.8510, Train Loss: 0.1104, Val Acc: 0.8824, Val Loss: 0.1012\n",
            "Restored LSTM model from best epoch 19 with val_loss: 0.101208\n",
            "NFL LSTM model 148/201 completed\n",
            "Split training data: 1283 train, 143 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.74, 0.745]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1283, 5, 11)\n",
            "Flattened training data shape: (1283, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 149/201 completed\n",
            "Split training data: 1242 train, 138 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.745, 0.75]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1242, 5, 11)\n",
            "Flattened training data shape: (1242, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8591, Train Loss: 0.1089, Val Acc: 0.8406, Val Loss: 0.1130\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.112976\n",
            "NFL LSTM model 150/201 completed\n",
            "Split training data: 3937 train, 438 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.75, 0.755]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (3937, 5, 11)\n",
            "Flattened training data shape: (3937, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.8674, Train Loss: 0.1046, Val Acc: 0.8539, Val Loss: 0.1120\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.111956\n",
            "NFL LSTM model 151/201 completed\n",
            "Split training data: 837 train, 94 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.755, 0.76]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (837, 5, 11)\n",
            "Flattened training data shape: (837, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 14, Train Acc: 0.8638, Train Loss: 0.1027, Val Acc: 0.8511, Val Loss: 0.1063\n",
            "Restored LSTM model from best epoch 14 with val_loss: 0.106317\n",
            "NFL LSTM model 152/201 completed\n",
            "Split training data: 1089 train, 121 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.76, 0.765]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1089, 5, 11)\n",
            "Flattened training data shape: (1089, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.8439, Train Loss: 0.1151, Val Acc: 0.8430, Val Loss: 0.1253\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.125273\n",
            "NFL LSTM model 153/201 completed\n",
            "Split training data: 1458 train, 163 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.765, 0.77]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1458, 5, 11)\n",
            "Flattened training data shape: (1458, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.8793, Train Loss: 0.0953, Val Acc: 0.8528, Val Loss: 0.0966\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.096584\n",
            "NFL LSTM model 154/201 completed\n",
            "Split training data: 1233 train, 138 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.77, 0.775]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1233, 5, 11)\n",
            "Flattened training data shape: (1233, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8540, Train Loss: 0.1098, Val Acc: 0.8623, Val Loss: 0.0986\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.098613\n",
            "NFL LSTM model 155/201 completed\n",
            "Split training data: 1552 train, 173 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.775, 0.78]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1552, 5, 11)\n",
            "Flattened training data shape: (1552, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.8789, Train Loss: 0.0961, Val Acc: 0.8786, Val Loss: 0.0906\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.090643\n",
            "NFL LSTM model 156/201 completed\n",
            "Split training data: 1314 train, 146 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.78, 0.785]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1314, 5, 11)\n",
            "Flattened training data shape: (1314, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.8714, Train Loss: 0.0984, Val Acc: 0.8219, Val Loss: 0.1305\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.130476\n",
            "NFL LSTM model 157/201 completed\n",
            "Split training data: 1422 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.785, 0.79]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1422, 5, 11)\n",
            "Flattened training data shape: (1422, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.8509, Train Loss: 0.1094, Val Acc: 0.7975, Val Loss: 0.1489\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.148865\n",
            "NFL LSTM model 158/201 completed\n",
            "Split training data: 1349 train, 150 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.79, 0.795]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1349, 5, 11)\n",
            "Flattened training data shape: (1349, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.8651, Train Loss: 0.1004, Val Acc: 0.8400, Val Loss: 0.1298\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.129777\n",
            "NFL LSTM model 159/201 completed\n",
            "Split training data: 1328 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.795, 0.8]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1328, 5, 11)\n",
            "Flattened training data shape: (1328, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8502, Train Loss: 0.1182, Val Acc: 0.8378, Val Loss: 0.1269\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.126944\n",
            "NFL LSTM model 160/201 completed\n",
            "Split training data: 1485 train, 166 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.8, 0.805]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1485, 5, 11)\n",
            "Flattened training data shape: (1485, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.8788, Train Loss: 0.0933, Val Acc: 0.8554, Val Loss: 0.1073\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.107297\n",
            "NFL LSTM model 161/201 completed\n",
            "Split training data: 1429 train, 159 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.805, 0.81]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1429, 5, 11)\n",
            "Flattened training data shape: (1429, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 22, Train Acc: 0.8866, Train Loss: 0.0923, Val Acc: 0.8742, Val Loss: 0.1043\n",
            "Restored LSTM model from best epoch 22 with val_loss: 0.104297\n",
            "NFL LSTM model 162/201 completed\n",
            "Split training data: 1412 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.81, 0.815]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1412, 5, 11)\n",
            "Flattened training data shape: (1412, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8909, Train Loss: 0.0860, Val Acc: 0.8790, Val Loss: 0.0880\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.088002\n",
            "NFL LSTM model 163/201 completed\n",
            "Split training data: 1371 train, 153 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.815, 0.82]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1371, 5, 11)\n",
            "Flattened training data shape: (1371, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 164/201 completed\n",
            "Split training data: 1420 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.82, 0.825]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1420, 5, 11)\n",
            "Flattened training data shape: (1420, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 165/201 completed\n",
            "Split training data: 1330 train, 148 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.825, 0.83]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1330, 5, 11)\n",
            "Flattened training data shape: (1330, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 166/201 completed\n",
            "Split training data: 1397 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.83, 0.835]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1397, 5, 11)\n",
            "Flattened training data shape: (1397, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8747, Train Loss: 0.1005, Val Acc: 0.8718, Val Loss: 0.0967\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.096671\n",
            "NFL LSTM model 167/201 completed\n",
            "Split training data: 1396 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.835, 0.84]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1396, 5, 11)\n",
            "Flattened training data shape: (1396, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 10\n",
            "Best epoch: 5, Train Acc: 0.8818, Train Loss: 0.1044, Val Acc: 0.8397, Val Loss: 0.1146\n",
            "Restored LSTM model from best epoch 5 with val_loss: 0.114561\n",
            "NFL LSTM model 168/201 completed\n",
            "Split training data: 1386 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.84, 0.845]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1386, 5, 11)\n",
            "Flattened training data shape: (1386, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8810, Train Loss: 0.0964, Val Acc: 0.8571, Val Loss: 0.0979\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.097876\n",
            "NFL LSTM model 169/201 completed\n",
            "Split training data: 1417 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.845, 0.85]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1417, 5, 11)\n",
            "Flattened training data shape: (1417, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 170/201 completed\n",
            "Split training data: 1395 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.85, 0.855]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1395, 5, 11)\n",
            "Flattened training data shape: (1395, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 171/201 completed\n",
            "Split training data: 1385 train, 154 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.855, 0.86]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1385, 5, 11)\n",
            "Flattened training data shape: (1385, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.8816, Train Loss: 0.0952, Val Acc: 0.8571, Val Loss: 0.1160\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.116048\n",
            "NFL LSTM model 172/201 completed\n",
            "Split training data: 1414 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.86, 0.865]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1414, 5, 11)\n",
            "Flattened training data shape: (1414, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.8946, Train Loss: 0.0839, Val Acc: 0.8354, Val Loss: 0.1223\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.122255\n",
            "NFL LSTM model 173/201 completed\n",
            "Split training data: 1367 train, 152 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.865, 0.87]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1367, 5, 11)\n",
            "Flattened training data shape: (1367, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 10, Train Acc: 0.8778, Train Loss: 0.0926, Val Acc: 0.8816, Val Loss: 0.0866\n",
            "Restored LSTM model from best epoch 10 with val_loss: 0.086648\n",
            "NFL LSTM model 174/201 completed\n",
            "Split training data: 1405 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.87, 0.875]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1405, 5, 11)\n",
            "Flattened training data shape: (1405, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.8662, Train Loss: 0.0988, Val Acc: 0.8854, Val Loss: 0.0818\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.081778\n",
            "NFL LSTM model 175/201 completed\n",
            "Split training data: 1422 train, 158 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.875, 0.88]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1422, 5, 11)\n",
            "Flattened training data shape: (1422, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 29\n",
            "Best epoch: 24, Train Acc: 0.8987, Train Loss: 0.0809, Val Acc: 0.9114, Val Loss: 0.0739\n",
            "Restored LSTM model from best epoch 24 with val_loss: 0.073914\n",
            "NFL LSTM model 176/201 completed\n",
            "Split training data: 1409 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.88, 0.885]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1409, 5, 11)\n",
            "Flattened training data shape: (1409, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 177/201 completed\n",
            "Split training data: 1405 train, 157 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.885, 0.89]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1405, 5, 11)\n",
            "Flattened training data shape: (1405, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 11, Train Acc: 0.8754, Train Loss: 0.0921, Val Acc: 0.8408, Val Loss: 0.1022\n",
            "Restored LSTM model from best epoch 11 with val_loss: 0.102205\n",
            "NFL LSTM model 178/201 completed\n",
            "Split training data: 1445 train, 161 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.89, 0.895]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1445, 5, 11)\n",
            "Flattened training data shape: (1445, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 179/201 completed\n",
            "Split training data: 1449 train, 162 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.895, 0.9]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1449, 5, 11)\n",
            "Flattened training data shape: (1449, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.8903, Train Loss: 0.0836, Val Acc: 0.8704, Val Loss: 0.0995\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.099456\n",
            "NFL LSTM model 180/201 completed\n",
            "Split training data: 1395 train, 156 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.9, 0.905]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1395, 5, 11)\n",
            "Flattened training data shape: (1395, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 17, Train Acc: 0.8903, Train Loss: 0.0850, Val Acc: 0.8462, Val Loss: 0.1028\n",
            "Restored LSTM model from best epoch 17 with val_loss: 0.102839\n",
            "NFL LSTM model 181/201 completed\n",
            "Split training data: 1434 train, 160 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.905, 0.91]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1434, 5, 11)\n",
            "Flattened training data shape: (1434, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 7, Train Acc: 0.8828, Train Loss: 0.0879, Val Acc: 0.8688, Val Loss: 0.0924\n",
            "Restored LSTM model from best epoch 7 with val_loss: 0.092351\n",
            "NFL LSTM model 182/201 completed\n",
            "Split training data: 1449 train, 161 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.91, 0.915]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1449, 5, 11)\n",
            "Flattened training data shape: (1449, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 183/201 completed\n",
            "Split training data: 1474 train, 164 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.915, 0.92]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1474, 5, 11)\n",
            "Flattened training data shape: (1474, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 184/201 completed\n",
            "Split training data: 1542 train, 172 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.92, 0.925]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1542, 5, 11)\n",
            "Flattened training data shape: (1542, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 185/201 completed\n",
            "Split training data: 1556 train, 173 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.925, 0.93]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1556, 5, 11)\n",
            "Flattened training data shape: (1556, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 8, Train Acc: 0.8895, Train Loss: 0.0863, Val Acc: 0.8613, Val Loss: 0.0979\n",
            "Restored LSTM model from best epoch 8 with val_loss: 0.097938\n",
            "NFL LSTM model 186/201 completed\n",
            "Split training data: 1576 train, 176 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.93, 0.935]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1576, 5, 11)\n",
            "Flattened training data shape: (1576, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 21, Train Acc: 0.9029, Train Loss: 0.0709, Val Acc: 0.8977, Val Loss: 0.0630\n",
            "Restored LSTM model from best epoch 21 with val_loss: 0.062957\n",
            "NFL LSTM model 187/201 completed\n",
            "Split training data: 1720 train, 192 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.935, 0.94]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1720, 5, 11)\n",
            "Flattened training data shape: (1720, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 188/201 completed\n",
            "Split training data: 1806 train, 201 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.94, 0.945]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1806, 5, 11)\n",
            "Flattened training data shape: (1806, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 18, Train Acc: 0.9181, Train Loss: 0.0623, Val Acc: 0.9204, Val Loss: 0.0573\n",
            "Restored LSTM model from best epoch 18 with val_loss: 0.057328\n",
            "NFL LSTM model 189/201 completed\n",
            "Split training data: 1807 train, 201 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.945, 0.95]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1807, 5, 11)\n",
            "Flattened training data shape: (1807, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 190/201 completed\n",
            "Split training data: 1856 train, 207 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.95, 0.955]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1856, 5, 11)\n",
            "Flattened training data shape: (1856, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 191/201 completed\n",
            "Split training data: 1764 train, 196 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.955, 0.96]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1764, 5, 11)\n",
            "Flattened training data shape: (1764, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 16, Train Acc: 0.9019, Train Loss: 0.0724, Val Acc: 0.9184, Val Loss: 0.0527\n",
            "Restored LSTM model from best epoch 16 with val_loss: 0.052699\n",
            "NFL LSTM model 192/201 completed\n",
            "Split training data: 1813 train, 202 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.96, 0.965]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1813, 5, 11)\n",
            "Flattened training data shape: (1813, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 6, Train Acc: 0.8924, Train Loss: 0.0848, Val Acc: 0.8663, Val Loss: 0.0866\n",
            "Restored LSTM model from best epoch 6 with val_loss: 0.086579\n",
            "NFL LSTM model 193/201 completed\n",
            "Split training data: 1690 train, 188 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.965, 0.97]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1690, 5, 11)\n",
            "Flattened training data shape: (1690, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 13, Train Acc: 0.9148, Train Loss: 0.0664, Val Acc: 0.9202, Val Loss: 0.0622\n",
            "Restored LSTM model from best epoch 13 with val_loss: 0.062246\n",
            "NFL LSTM model 194/201 completed\n",
            "Split training data: 3959 train, 440 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.97, 0.975]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (3959, 5, 11)\n",
            "Flattened training data shape: (3959, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 23, Train Acc: 0.9166, Train Loss: 0.0644, Val Acc: 0.9205, Val Loss: 0.0628\n",
            "Restored LSTM model from best epoch 23 with val_loss: 0.062784\n",
            "NFL LSTM model 195/201 completed\n",
            "Split training data: 1865 train, 208 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.975, 0.98]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1865, 5, 11)\n",
            "Flattened training data shape: (1865, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.9131, Train Loss: 0.0698, Val Acc: 0.8894, Val Loss: 0.0736\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.073635\n",
            "NFL LSTM model 196/201 completed\n",
            "Split training data: 1865 train, 208 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.98, 0.985]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1865, 5, 11)\n",
            "Flattened training data shape: (1865, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 12, Train Acc: 0.9110, Train Loss: 0.0669, Val Acc: 0.9279, Val Loss: 0.0561\n",
            "Restored LSTM model from best epoch 12 with val_loss: 0.056110\n",
            "NFL LSTM model 197/201 completed\n",
            "Split training data: 1819 train, 203 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.985, 0.99]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1819, 5, 11)\n",
            "Flattened training data shape: (1819, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 198/201 completed\n",
            "Split training data: 1888 train, 210 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.99, 0.995]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1888, 5, 11)\n",
            "Flattened training data shape: (1888, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 199/201 completed\n",
            "Split training data: 1830 train, 204 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [0.995, 1.0]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (1830, 5, 11)\n",
            "Flattened training data shape: (1830, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "NFL LSTM model 200/201 completed\n",
            "Split training data: 2333 train, 260 validation\n",
            "\n",
            "Training direct prediction LSTM model for timestep range [1.0, 1.005]\n",
            "Fitting scaler on training data...\n",
            "Training data shape: (2333, 5, 11)\n",
            "Flattened training data shape: (2333, 55)\n",
            "Scaler fitted with 55 features\n",
            "Starting LSTM training on device: cpu\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 15, Train Acc: 0.9438, Train Loss: 0.0454, Val Acc: 0.9538, Val Loss: 0.0337\n",
            "Restored LSTM model from best epoch 15 with val_loss: 0.033692\n",
            "NFL LSTM model 201/201 completed\n"
          ]
        }
      ],
      "source": [
        "all_models[\"lstm\"] = setup_direct_lstm_models(training_data_seq, None, num_models=201)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data shape: (2761, 11)\n",
            "Flattened data shape: (2761, 11)\n",
            "Split training data: 2484 train, 277 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.0, 0.005]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.6888, Train Loss: 0.2041, Val Acc: 0.6643, Val Loss: 0.2076\n",
            "Restored model from best epoch 14 with val_loss: 0.207554\n",
            "NFL direct model 1/201 completed\n",
            "Original data shape: (979, 11)\n",
            "Flattened data shape: (979, 11)\n",
            "Split training data: 881 train, 98 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.005, 0.01]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.6742, Train Loss: 0.2095, Val Acc: 0.6531, Val Loss: 0.2311\n",
            "Restored model from best epoch 9 with val_loss: 0.231136\n",
            "NFL direct model 2/201 completed\n",
            "Original data shape: (1816, 11)\n",
            "Flattened data shape: (1816, 11)\n",
            "Split training data: 1634 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.01, 0.015]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.6695, Train Loss: 0.2088, Val Acc: 0.7143, Val Loss: 0.1957\n",
            "Restored model from best epoch 8 with val_loss: 0.195706\n",
            "NFL direct model 3/201 completed\n",
            "Original data shape: (1536, 11)\n",
            "Flattened data shape: (1536, 11)\n",
            "Split training data: 1382 train, 154 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.015, 0.02]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.6737, Train Loss: 0.2114, Val Acc: 0.6623, Val Loss: 0.1896\n",
            "Restored model from best epoch 13 with val_loss: 0.189564\n",
            "NFL direct model 4/201 completed\n",
            "Original data shape: (1718, 11)\n",
            "Flattened data shape: (1718, 11)\n",
            "Split training data: 1546 train, 172 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.02, 0.025]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.6455, Train Loss: 0.2242, Val Acc: 0.6337, Val Loss: 0.2259\n",
            "Restored model from best epoch 3 with val_loss: 0.225869\n",
            "NFL direct model 5/201 completed\n",
            "Original data shape: (1772, 11)\n",
            "Flattened data shape: (1772, 11)\n",
            "Split training data: 1594 train, 178 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.025, 0.03]\n",
            "Early stopping at epoch 34\n",
            "Best epoch: 24, Train Acc: 0.6957, Train Loss: 0.2016, Val Acc: 0.6629, Val Loss: 0.2020\n",
            "Restored model from best epoch 24 with val_loss: 0.201972\n",
            "NFL direct model 6/201 completed\n",
            "Original data shape: (1685, 11)\n",
            "Flattened data shape: (1685, 11)\n",
            "Split training data: 1516 train, 169 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.03, 0.035]\n",
            "Early stopping at epoch 36\n",
            "Best epoch: 26, Train Acc: 0.6959, Train Loss: 0.1985, Val Acc: 0.6982, Val Loss: 0.2040\n",
            "Restored model from best epoch 26 with val_loss: 0.204031\n",
            "NFL direct model 7/201 completed\n",
            "Original data shape: (1829, 11)\n",
            "Flattened data shape: (1829, 11)\n",
            "Split training data: 1646 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.035, 0.04]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.6586, Train Loss: 0.2167, Val Acc: 0.6557, Val Loss: 0.2141\n",
            "Restored model from best epoch 6 with val_loss: 0.214098\n",
            "NFL direct model 8/201 completed\n",
            "Original data shape: (1743, 11)\n",
            "Flattened data shape: (1743, 11)\n",
            "Split training data: 1568 train, 175 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.04, 0.045]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.6773, Train Loss: 0.2087, Val Acc: 0.7143, Val Loss: 0.1893\n",
            "Restored model from best epoch 13 with val_loss: 0.189329\n",
            "NFL direct model 9/201 completed\n",
            "Original data shape: (1865, 11)\n",
            "Flattened data shape: (1865, 11)\n",
            "Split training data: 1678 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.045, 0.05]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.6609, Train Loss: 0.2225, Val Acc: 0.6898, Val Loss: 0.2028\n",
            "Restored model from best epoch 4 with val_loss: 0.202844\n",
            "NFL direct model 10/201 completed\n",
            "Original data shape: (1812, 11)\n",
            "Flattened data shape: (1812, 11)\n",
            "Split training data: 1630 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.05, 0.055]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.6742, Train Loss: 0.2114, Val Acc: 0.6703, Val Loss: 0.2131\n",
            "Restored model from best epoch 6 with val_loss: 0.213135\n",
            "NFL direct model 11/201 completed\n",
            "Original data shape: (1748, 11)\n",
            "Flattened data shape: (1748, 11)\n",
            "Split training data: 1573 train, 175 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.055, 0.06]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.6879, Train Loss: 0.2054, Val Acc: 0.6571, Val Loss: 0.2214\n",
            "Restored model from best epoch 9 with val_loss: 0.221426\n",
            "NFL direct model 12/201 completed\n",
            "Original data shape: (1898, 11)\n",
            "Flattened data shape: (1898, 11)\n",
            "Split training data: 1708 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.06, 0.065]\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 1, Train Acc: 0.6019, Train Loss: 0.2391, Val Acc: 0.6474, Val Loss: 0.2387\n",
            "Restored model from best epoch 1 with val_loss: 0.238739\n",
            "NFL direct model 13/201 completed\n",
            "Original data shape: (1825, 11)\n",
            "Flattened data shape: (1825, 11)\n",
            "Split training data: 1642 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.065, 0.07]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.6510, Train Loss: 0.2199, Val Acc: 0.6667, Val Loss: 0.2077\n",
            "Restored model from best epoch 4 with val_loss: 0.207727\n",
            "NFL direct model 14/201 completed\n",
            "Original data shape: (1864, 11)\n",
            "Flattened data shape: (1864, 11)\n",
            "Split training data: 1677 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.07, 0.075]\n",
            "Early stopping at epoch 30\n",
            "Best epoch: 20, Train Acc: 0.7001, Train Loss: 0.1980, Val Acc: 0.6952, Val Loss: 0.1876\n",
            "Restored model from best epoch 20 with val_loss: 0.187628\n",
            "NFL direct model 15/201 completed\n",
            "Original data shape: (1810, 11)\n",
            "Flattened data shape: (1810, 11)\n",
            "Split training data: 1629 train, 181 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.075, 0.08]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.6918, Train Loss: 0.2026, Val Acc: 0.7403, Val Loss: 0.1956\n",
            "Restored model from best epoch 17 with val_loss: 0.195594\n",
            "NFL direct model 16/201 completed\n",
            "Original data shape: (1880, 11)\n",
            "Flattened data shape: (1880, 11)\n",
            "Split training data: 1692 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.08, 0.085]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.6921, Train Loss: 0.2023, Val Acc: 0.7181, Val Loss: 0.1890\n",
            "Restored model from best epoch 9 with val_loss: 0.188951\n",
            "NFL direct model 17/201 completed\n",
            "Original data shape: (1902, 11)\n",
            "Flattened data shape: (1902, 11)\n",
            "Split training data: 1711 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.085, 0.09]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.6897, Train Loss: 0.2021, Val Acc: 0.6597, Val Loss: 0.2194\n",
            "Restored model from best epoch 10 with val_loss: 0.219411\n",
            "NFL direct model 18/201 completed\n",
            "Original data shape: (1798, 11)\n",
            "Flattened data shape: (1798, 11)\n",
            "Split training data: 1618 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.09, 0.095]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.6724, Train Loss: 0.2108, Val Acc: 0.7056, Val Loss: 0.1994\n",
            "Restored model from best epoch 6 with val_loss: 0.199423\n",
            "NFL direct model 19/201 completed\n",
            "Original data shape: (1881, 11)\n",
            "Flattened data shape: (1881, 11)\n",
            "Split training data: 1692 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.095, 0.1]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.6525, Train Loss: 0.2183, Val Acc: 0.5820, Val Loss: 0.2331\n",
            "Restored model from best epoch 2 with val_loss: 0.233123\n",
            "NFL direct model 20/201 completed\n",
            "Original data shape: (1848, 11)\n",
            "Flattened data shape: (1848, 11)\n",
            "Split training data: 1663 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.1, 0.105]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.6849, Train Loss: 0.2032, Val Acc: 0.7081, Val Loss: 0.1908\n",
            "Restored model from best epoch 8 with val_loss: 0.190754\n",
            "NFL direct model 21/201 completed\n",
            "Original data shape: (1831, 11)\n",
            "Flattened data shape: (1831, 11)\n",
            "Split training data: 1647 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.105, 0.11]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.6697, Train Loss: 0.2113, Val Acc: 0.6467, Val Loss: 0.2076\n",
            "Restored model from best epoch 3 with val_loss: 0.207621\n",
            "NFL direct model 22/201 completed\n",
            "Original data shape: (1892, 11)\n",
            "Flattened data shape: (1892, 11)\n",
            "Split training data: 1702 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.11, 0.115]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.6939, Train Loss: 0.2076, Val Acc: 0.6368, Val Loss: 0.2138\n",
            "Restored model from best epoch 5 with val_loss: 0.213829\n",
            "NFL direct model 23/201 completed\n",
            "Original data shape: (1819, 11)\n",
            "Flattened data shape: (1819, 11)\n",
            "Split training data: 1637 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.115, 0.12]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.7080, Train Loss: 0.1962, Val Acc: 0.7637, Val Loss: 0.1740\n",
            "Restored model from best epoch 14 with val_loss: 0.174038\n",
            "NFL direct model 24/201 completed\n",
            "Original data shape: (1885, 11)\n",
            "Flattened data shape: (1885, 11)\n",
            "Split training data: 1696 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.12, 0.125]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.6952, Train Loss: 0.2054, Val Acc: 0.6825, Val Loss: 0.1976\n",
            "Restored model from best epoch 9 with val_loss: 0.197571\n",
            "NFL direct model 25/201 completed\n",
            "Original data shape: (1899, 11)\n",
            "Flattened data shape: (1899, 11)\n",
            "Split training data: 1709 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.125, 0.13]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.6823, Train Loss: 0.2106, Val Acc: 0.6895, Val Loss: 0.2104\n",
            "Restored model from best epoch 2 with val_loss: 0.210384\n",
            "NFL direct model 26/201 completed\n",
            "Original data shape: (1876, 11)\n",
            "Flattened data shape: (1876, 11)\n",
            "Split training data: 1688 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.13, 0.135]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.7180, Train Loss: 0.1888, Val Acc: 0.6755, Val Loss: 0.1998\n",
            "Restored model from best epoch 15 with val_loss: 0.199756\n",
            "NFL direct model 27/201 completed\n",
            "Original data shape: (1845, 11)\n",
            "Flattened data shape: (1845, 11)\n",
            "Split training data: 1660 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.135, 0.14]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.6904, Train Loss: 0.2005, Val Acc: 0.7243, Val Loss: 0.1913\n",
            "Restored model from best epoch 9 with val_loss: 0.191256\n",
            "NFL direct model 28/201 completed\n",
            "Original data shape: (1789, 11)\n",
            "Flattened data shape: (1789, 11)\n",
            "Split training data: 1610 train, 179 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.14, 0.145]\n",
            "Early stopping at epoch 63\n",
            "Best epoch: 53, Train Acc: 0.7826, Train Loss: 0.1611, Val Acc: 0.7207, Val Loss: 0.1890\n",
            "Restored model from best epoch 53 with val_loss: 0.189023\n",
            "NFL direct model 29/201 completed\n",
            "Original data shape: (1838, 11)\n",
            "Flattened data shape: (1838, 11)\n",
            "Split training data: 1654 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.145, 0.15]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.7128, Train Loss: 0.1934, Val Acc: 0.7717, Val Loss: 0.1607\n",
            "Restored model from best epoch 10 with val_loss: 0.160674\n",
            "NFL direct model 30/201 completed\n",
            "Original data shape: (1906, 11)\n",
            "Flattened data shape: (1906, 11)\n",
            "Split training data: 1715 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.15, 0.155]\n",
            "Early stopping at epoch 34\n",
            "Best epoch: 24, Train Acc: 0.7359, Train Loss: 0.1804, Val Acc: 0.7120, Val Loss: 0.1873\n",
            "Restored model from best epoch 24 with val_loss: 0.187251\n",
            "NFL direct model 31/201 completed\n",
            "Original data shape: (1752, 11)\n",
            "Flattened data shape: (1752, 11)\n",
            "Split training data: 1576 train, 176 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.155, 0.16]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.6872, Train Loss: 0.2039, Val Acc: 0.7330, Val Loss: 0.1938\n",
            "Restored model from best epoch 4 with val_loss: 0.193767\n",
            "NFL direct model 32/201 completed\n",
            "Original data shape: (1956, 11)\n",
            "Flattened data shape: (1956, 11)\n",
            "Split training data: 1760 train, 196 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.16, 0.165]\n",
            "Early stopping at epoch 32\n",
            "Best epoch: 22, Train Acc: 0.7369, Train Loss: 0.1792, Val Acc: 0.7143, Val Loss: 0.1799\n",
            "Restored model from best epoch 22 with val_loss: 0.179932\n",
            "NFL direct model 33/201 completed\n",
            "Original data shape: (1784, 11)\n",
            "Flattened data shape: (1784, 11)\n",
            "Split training data: 1605 train, 179 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.165, 0.17]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.6854, Train Loss: 0.2064, Val Acc: 0.6592, Val Loss: 0.2043\n",
            "Restored model from best epoch 4 with val_loss: 0.204279\n",
            "NFL direct model 34/201 completed\n",
            "Original data shape: (1844, 11)\n",
            "Flattened data shape: (1844, 11)\n",
            "Split training data: 1659 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.17, 0.175]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.6992, Train Loss: 0.1984, Val Acc: 0.7027, Val Loss: 0.2073\n",
            "Restored model from best epoch 4 with val_loss: 0.207265\n",
            "NFL direct model 35/201 completed\n",
            "Original data shape: (1915, 11)\n",
            "Flattened data shape: (1915, 11)\n",
            "Split training data: 1723 train, 192 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.175, 0.18]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.6663, Train Loss: 0.2105, Val Acc: 0.7083, Val Loss: 0.1963\n",
            "Restored model from best epoch 3 with val_loss: 0.196342\n",
            "NFL direct model 36/201 completed\n",
            "Original data shape: (1794, 11)\n",
            "Flattened data shape: (1794, 11)\n",
            "Split training data: 1614 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.18, 0.185]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.7286, Train Loss: 0.1815, Val Acc: 0.7500, Val Loss: 0.1686\n",
            "Restored model from best epoch 17 with val_loss: 0.168599\n",
            "NFL direct model 37/201 completed\n",
            "Original data shape: (1855, 11)\n",
            "Flattened data shape: (1855, 11)\n",
            "Split training data: 1669 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.185, 0.19]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.7154, Train Loss: 0.1892, Val Acc: 0.7581, Val Loss: 0.1737\n",
            "Restored model from best epoch 14 with val_loss: 0.173741\n",
            "NFL direct model 38/201 completed\n",
            "Original data shape: (1795, 11)\n",
            "Flattened data shape: (1795, 11)\n",
            "Split training data: 1615 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.19, 0.195]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.7152, Train Loss: 0.1874, Val Acc: 0.7333, Val Loss: 0.1850\n",
            "Restored model from best epoch 5 with val_loss: 0.184961\n",
            "NFL direct model 39/201 completed\n",
            "Original data shape: (1868, 11)\n",
            "Flattened data shape: (1868, 11)\n",
            "Split training data: 1681 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.195, 0.2]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.7026, Train Loss: 0.1965, Val Acc: 0.7647, Val Loss: 0.1662\n",
            "Restored model from best epoch 4 with val_loss: 0.166248\n",
            "NFL direct model 40/201 completed\n",
            "Original data shape: (1819, 11)\n",
            "Flattened data shape: (1819, 11)\n",
            "Split training data: 1637 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.2, 0.205]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.7049, Train Loss: 0.1894, Val Acc: 0.6813, Val Loss: 0.1961\n",
            "Restored model from best epoch 11 with val_loss: 0.196111\n",
            "NFL direct model 41/201 completed\n",
            "Original data shape: (1837, 11)\n",
            "Flattened data shape: (1837, 11)\n",
            "Split training data: 1653 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.205, 0.21]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.6897, Train Loss: 0.2039, Val Acc: 0.6304, Val Loss: 0.2142\n",
            "Restored model from best epoch 2 with val_loss: 0.214189\n",
            "NFL direct model 42/201 completed\n",
            "Original data shape: (1944, 11)\n",
            "Flattened data shape: (1944, 11)\n",
            "Split training data: 1749 train, 195 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.21, 0.215]\n",
            "Early stopping at epoch 45\n",
            "Best epoch: 35, Train Acc: 0.7536, Train Loss: 0.1676, Val Acc: 0.7487, Val Loss: 0.1781\n",
            "Restored model from best epoch 35 with val_loss: 0.178115\n",
            "NFL direct model 43/201 completed\n",
            "Original data shape: (1754, 11)\n",
            "Flattened data shape: (1754, 11)\n",
            "Split training data: 1578 train, 176 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.215, 0.22]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.6806, Train Loss: 0.2041, Val Acc: 0.7500, Val Loss: 0.1842\n",
            "Restored model from best epoch 3 with val_loss: 0.184176\n",
            "NFL direct model 44/201 completed\n",
            "Original data shape: (1877, 11)\n",
            "Flattened data shape: (1877, 11)\n",
            "Split training data: 1689 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.22, 0.225]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.6898, Train Loss: 0.2084, Val Acc: 0.6436, Val Loss: 0.2114\n",
            "Restored model from best epoch 2 with val_loss: 0.211426\n",
            "NFL direct model 45/201 completed\n",
            "Original data shape: (1898, 11)\n",
            "Flattened data shape: (1898, 11)\n",
            "Split training data: 1708 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.225, 0.23]\n",
            "Early stopping at epoch 32\n",
            "Best epoch: 22, Train Acc: 0.7559, Train Loss: 0.1730, Val Acc: 0.7368, Val Loss: 0.1831\n",
            "Restored model from best epoch 22 with val_loss: 0.183113\n",
            "NFL direct model 46/201 completed\n",
            "Original data shape: (1775, 11)\n",
            "Flattened data shape: (1775, 11)\n",
            "Split training data: 1597 train, 178 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.23, 0.235]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.7120, Train Loss: 0.1917, Val Acc: 0.7191, Val Loss: 0.1894\n",
            "Restored model from best epoch 4 with val_loss: 0.189419\n",
            "NFL direct model 47/201 completed\n",
            "Original data shape: (1906, 11)\n",
            "Flattened data shape: (1906, 11)\n",
            "Split training data: 1715 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.235, 0.24]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.7394, Train Loss: 0.1789, Val Acc: 0.6492, Val Loss: 0.2061\n",
            "Restored model from best epoch 8 with val_loss: 0.206128\n",
            "NFL direct model 48/201 completed\n",
            "Original data shape: (1800, 11)\n",
            "Flattened data shape: (1800, 11)\n",
            "Split training data: 1620 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.24, 0.245]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.6846, Train Loss: 0.2092, Val Acc: 0.7111, Val Loss: 0.1935\n",
            "Restored model from best epoch 3 with val_loss: 0.193506\n",
            "NFL direct model 49/201 completed\n",
            "Original data shape: (1688, 11)\n",
            "Flattened data shape: (1688, 11)\n",
            "Split training data: 1519 train, 169 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.245, 0.25]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.7380, Train Loss: 0.1795, Val Acc: 0.7219, Val Loss: 0.1934\n",
            "Restored model from best epoch 13 with val_loss: 0.193372\n",
            "NFL direct model 50/201 completed\n",
            "Original data shape: (5504, 11)\n",
            "Flattened data shape: (5504, 11)\n",
            "Split training data: 4953 train, 551 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.25, 0.255]\n",
            "Early stopping at epoch 57\n",
            "Best epoch: 47, Train Acc: 0.7787, Train Loss: 0.1515, Val Acc: 0.7368, Val Loss: 0.1691\n",
            "Restored model from best epoch 47 with val_loss: 0.169122\n",
            "NFL direct model 51/201 completed\n",
            "Original data shape: (1093, 11)\n",
            "Flattened data shape: (1093, 11)\n",
            "Split training data: 983 train, 110 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.255, 0.26]\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 12, Train Acc: 0.7345, Train Loss: 0.1788, Val Acc: 0.7091, Val Loss: 0.1931\n",
            "Restored model from best epoch 12 with val_loss: 0.193135\n",
            "NFL direct model 52/201 completed\n",
            "Original data shape: (1440, 11)\n",
            "Flattened data shape: (1440, 11)\n",
            "Split training data: 1296 train, 144 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.26, 0.265]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.7269, Train Loss: 0.1796, Val Acc: 0.7431, Val Loss: 0.1801\n",
            "Restored model from best epoch 15 with val_loss: 0.180071\n",
            "NFL direct model 53/201 completed\n",
            "Original data shape: (1939, 11)\n",
            "Flattened data shape: (1939, 11)\n",
            "Split training data: 1745 train, 194 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.265, 0.27]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7032, Train Loss: 0.1980, Val Acc: 0.7010, Val Loss: 0.1570\n",
            "Restored model from best epoch 2 with val_loss: 0.157040\n",
            "NFL direct model 54/201 completed\n",
            "Original data shape: (1733, 11)\n",
            "Flattened data shape: (1733, 11)\n",
            "Split training data: 1559 train, 174 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.27, 0.275]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.7453, Train Loss: 0.1736, Val Acc: 0.6897, Val Loss: 0.2040\n",
            "Restored model from best epoch 8 with val_loss: 0.203952\n",
            "NFL direct model 55/201 completed\n",
            "Original data shape: (1972, 11)\n",
            "Flattened data shape: (1972, 11)\n",
            "Split training data: 1774 train, 198 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.275, 0.28]\n",
            "Early stopping at epoch 31\n",
            "Best epoch: 21, Train Acc: 0.7554, Train Loss: 0.1679, Val Acc: 0.6869, Val Loss: 0.2014\n",
            "Restored model from best epoch 21 with val_loss: 0.201384\n",
            "NFL direct model 56/201 completed\n",
            "Original data shape: (1710, 11)\n",
            "Flattened data shape: (1710, 11)\n",
            "Split training data: 1539 train, 171 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.28, 0.285]\n",
            "Early stopping at epoch 36\n",
            "Best epoch: 26, Train Acc: 0.7472, Train Loss: 0.1732, Val Acc: 0.7251, Val Loss: 0.1747\n",
            "Restored model from best epoch 26 with val_loss: 0.174685\n",
            "NFL direct model 57/201 completed\n",
            "Original data shape: (1981, 11)\n",
            "Flattened data shape: (1981, 11)\n",
            "Split training data: 1782 train, 199 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.285, 0.29]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.7688, Train Loss: 0.1638, Val Acc: 0.7487, Val Loss: 0.1776\n",
            "Restored model from best epoch 17 with val_loss: 0.177561\n",
            "NFL direct model 58/201 completed\n",
            "Original data shape: (1807, 11)\n",
            "Flattened data shape: (1807, 11)\n",
            "Split training data: 1626 train, 181 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.29, 0.295]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.7116, Train Loss: 0.1913, Val Acc: 0.7127, Val Loss: 0.1865\n",
            "Restored model from best epoch 6 with val_loss: 0.186475\n",
            "NFL direct model 59/201 completed\n",
            "Original data shape: (1868, 11)\n",
            "Flattened data shape: (1868, 11)\n",
            "Split training data: 1681 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.295, 0.3]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.7472, Train Loss: 0.1716, Val Acc: 0.7166, Val Loss: 0.1832\n",
            "Restored model from best epoch 13 with val_loss: 0.183220\n",
            "NFL direct model 60/201 completed\n",
            "Original data shape: (1889, 11)\n",
            "Flattened data shape: (1889, 11)\n",
            "Split training data: 1700 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.3, 0.305]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.7276, Train Loss: 0.1834, Val Acc: 0.7196, Val Loss: 0.1835\n",
            "Restored model from best epoch 8 with val_loss: 0.183546\n",
            "NFL direct model 61/201 completed\n",
            "Original data shape: (1762, 11)\n",
            "Flattened data shape: (1762, 11)\n",
            "Split training data: 1585 train, 177 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.305, 0.31]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.7098, Train Loss: 0.1914, Val Acc: 0.7232, Val Loss: 0.1843\n",
            "Restored model from best epoch 3 with val_loss: 0.184340\n",
            "NFL direct model 62/201 completed\n",
            "Original data shape: (1898, 11)\n",
            "Flattened data shape: (1898, 11)\n",
            "Split training data: 1708 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.31, 0.315]\n",
            "Early stopping at epoch 44\n",
            "Best epoch: 34, Train Acc: 0.7875, Train Loss: 0.1494, Val Acc: 0.6947, Val Loss: 0.1926\n",
            "Restored model from best epoch 34 with val_loss: 0.192581\n",
            "NFL direct model 63/201 completed\n",
            "Original data shape: (1863, 11)\n",
            "Flattened data shape: (1863, 11)\n",
            "Split training data: 1676 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.315, 0.32]\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 18, Train Acc: 0.7393, Train Loss: 0.1739, Val Acc: 0.7380, Val Loss: 0.1758\n",
            "Restored model from best epoch 18 with val_loss: 0.175750\n",
            "NFL direct model 64/201 completed\n",
            "Original data shape: (1881, 11)\n",
            "Flattened data shape: (1881, 11)\n",
            "Split training data: 1692 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.32, 0.325]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.7139, Train Loss: 0.1904, Val Acc: 0.7778, Val Loss: 0.1591\n",
            "Restored model from best epoch 3 with val_loss: 0.159085\n",
            "NFL direct model 65/201 completed\n",
            "Original data shape: (1802, 11)\n",
            "Flattened data shape: (1802, 11)\n",
            "Split training data: 1621 train, 181 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.325, 0.33]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.7711, Train Loss: 0.1628, Val Acc: 0.7348, Val Loss: 0.1848\n",
            "Restored model from best epoch 14 with val_loss: 0.184820\n",
            "NFL direct model 66/201 completed\n",
            "Original data shape: (1865, 11)\n",
            "Flattened data shape: (1865, 11)\n",
            "Split training data: 1678 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.33, 0.335]\n",
            "Early stopping at epoch 33\n",
            "Best epoch: 23, Train Acc: 0.7706, Train Loss: 0.1604, Val Acc: 0.7647, Val Loss: 0.1538\n",
            "Restored model from best epoch 23 with val_loss: 0.153847\n",
            "NFL direct model 67/201 completed\n",
            "Original data shape: (1898, 11)\n",
            "Flattened data shape: (1898, 11)\n",
            "Split training data: 1708 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.335, 0.34]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.7436, Train Loss: 0.1727, Val Acc: 0.7579, Val Loss: 0.1639\n",
            "Restored model from best epoch 7 with val_loss: 0.163872\n",
            "NFL direct model 68/201 completed\n",
            "Original data shape: (1834, 11)\n",
            "Flattened data shape: (1834, 11)\n",
            "Split training data: 1650 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.34, 0.345]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.7418, Train Loss: 0.1735, Val Acc: 0.6739, Val Loss: 0.1954\n",
            "Restored model from best epoch 6 with val_loss: 0.195446\n",
            "NFL direct model 69/201 completed\n",
            "Original data shape: (1927, 11)\n",
            "Flattened data shape: (1927, 11)\n",
            "Split training data: 1734 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.345, 0.35]\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 12, Train Acc: 0.7578, Train Loss: 0.1592, Val Acc: 0.7668, Val Loss: 0.1380\n",
            "Restored model from best epoch 12 with val_loss: 0.138009\n",
            "NFL direct model 70/201 completed\n",
            "Original data shape: (1888, 11)\n",
            "Flattened data shape: (1888, 11)\n",
            "Split training data: 1699 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.35, 0.355]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7181, Train Loss: 0.1896, Val Acc: 0.7196, Val Loss: 0.1918\n",
            "Restored model from best epoch 2 with val_loss: 0.191802\n",
            "NFL direct model 71/201 completed\n",
            "Original data shape: (1876, 11)\n",
            "Flattened data shape: (1876, 11)\n",
            "Split training data: 1688 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.355, 0.36]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.7364, Train Loss: 0.1760, Val Acc: 0.7606, Val Loss: 0.1626\n",
            "Restored model from best epoch 7 with val_loss: 0.162552\n",
            "NFL direct model 72/201 completed\n",
            "Original data shape: (1837, 11)\n",
            "Flattened data shape: (1837, 11)\n",
            "Split training data: 1653 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.36, 0.365]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.7592, Train Loss: 0.1674, Val Acc: 0.7772, Val Loss: 0.1470\n",
            "Restored model from best epoch 15 with val_loss: 0.147026\n",
            "NFL direct model 73/201 completed\n",
            "Original data shape: (1910, 11)\n",
            "Flattened data shape: (1910, 11)\n",
            "Split training data: 1719 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.365, 0.37]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7155, Train Loss: 0.1884, Val Acc: 0.7277, Val Loss: 0.1745\n",
            "Restored model from best epoch 2 with val_loss: 0.174540\n",
            "NFL direct model 74/201 completed\n",
            "Original data shape: (1813, 11)\n",
            "Flattened data shape: (1813, 11)\n",
            "Split training data: 1631 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.37, 0.375]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.7456, Train Loss: 0.1749, Val Acc: 0.6648, Val Loss: 0.2028\n",
            "Restored model from best epoch 4 with val_loss: 0.202785\n",
            "NFL direct model 75/201 completed\n",
            "Original data shape: (1885, 11)\n",
            "Flattened data shape: (1885, 11)\n",
            "Split training data: 1696 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.375, 0.38]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.7571, Train Loss: 0.1636, Val Acc: 0.7566, Val Loss: 0.1605\n",
            "Restored model from best epoch 11 with val_loss: 0.160461\n",
            "NFL direct model 76/201 completed\n",
            "Original data shape: (1836, 11)\n",
            "Flattened data shape: (1836, 11)\n",
            "Split training data: 1652 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.38, 0.385]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.7657, Train Loss: 0.1602, Val Acc: 0.7880, Val Loss: 0.1499\n",
            "Restored model from best epoch 10 with val_loss: 0.149927\n",
            "NFL direct model 77/201 completed\n",
            "Original data shape: (1845, 11)\n",
            "Flattened data shape: (1845, 11)\n",
            "Split training data: 1660 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.385, 0.39]\n",
            "Early stopping at epoch 31\n",
            "Best epoch: 21, Train Acc: 0.7723, Train Loss: 0.1559, Val Acc: 0.7622, Val Loss: 0.1640\n",
            "Restored model from best epoch 21 with val_loss: 0.164020\n",
            "NFL direct model 78/201 completed\n",
            "Original data shape: (1849, 11)\n",
            "Flattened data shape: (1849, 11)\n",
            "Split training data: 1664 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.39, 0.395]\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 16, Train Acc: 0.7638, Train Loss: 0.1585, Val Acc: 0.7676, Val Loss: 0.1743\n",
            "Restored model from best epoch 16 with val_loss: 0.174314\n",
            "NFL direct model 79/201 completed\n",
            "Original data shape: (1900, 11)\n",
            "Flattened data shape: (1900, 11)\n",
            "Split training data: 1710 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.395, 0.4]\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 12, Train Acc: 0.7567, Train Loss: 0.1656, Val Acc: 0.7895, Val Loss: 0.1604\n",
            "Restored model from best epoch 12 with val_loss: 0.160370\n",
            "NFL direct model 80/201 completed\n",
            "Original data shape: (1856, 11)\n",
            "Flattened data shape: (1856, 11)\n",
            "Split training data: 1670 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.4, 0.405]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7054, Train Loss: 0.2018, Val Acc: 0.6720, Val Loss: 0.2009\n",
            "Restored model from best epoch 2 with val_loss: 0.200878\n",
            "NFL direct model 81/201 completed\n",
            "Original data shape: (1764, 11)\n",
            "Flattened data shape: (1764, 11)\n",
            "Split training data: 1587 train, 177 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.405, 0.41]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.7851, Train Loss: 0.1513, Val Acc: 0.7571, Val Loss: 0.1757\n",
            "Restored model from best epoch 13 with val_loss: 0.175674\n",
            "NFL direct model 82/201 completed\n",
            "Original data shape: (1925, 11)\n",
            "Flattened data shape: (1925, 11)\n",
            "Split training data: 1732 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.41, 0.415]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.7390, Train Loss: 0.1819, Val Acc: 0.7979, Val Loss: 0.1159\n",
            "Restored model from best epoch 5 with val_loss: 0.115916\n",
            "NFL direct model 83/201 completed\n",
            "Original data shape: (1855, 11)\n",
            "Flattened data shape: (1855, 11)\n",
            "Split training data: 1669 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.415, 0.42]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.7543, Train Loss: 0.1673, Val Acc: 0.7634, Val Loss: 0.1631\n",
            "Restored model from best epoch 6 with val_loss: 0.163127\n",
            "NFL direct model 84/201 completed\n",
            "Original data shape: (1912, 11)\n",
            "Flattened data shape: (1912, 11)\n",
            "Split training data: 1720 train, 192 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.42, 0.425]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.7448, Train Loss: 0.1717, Val Acc: 0.7552, Val Loss: 0.1659\n",
            "Restored model from best epoch 10 with val_loss: 0.165930\n",
            "NFL direct model 85/201 completed\n",
            "Original data shape: (1865, 11)\n",
            "Flattened data shape: (1865, 11)\n",
            "Split training data: 1678 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.425, 0.43]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.7634, Train Loss: 0.1639, Val Acc: 0.7166, Val Loss: 0.1891\n",
            "Restored model from best epoch 5 with val_loss: 0.189056\n",
            "NFL direct model 86/201 completed\n",
            "Original data shape: (1857, 11)\n",
            "Flattened data shape: (1857, 11)\n",
            "Split training data: 1671 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.43, 0.435]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.7564, Train Loss: 0.1701, Val Acc: 0.7312, Val Loss: 0.1713\n",
            "Restored model from best epoch 4 with val_loss: 0.171311\n",
            "NFL direct model 87/201 completed\n",
            "Original data shape: (1924, 11)\n",
            "Flattened data shape: (1924, 11)\n",
            "Split training data: 1731 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.435, 0.44]\n",
            "Early stopping at epoch 11\n",
            "Best epoch: 1, Train Acc: 0.6187, Train Loss: 0.2319, Val Acc: 0.6528, Val Loss: 0.2533\n",
            "Restored model from best epoch 1 with val_loss: 0.253341\n",
            "NFL direct model 88/201 completed\n",
            "Original data shape: (1819, 11)\n",
            "Flattened data shape: (1819, 11)\n",
            "Split training data: 1637 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.44, 0.445]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.7801, Train Loss: 0.1534, Val Acc: 0.7582, Val Loss: 0.1570\n",
            "Restored model from best epoch 14 with val_loss: 0.157023\n",
            "NFL direct model 89/201 completed\n",
            "Original data shape: (1899, 11)\n",
            "Flattened data shape: (1899, 11)\n",
            "Split training data: 1709 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.445, 0.45]\n",
            "Early stopping at epoch 55\n",
            "Best epoch: 45, Train Acc: 0.8344, Train Loss: 0.1202, Val Acc: 0.7947, Val Loss: 0.1377\n",
            "Restored model from best epoch 45 with val_loss: 0.137699\n",
            "NFL direct model 90/201 completed\n",
            "Original data shape: (1878, 11)\n",
            "Flattened data shape: (1878, 11)\n",
            "Split training data: 1690 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.45, 0.455]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.7627, Train Loss: 0.1682, Val Acc: 0.7553, Val Loss: 0.1669\n",
            "Restored model from best epoch 7 with val_loss: 0.166856\n",
            "NFL direct model 91/201 completed\n",
            "Original data shape: (1879, 11)\n",
            "Flattened data shape: (1879, 11)\n",
            "Split training data: 1691 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.455, 0.46]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7085, Train Loss: 0.1948, Val Acc: 0.7234, Val Loss: 0.1734\n",
            "Restored model from best epoch 2 with val_loss: 0.173353\n",
            "NFL direct model 92/201 completed\n",
            "Original data shape: (1915, 11)\n",
            "Flattened data shape: (1915, 11)\n",
            "Split training data: 1723 train, 192 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.46, 0.465]\n",
            "Early stopping at epoch 35\n",
            "Best epoch: 25, Train Acc: 0.8166, Train Loss: 0.1359, Val Acc: 0.7865, Val Loss: 0.1521\n",
            "Restored model from best epoch 25 with val_loss: 0.152082\n",
            "NFL direct model 93/201 completed\n",
            "Original data shape: (1796, 11)\n",
            "Flattened data shape: (1796, 11)\n",
            "Split training data: 1616 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.465, 0.47]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.7964, Train Loss: 0.1428, Val Acc: 0.7778, Val Loss: 0.1580\n",
            "Restored model from best epoch 15 with val_loss: 0.158043\n",
            "NFL direct model 94/201 completed\n",
            "Original data shape: (5182, 11)\n",
            "Flattened data shape: (5182, 11)\n",
            "Split training data: 4663 train, 519 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.47, 0.475]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.7896, Train Loss: 0.1447, Val Acc: 0.7707, Val Loss: 0.1474\n",
            "Restored model from best epoch 15 with val_loss: 0.147412\n",
            "NFL direct model 95/201 completed\n",
            "Original data shape: (2633, 11)\n",
            "Flattened data shape: (2633, 11)\n",
            "Split training data: 2369 train, 264 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.475, 0.48]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8041, Train Loss: 0.1565, Val Acc: 0.8068, Val Loss: 0.1466\n",
            "Restored model from best epoch 15 with val_loss: 0.146611\n",
            "NFL direct model 96/201 completed\n",
            "Original data shape: (2831, 11)\n",
            "Flattened data shape: (2831, 11)\n",
            "Split training data: 2547 train, 284 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.48, 0.485]\n",
            "Early stopping at epoch 68\n",
            "Best epoch: 58, Train Acc: 0.8394, Train Loss: 0.1171, Val Acc: 0.8239, Val Loss: 0.1241\n",
            "Restored model from best epoch 58 with val_loss: 0.124110\n",
            "NFL direct model 97/201 completed\n",
            "Original data shape: (3197, 11)\n",
            "Flattened data shape: (3197, 11)\n",
            "Split training data: 2877 train, 320 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.485, 0.49]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.7789, Train Loss: 0.1516, Val Acc: 0.7750, Val Loss: 0.1562\n",
            "Restored model from best epoch 6 with val_loss: 0.156200\n",
            "NFL direct model 98/201 completed\n",
            "Original data shape: (3857, 11)\n",
            "Flattened data shape: (3857, 11)\n",
            "Split training data: 3471 train, 386 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.49, 0.495]\n",
            "Early stopping at epoch 34\n",
            "Best epoch: 24, Train Acc: 0.8116, Train Loss: 0.1340, Val Acc: 0.7953, Val Loss: 0.1325\n",
            "Restored model from best epoch 24 with val_loss: 0.132471\n",
            "NFL direct model 99/201 completed\n",
            "Original data shape: (4634, 11)\n",
            "Flattened data shape: (4634, 11)\n",
            "Split training data: 4170 train, 464 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.495, 0.5]\n",
            "Early stopping at epoch 36\n",
            "Best epoch: 26, Train Acc: 0.8149, Train Loss: 0.1325, Val Acc: 0.7931, Val Loss: 0.1350\n",
            "Restored model from best epoch 26 with val_loss: 0.135009\n",
            "NFL direct model 100/201 completed\n",
            "Original data shape: (8430, 11)\n",
            "Flattened data shape: (8430, 11)\n",
            "Split training data: 7587 train, 843 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.5, 0.505]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8008, Train Loss: 0.1414, Val Acc: 0.7675, Val Loss: 0.1486\n",
            "Restored model from best epoch 6 with val_loss: 0.148643\n",
            "NFL direct model 101/201 completed\n",
            "Original data shape: (1093, 11)\n",
            "Flattened data shape: (1093, 11)\n",
            "Split training data: 983 train, 110 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.505, 0.51]\n",
            "Early stopping at epoch 37\n",
            "Best epoch: 27, Train Acc: 0.8342, Train Loss: 0.1253, Val Acc: 0.8182, Val Loss: 0.1363\n",
            "Restored model from best epoch 27 with val_loss: 0.136345\n",
            "NFL direct model 102/201 completed\n",
            "Original data shape: (1768, 11)\n",
            "Flattened data shape: (1768, 11)\n",
            "Split training data: 1591 train, 177 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.51, 0.515]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.8064, Train Loss: 0.1401, Val Acc: 0.7514, Val Loss: 0.1650\n",
            "Restored model from best epoch 10 with val_loss: 0.164950\n",
            "NFL direct model 103/201 completed\n",
            "Original data shape: (1533, 11)\n",
            "Flattened data shape: (1533, 11)\n",
            "Split training data: 1379 train, 154 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.515, 0.52]\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 18, Train Acc: 0.8260, Train Loss: 0.1320, Val Acc: 0.8117, Val Loss: 0.1329\n",
            "Restored model from best epoch 18 with val_loss: 0.132936\n",
            "NFL direct model 104/201 completed\n",
            "Original data shape: (1733, 11)\n",
            "Flattened data shape: (1733, 11)\n",
            "Split training data: 1559 train, 174 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.52, 0.525]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8127, Train Loss: 0.1378, Val Acc: 0.7931, Val Loss: 0.1333\n",
            "Restored model from best epoch 15 with val_loss: 0.133278\n",
            "NFL direct model 105/201 completed\n",
            "Original data shape: (1831, 11)\n",
            "Flattened data shape: (1831, 11)\n",
            "Split training data: 1647 train, 184 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.525, 0.53]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8197, Train Loss: 0.1312, Val Acc: 0.8043, Val Loss: 0.1410\n",
            "Restored model from best epoch 15 with val_loss: 0.140958\n",
            "NFL direct model 106/201 completed\n",
            "Original data shape: (1761, 11)\n",
            "Flattened data shape: (1761, 11)\n",
            "Split training data: 1584 train, 177 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.53, 0.535]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.7753, Train Loss: 0.1585, Val Acc: 0.7514, Val Loss: 0.1654\n",
            "Restored model from best epoch 3 with val_loss: 0.165446\n",
            "NFL direct model 107/201 completed\n",
            "Original data shape: (1851, 11)\n",
            "Flattened data shape: (1851, 11)\n",
            "Split training data: 1665 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.535, 0.54]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8246, Train Loss: 0.1308, Val Acc: 0.8333, Val Loss: 0.1287\n",
            "Restored model from best epoch 6 with val_loss: 0.128657\n",
            "NFL direct model 108/201 completed\n",
            "Original data shape: (1777, 11)\n",
            "Flattened data shape: (1777, 11)\n",
            "Split training data: 1599 train, 178 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.54, 0.545]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.7999, Train Loss: 0.1410, Val Acc: 0.8034, Val Loss: 0.1467\n",
            "Restored model from best epoch 9 with val_loss: 0.146747\n",
            "NFL direct model 109/201 completed\n",
            "Original data shape: (1773, 11)\n",
            "Flattened data shape: (1773, 11)\n",
            "Split training data: 1595 train, 178 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.545, 0.55]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.8157, Train Loss: 0.1325, Val Acc: 0.7921, Val Loss: 0.1532\n",
            "Restored model from best epoch 11 with val_loss: 0.153236\n",
            "NFL direct model 110/201 completed\n",
            "Original data shape: (1823, 11)\n",
            "Flattened data shape: (1823, 11)\n",
            "Split training data: 1640 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.55, 0.555]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.8293, Train Loss: 0.1249, Val Acc: 0.8033, Val Loss: 0.1337\n",
            "Restored model from best epoch 17 with val_loss: 0.133689\n",
            "NFL direct model 111/201 completed\n",
            "Original data shape: (1802, 11)\n",
            "Flattened data shape: (1802, 11)\n",
            "Split training data: 1621 train, 181 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.555, 0.56]\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 18, Train Acc: 0.8322, Train Loss: 0.1270, Val Acc: 0.8177, Val Loss: 0.1357\n",
            "Restored model from best epoch 18 with val_loss: 0.135693\n",
            "NFL direct model 112/201 completed\n",
            "Original data shape: (1822, 11)\n",
            "Flattened data shape: (1822, 11)\n",
            "Split training data: 1639 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.56, 0.565]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7797, Train Loss: 0.1579, Val Acc: 0.8306, Val Loss: 0.1332\n",
            "Restored model from best epoch 2 with val_loss: 0.133186\n",
            "NFL direct model 113/201 completed\n",
            "Original data shape: (1864, 11)\n",
            "Flattened data shape: (1864, 11)\n",
            "Split training data: 1677 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.565, 0.57]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.8187, Train Loss: 0.1291, Val Acc: 0.7861, Val Loss: 0.1438\n",
            "Restored model from best epoch 11 with val_loss: 0.143801\n",
            "NFL direct model 114/201 completed\n",
            "Original data shape: (1895, 11)\n",
            "Flattened data shape: (1895, 11)\n",
            "Split training data: 1705 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.57, 0.575]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8018, Train Loss: 0.1414, Val Acc: 0.8211, Val Loss: 0.1398\n",
            "Restored model from best epoch 3 with val_loss: 0.139841\n",
            "NFL direct model 115/201 completed\n",
            "Original data shape: (1891, 11)\n",
            "Flattened data shape: (1891, 11)\n",
            "Split training data: 1701 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.575, 0.58]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8019, Train Loss: 0.1424, Val Acc: 0.7895, Val Loss: 0.1493\n",
            "Restored model from best epoch 4 with val_loss: 0.149259\n",
            "NFL direct model 116/201 completed\n",
            "Original data shape: (1795, 11)\n",
            "Flattened data shape: (1795, 11)\n",
            "Split training data: 1615 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.58, 0.585]\n",
            "Early stopping at epoch 33\n",
            "Best epoch: 23, Train Acc: 0.8316, Train Loss: 0.1262, Val Acc: 0.8944, Val Loss: 0.0905\n",
            "Restored model from best epoch 23 with val_loss: 0.090545\n",
            "NFL direct model 117/201 completed\n",
            "Original data shape: (1820, 11)\n",
            "Flattened data shape: (1820, 11)\n",
            "Split training data: 1638 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.585, 0.59]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.7998, Train Loss: 0.1476, Val Acc: 0.8571, Val Loss: 0.1150\n",
            "Restored model from best epoch 4 with val_loss: 0.114953\n",
            "NFL direct model 118/201 completed\n",
            "Original data shape: (1927, 11)\n",
            "Flattened data shape: (1927, 11)\n",
            "Split training data: 1734 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.59, 0.595]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8057, Train Loss: 0.1416, Val Acc: 0.7513, Val Loss: 0.1385\n",
            "Restored model from best epoch 6 with val_loss: 0.138487\n",
            "NFL direct model 119/201 completed\n",
            "Original data shape: (1895, 11)\n",
            "Flattened data shape: (1895, 11)\n",
            "Split training data: 1705 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.595, 0.6]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.8217, Train Loss: 0.1342, Val Acc: 0.7895, Val Loss: 0.1491\n",
            "Restored model from best epoch 5 with val_loss: 0.149150\n",
            "NFL direct model 120/201 completed\n",
            "Original data shape: (1853, 11)\n",
            "Flattened data shape: (1853, 11)\n",
            "Split training data: 1667 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.6, 0.605]\n",
            "Early stopping at epoch 29\n",
            "Best epoch: 19, Train Acc: 0.8170, Train Loss: 0.1330, Val Acc: 0.8065, Val Loss: 0.1246\n",
            "Restored model from best epoch 19 with val_loss: 0.124596\n",
            "NFL direct model 121/201 completed\n",
            "Original data shape: (1901, 11)\n",
            "Flattened data shape: (1901, 11)\n",
            "Split training data: 1710 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.605, 0.61]\n",
            "Early stopping at epoch 35\n",
            "Best epoch: 25, Train Acc: 0.8474, Train Loss: 0.1164, Val Acc: 0.7958, Val Loss: 0.1500\n",
            "Restored model from best epoch 25 with val_loss: 0.149992\n",
            "NFL direct model 122/201 completed\n",
            "Original data shape: (1914, 11)\n",
            "Flattened data shape: (1914, 11)\n",
            "Split training data: 1722 train, 192 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.61, 0.615]\n",
            "Early stopping at epoch 37\n",
            "Best epoch: 27, Train Acc: 0.8432, Train Loss: 0.1172, Val Acc: 0.8073, Val Loss: 0.1322\n",
            "Restored model from best epoch 27 with val_loss: 0.132172\n",
            "NFL direct model 123/201 completed\n",
            "Original data shape: (1870, 11)\n",
            "Flattened data shape: (1870, 11)\n",
            "Split training data: 1683 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.615, 0.62]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.8259, Train Loss: 0.1190, Val Acc: 0.7861, Val Loss: 0.1386\n",
            "Restored model from best epoch 13 with val_loss: 0.138601\n",
            "NFL direct model 124/201 completed\n",
            "Original data shape: (1856, 11)\n",
            "Flattened data shape: (1856, 11)\n",
            "Split training data: 1670 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.62, 0.625]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.7796, Train Loss: 0.1606, Val Acc: 0.7903, Val Loss: 0.1395\n",
            "Restored model from best epoch 2 with val_loss: 0.139466\n",
            "NFL direct model 125/201 completed\n",
            "Original data shape: (1927, 11)\n",
            "Flattened data shape: (1927, 11)\n",
            "Split training data: 1734 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.625, 0.63]\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 16, Train Acc: 0.8299, Train Loss: 0.1256, Val Acc: 0.7824, Val Loss: 0.1201\n",
            "Restored model from best epoch 16 with val_loss: 0.120098\n",
            "NFL direct model 126/201 completed\n",
            "Original data shape: (1866, 11)\n",
            "Flattened data shape: (1866, 11)\n",
            "Split training data: 1679 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.63, 0.635]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8058, Train Loss: 0.1374, Val Acc: 0.7968, Val Loss: 0.1509\n",
            "Restored model from best epoch 3 with val_loss: 0.150906\n",
            "NFL direct model 127/201 completed\n",
            "Original data shape: (1857, 11)\n",
            "Flattened data shape: (1857, 11)\n",
            "Split training data: 1671 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.635, 0.64]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.8187, Train Loss: 0.1356, Val Acc: 0.8011, Val Loss: 0.1374\n",
            "Restored model from best epoch 10 with val_loss: 0.137424\n",
            "NFL direct model 128/201 completed\n",
            "Original data shape: (1778, 11)\n",
            "Flattened data shape: (1778, 11)\n",
            "Split training data: 1600 train, 178 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.64, 0.645]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.8187, Train Loss: 0.1303, Val Acc: 0.8258, Val Loss: 0.1289\n",
            "Restored model from best epoch 11 with val_loss: 0.128873\n",
            "NFL direct model 129/201 completed\n",
            "Original data shape: (1932, 11)\n",
            "Flattened data shape: (1932, 11)\n",
            "Split training data: 1738 train, 194 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.645, 0.65]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.8285, Train Loss: 0.1241, Val Acc: 0.8247, Val Loss: 0.1180\n",
            "Restored model from best epoch 14 with val_loss: 0.118029\n",
            "NFL direct model 130/201 completed\n",
            "Original data shape: (1868, 11)\n",
            "Flattened data shape: (1868, 11)\n",
            "Split training data: 1681 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.65, 0.655]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.8096, Train Loss: 0.1345, Val Acc: 0.7914, Val Loss: 0.1436\n",
            "Restored model from best epoch 5 with val_loss: 0.143603\n",
            "NFL direct model 131/201 completed\n",
            "Original data shape: (1904, 11)\n",
            "Flattened data shape: (1904, 11)\n",
            "Split training data: 1713 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.655, 0.66]\n",
            "Early stopping at epoch 32\n",
            "Best epoch: 22, Train Acc: 0.8476, Train Loss: 0.1099, Val Acc: 0.8063, Val Loss: 0.1327\n",
            "Restored model from best epoch 22 with val_loss: 0.132740\n",
            "NFL direct model 132/201 completed\n",
            "Original data shape: (1908, 11)\n",
            "Flattened data shape: (1908, 11)\n",
            "Split training data: 1717 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.66, 0.665]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.8014, Train Loss: 0.1484, Val Acc: 0.7958, Val Loss: 0.1380\n",
            "Restored model from best epoch 2 with val_loss: 0.138035\n",
            "NFL direct model 133/201 completed\n",
            "Original data shape: (1866, 11)\n",
            "Flattened data shape: (1866, 11)\n",
            "Split training data: 1679 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.665, 0.67]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8303, Train Loss: 0.1215, Val Acc: 0.8075, Val Loss: 0.1401\n",
            "Restored model from best epoch 6 with val_loss: 0.140121\n",
            "NFL direct model 134/201 completed\n",
            "Original data shape: (1877, 11)\n",
            "Flattened data shape: (1877, 11)\n",
            "Split training data: 1689 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.67, 0.675]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.8348, Train Loss: 0.1248, Val Acc: 0.8564, Val Loss: 0.1125\n",
            "Restored model from best epoch 8 with val_loss: 0.112512\n",
            "NFL direct model 135/201 completed\n",
            "Original data shape: (1822, 11)\n",
            "Flattened data shape: (1822, 11)\n",
            "Split training data: 1639 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.675, 0.68]\n",
            "Early stopping at epoch 12\n",
            "Best epoch: 2, Train Acc: 0.8017, Train Loss: 0.1502, Val Acc: 0.7705, Val Loss: 0.1453\n",
            "Restored model from best epoch 2 with val_loss: 0.145299\n",
            "NFL direct model 136/201 completed\n",
            "Original data shape: (1850, 11)\n",
            "Flattened data shape: (1850, 11)\n",
            "Split training data: 1665 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.68, 0.685]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8078, Train Loss: 0.1412, Val Acc: 0.8108, Val Loss: 0.1264\n",
            "Restored model from best epoch 3 with val_loss: 0.126386\n",
            "NFL direct model 137/201 completed\n",
            "Original data shape: (1926, 11)\n",
            "Flattened data shape: (1926, 11)\n",
            "Split training data: 1733 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.685, 0.69]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8240, Train Loss: 0.1280, Val Acc: 0.8238, Val Loss: 0.0854\n",
            "Restored model from best epoch 7 with val_loss: 0.085405\n",
            "NFL direct model 138/201 completed\n",
            "Original data shape: (1875, 11)\n",
            "Flattened data shape: (1875, 11)\n",
            "Split training data: 1687 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.69, 0.695]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8074, Train Loss: 0.1339, Val Acc: 0.8457, Val Loss: 0.1098\n",
            "Restored model from best epoch 4 with val_loss: 0.109823\n",
            "NFL direct model 139/201 completed\n",
            "Original data shape: (1849, 11)\n",
            "Flattened data shape: (1849, 11)\n",
            "Split training data: 1664 train, 185 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.695, 0.7]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8125, Train Loss: 0.1310, Val Acc: 0.8000, Val Loss: 0.1325\n",
            "Restored model from best epoch 3 with val_loss: 0.132520\n",
            "NFL direct model 140/201 completed\n",
            "Original data shape: (1902, 11)\n",
            "Flattened data shape: (1902, 11)\n",
            "Split training data: 1711 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.7, 0.705]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8440, Train Loss: 0.1110, Val Acc: 0.8272, Val Loss: 0.1253\n",
            "Restored model from best epoch 15 with val_loss: 0.125347\n",
            "NFL direct model 141/201 completed\n",
            "Original data shape: (1820, 11)\n",
            "Flattened data shape: (1820, 11)\n",
            "Split training data: 1638 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.705, 0.71]\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 18, Train Acc: 0.8578, Train Loss: 0.1047, Val Acc: 0.8571, Val Loss: 0.0947\n",
            "Restored model from best epoch 18 with val_loss: 0.094688\n",
            "NFL direct model 142/201 completed\n",
            "Original data shape: (1822, 11)\n",
            "Flattened data shape: (1822, 11)\n",
            "Split training data: 1639 train, 183 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.71, 0.715]\n",
            "Early stopping at epoch 42\n",
            "Best epoch: 32, Train Acc: 0.8652, Train Loss: 0.0973, Val Acc: 0.7705, Val Loss: 0.1626\n",
            "Restored model from best epoch 32 with val_loss: 0.162561\n",
            "NFL direct model 143/201 completed\n",
            "Original data shape: (1896, 11)\n",
            "Flattened data shape: (1896, 11)\n",
            "Split training data: 1706 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.715, 0.72]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8306, Train Loss: 0.1201, Val Acc: 0.8263, Val Loss: 0.1279\n",
            "Restored model from best epoch 7 with val_loss: 0.127882\n",
            "NFL direct model 144/201 completed\n",
            "Original data shape: (1876, 11)\n",
            "Flattened data shape: (1876, 11)\n",
            "Split training data: 1688 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.72, 0.725]\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 12, Train Acc: 0.8643, Train Loss: 0.1057, Val Acc: 0.8351, Val Loss: 0.1134\n",
            "Restored model from best epoch 12 with val_loss: 0.113369\n",
            "NFL direct model 145/201 completed\n",
            "Original data shape: (1928, 11)\n",
            "Flattened data shape: (1928, 11)\n",
            "Split training data: 1735 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.725, 0.73]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.8450, Train Loss: 0.1128, Val Acc: 0.8031, Val Loss: 0.1546\n",
            "Restored model from best epoch 9 with val_loss: 0.154602\n",
            "NFL direct model 146/201 completed\n",
            "Original data shape: (1913, 11)\n",
            "Flattened data shape: (1913, 11)\n",
            "Split training data: 1721 train, 192 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.73, 0.735]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8507, Train Loss: 0.1088, Val Acc: 0.8073, Val Loss: 0.1298\n",
            "Restored model from best epoch 15 with val_loss: 0.129821\n",
            "NFL direct model 147/201 completed\n",
            "Original data shape: (1852, 11)\n",
            "Flattened data shape: (1852, 11)\n",
            "Split training data: 1666 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.735, 0.74]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.8493, Train Loss: 0.1150, Val Acc: 0.8387, Val Loss: 0.1094\n",
            "Restored model from best epoch 11 with val_loss: 0.109351\n",
            "NFL direct model 148/201 completed\n",
            "Original data shape: (1724, 11)\n",
            "Flattened data shape: (1724, 11)\n",
            "Split training data: 1551 train, 173 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.74, 0.745]\n",
            "Early stopping at epoch 26\n",
            "Best epoch: 16, Train Acc: 0.8640, Train Loss: 0.1045, Val Acc: 0.8324, Val Loss: 0.1212\n",
            "Restored model from best epoch 16 with val_loss: 0.121176\n",
            "NFL direct model 149/201 completed\n",
            "Original data shape: (1659, 11)\n",
            "Flattened data shape: (1659, 11)\n",
            "Split training data: 1493 train, 166 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.745, 0.75]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8386, Train Loss: 0.1160, Val Acc: 0.8554, Val Loss: 0.1052\n",
            "Restored model from best epoch 6 with val_loss: 0.105169\n",
            "NFL direct model 150/201 completed\n",
            "Original data shape: (5362, 11)\n",
            "Flattened data shape: (5362, 11)\n",
            "Split training data: 4825 train, 537 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.75, 0.755]\n",
            "Early stopping at epoch 48\n",
            "Best epoch: 38, Train Acc: 0.8779, Train Loss: 0.0892, Val Acc: 0.8845, Val Loss: 0.0864\n",
            "Restored model from best epoch 38 with val_loss: 0.086356\n",
            "NFL direct model 151/201 completed\n",
            "Original data shape: (1104, 11)\n",
            "Flattened data shape: (1104, 11)\n",
            "Split training data: 993 train, 111 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.755, 0.76]\n",
            "Early stopping at epoch 22\n",
            "Best epoch: 12, Train Acc: 0.8671, Train Loss: 0.0950, Val Acc: 0.9189, Val Loss: 0.0670\n",
            "Restored model from best epoch 12 with val_loss: 0.067047\n",
            "NFL direct model 152/201 completed\n",
            "Original data shape: (1473, 11)\n",
            "Flattened data shape: (1473, 11)\n",
            "Split training data: 1325 train, 148 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.76, 0.765]\n",
            "Early stopping at epoch 15\n",
            "Best epoch: 5, Train Acc: 0.8491, Train Loss: 0.1116, Val Acc: 0.8716, Val Loss: 0.1022\n",
            "Restored model from best epoch 5 with val_loss: 0.102218\n",
            "NFL direct model 153/201 completed\n",
            "Original data shape: (1940, 11)\n",
            "Flattened data shape: (1940, 11)\n",
            "Split training data: 1746 train, 194 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.765, 0.77]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8603, Train Loss: 0.1043, Val Acc: 0.8918, Val Loss: 0.0641\n",
            "Restored model from best epoch 7 with val_loss: 0.064092\n",
            "NFL direct model 154/201 completed\n",
            "Original data shape: (1679, 11)\n",
            "Flattened data shape: (1679, 11)\n",
            "Split training data: 1511 train, 168 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.77, 0.775]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.8570, Train Loss: 0.1051, Val Acc: 0.8810, Val Loss: 0.1060\n",
            "Restored model from best epoch 10 with val_loss: 0.105993\n",
            "NFL direct model 155/201 completed\n",
            "Original data shape: (2066, 11)\n",
            "Flattened data shape: (2066, 11)\n",
            "Split training data: 1859 train, 207 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.775, 0.78]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8666, Train Loss: 0.1063, Val Acc: 0.8696, Val Loss: 0.1155\n",
            "Restored model from best epoch 7 with val_loss: 0.115485\n",
            "NFL direct model 156/201 completed\n",
            "Original data shape: (1769, 11)\n",
            "Flattened data shape: (1769, 11)\n",
            "Split training data: 1592 train, 177 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.78, 0.785]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.8549, Train Loss: 0.1065, Val Acc: 0.8531, Val Loss: 0.1058\n",
            "Restored model from best epoch 9 with val_loss: 0.105816\n",
            "NFL direct model 157/201 completed\n",
            "Original data shape: (1930, 11)\n",
            "Flattened data shape: (1930, 11)\n",
            "Split training data: 1737 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.785, 0.79]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8549, Train Loss: 0.1033, Val Acc: 0.8912, Val Loss: 0.0771\n",
            "Restored model from best epoch 7 with val_loss: 0.077124\n",
            "NFL direct model 158/201 completed\n",
            "Original data shape: (1812, 11)\n",
            "Flattened data shape: (1812, 11)\n",
            "Split training data: 1630 train, 182 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.79, 0.795]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8699, Train Loss: 0.0947, Val Acc: 0.8846, Val Loss: 0.0955\n",
            "Restored model from best epoch 7 with val_loss: 0.095517\n",
            "NFL direct model 159/201 completed\n",
            "Original data shape: (1790, 11)\n",
            "Flattened data shape: (1790, 11)\n",
            "Split training data: 1611 train, 179 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.795, 0.8]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.8597, Train Loss: 0.1037, Val Acc: 0.8436, Val Loss: 0.1176\n",
            "Restored model from best epoch 13 with val_loss: 0.117581\n",
            "NFL direct model 160/201 completed\n",
            "Original data shape: (2000, 11)\n",
            "Flattened data shape: (2000, 11)\n",
            "Split training data: 1800 train, 200 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.8, 0.805]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.8672, Train Loss: 0.0997, Val Acc: 0.8550, Val Loss: 0.0823\n",
            "Restored model from best epoch 13 with val_loss: 0.082326\n",
            "NFL direct model 161/201 completed\n",
            "Original data shape: (1921, 11)\n",
            "Flattened data shape: (1921, 11)\n",
            "Split training data: 1728 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.805, 0.81]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8490, Train Loss: 0.1117, Val Acc: 0.8342, Val Loss: 0.0889\n",
            "Restored model from best epoch 4 with val_loss: 0.088859\n",
            "NFL direct model 162/201 completed\n",
            "Original data shape: (1896, 11)\n",
            "Flattened data shape: (1896, 11)\n",
            "Split training data: 1706 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.81, 0.815]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.8687, Train Loss: 0.0954, Val Acc: 0.8684, Val Loss: 0.0958\n",
            "Restored model from best epoch 13 with val_loss: 0.095849\n",
            "NFL direct model 163/201 completed\n",
            "Original data shape: (1868, 11)\n",
            "Flattened data shape: (1868, 11)\n",
            "Split training data: 1681 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.815, 0.82]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8471, Train Loss: 0.1066, Val Acc: 0.8824, Val Loss: 0.0862\n",
            "Restored model from best epoch 4 with val_loss: 0.086220\n",
            "NFL direct model 164/201 completed\n",
            "Original data shape: (1950, 11)\n",
            "Flattened data shape: (1950, 11)\n",
            "Split training data: 1755 train, 195 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.82, 0.825]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.8712, Train Loss: 0.0924, Val Acc: 0.8872, Val Loss: 0.1359\n",
            "Restored model from best epoch 17 with val_loss: 0.135946\n",
            "NFL direct model 165/201 completed\n",
            "Original data shape: (1799, 11)\n",
            "Flattened data shape: (1799, 11)\n",
            "Split training data: 1619 train, 180 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.825, 0.83]\n",
            "Early stopping at epoch 33\n",
            "Best epoch: 23, Train Acc: 0.8863, Train Loss: 0.0883, Val Acc: 0.8222, Val Loss: 0.1129\n",
            "Restored model from best epoch 23 with val_loss: 0.112942\n",
            "NFL direct model 166/201 completed\n",
            "Original data shape: (1875, 11)\n",
            "Flattened data shape: (1875, 11)\n",
            "Split training data: 1687 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.83, 0.835]\n",
            "Early stopping at epoch 42\n",
            "Best epoch: 32, Train Acc: 0.8951, Train Loss: 0.0789, Val Acc: 0.8936, Val Loss: 0.0861\n",
            "Restored model from best epoch 32 with val_loss: 0.086135\n",
            "NFL direct model 167/201 completed\n",
            "Original data shape: (1884, 11)\n",
            "Flattened data shape: (1884, 11)\n",
            "Split training data: 1695 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.835, 0.84]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.8796, Train Loss: 0.0913, Val Acc: 0.8836, Val Loss: 0.0865\n",
            "Restored model from best epoch 6 with val_loss: 0.086547\n",
            "NFL direct model 168/201 completed\n",
            "Original data shape: (1875, 11)\n",
            "Flattened data shape: (1875, 11)\n",
            "Split training data: 1687 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.84, 0.845]\n",
            "Early stopping at epoch 47\n",
            "Best epoch: 37, Train Acc: 0.9105, Train Loss: 0.0662, Val Acc: 0.8617, Val Loss: 0.0992\n",
            "Restored model from best epoch 37 with val_loss: 0.099192\n",
            "NFL direct model 169/201 completed\n",
            "Original data shape: (1908, 11)\n",
            "Flattened data shape: (1908, 11)\n",
            "Split training data: 1717 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.845, 0.85]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.8579, Train Loss: 0.1013, Val Acc: 0.8482, Val Loss: 0.1098\n",
            "Restored model from best epoch 8 with val_loss: 0.109791\n",
            "NFL direct model 170/201 completed\n",
            "Original data shape: (1899, 11)\n",
            "Flattened data shape: (1899, 11)\n",
            "Split training data: 1709 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.85, 0.855]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.8765, Train Loss: 0.0922, Val Acc: 0.8895, Val Loss: 0.0743\n",
            "Restored model from best epoch 8 with val_loss: 0.074276\n",
            "NFL direct model 171/201 completed\n",
            "Original data shape: (1851, 11)\n",
            "Flattened data shape: (1851, 11)\n",
            "Split training data: 1665 train, 186 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.855, 0.86]\n",
            "Early stopping at epoch 25\n",
            "Best epoch: 15, Train Acc: 0.8715, Train Loss: 0.0893, Val Acc: 0.8656, Val Loss: 0.0997\n",
            "Restored model from best epoch 15 with val_loss: 0.099718\n",
            "NFL direct model 172/201 completed\n",
            "Original data shape: (1899, 11)\n",
            "Flattened data shape: (1899, 11)\n",
            "Split training data: 1709 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.86, 0.865]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8520, Train Loss: 0.1085, Val Acc: 0.8684, Val Loss: 0.0934\n",
            "Restored model from best epoch 3 with val_loss: 0.093356\n",
            "NFL direct model 173/201 completed\n",
            "Original data shape: (1866, 11)\n",
            "Flattened data shape: (1866, 11)\n",
            "Split training data: 1679 train, 187 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.865, 0.87]\n",
            "Early stopping at epoch 21\n",
            "Best epoch: 11, Train Acc: 0.8916, Train Loss: 0.0850, Val Acc: 0.9144, Val Loss: 0.0583\n",
            "Restored model from best epoch 11 with val_loss: 0.058309\n",
            "NFL direct model 174/201 completed\n",
            "Original data shape: (1874, 11)\n",
            "Flattened data shape: (1874, 11)\n",
            "Split training data: 1686 train, 188 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.87, 0.875]\n",
            "Early stopping at epoch 18\n",
            "Best epoch: 8, Train Acc: 0.8701, Train Loss: 0.0926, Val Acc: 0.8457, Val Loss: 0.0965\n",
            "Restored model from best epoch 8 with val_loss: 0.096456\n",
            "NFL direct model 175/201 completed\n",
            "Original data shape: (1933, 11)\n",
            "Flattened data shape: (1933, 11)\n",
            "Split training data: 1739 train, 194 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.875, 0.88]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8649, Train Loss: 0.1001, Val Acc: 0.8918, Val Loss: 0.0710\n",
            "Restored model from best epoch 4 with val_loss: 0.070991\n",
            "NFL direct model 176/201 completed\n",
            "Original data shape: (1894, 11)\n",
            "Flattened data shape: (1894, 11)\n",
            "Split training data: 1704 train, 190 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.88, 0.885]\n",
            "Early stopping at epoch 20\n",
            "Best epoch: 10, Train Acc: 0.8773, Train Loss: 0.0881, Val Acc: 0.8632, Val Loss: 0.0932\n",
            "Restored model from best epoch 10 with val_loss: 0.093176\n",
            "NFL direct model 177/201 completed\n",
            "Original data shape: (1902, 11)\n",
            "Flattened data shape: (1902, 11)\n",
            "Split training data: 1711 train, 191 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.885, 0.89]\n",
            "Early stopping at epoch 51\n",
            "Best epoch: 41, Train Acc: 0.9106, Train Loss: 0.0651, Val Acc: 0.8534, Val Loss: 0.0894\n",
            "Restored model from best epoch 41 with val_loss: 0.089429\n",
            "NFL direct model 178/201 completed\n",
            "Original data shape: (1929, 11)\n",
            "Flattened data shape: (1929, 11)\n",
            "Split training data: 1736 train, 193 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.89, 0.895]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.8906, Train Loss: 0.0820, Val Acc: 0.8756, Val Loss: 0.0680\n",
            "Restored model from best epoch 13 with val_loss: 0.067962\n",
            "NFL direct model 179/201 completed\n",
            "Original data shape: (1948, 11)\n",
            "Flattened data shape: (1948, 11)\n",
            "Split training data: 1753 train, 195 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.895, 0.9]\n",
            "Early stopping at epoch 33\n",
            "Best epoch: 23, Train Acc: 0.9076, Train Loss: 0.0702, Val Acc: 0.8667, Val Loss: 0.0722\n",
            "Restored model from best epoch 23 with val_loss: 0.072244\n",
            "NFL direct model 180/201 completed\n",
            "Original data shape: (1887, 11)\n",
            "Flattened data shape: (1887, 11)\n",
            "Split training data: 1698 train, 189 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.9, 0.905]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8628, Train Loss: 0.0991, Val Acc: 0.8095, Val Loss: 0.1272\n",
            "Restored model from best epoch 4 with val_loss: 0.127171\n",
            "NFL direct model 181/201 completed\n",
            "Original data shape: (1935, 11)\n",
            "Flattened data shape: (1935, 11)\n",
            "Split training data: 1741 train, 194 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.905, 0.91]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.8851, Train Loss: 0.0839, Val Acc: 0.8763, Val Loss: 0.0672\n",
            "Restored model from best epoch 7 with val_loss: 0.067153\n",
            "NFL direct model 182/201 completed\n",
            "Original data shape: (1966, 11)\n",
            "Flattened data shape: (1966, 11)\n",
            "Split training data: 1769 train, 197 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.91, 0.915]\n",
            "Early stopping at epoch 47\n",
            "Best epoch: 37, Train Acc: 0.9231, Train Loss: 0.0586, Val Acc: 0.8731, Val Loss: 0.0742\n",
            "Restored model from best epoch 37 with val_loss: 0.074152\n",
            "NFL direct model 183/201 completed\n",
            "Original data shape: (1959, 11)\n",
            "Flattened data shape: (1959, 11)\n",
            "Split training data: 1763 train, 196 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.915, 0.92]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8843, Train Loss: 0.0851, Val Acc: 0.8980, Val Loss: 0.0620\n",
            "Restored model from best epoch 4 with val_loss: 0.062033\n",
            "NFL direct model 184/201 completed\n",
            "Original data shape: (2080, 11)\n",
            "Flattened data shape: (2080, 11)\n",
            "Split training data: 1872 train, 208 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.92, 0.925]\n",
            "Early stopping at epoch 33\n",
            "Best epoch: 23, Train Acc: 0.9087, Train Loss: 0.0683, Val Acc: 0.8606, Val Loss: 0.0964\n",
            "Restored model from best epoch 23 with val_loss: 0.096366\n",
            "NFL direct model 185/201 completed\n",
            "Original data shape: (2162, 11)\n",
            "Flattened data shape: (2162, 11)\n",
            "Split training data: 1945 train, 217 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.925, 0.93]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8679, Train Loss: 0.0992, Val Acc: 0.8986, Val Loss: 0.0722\n",
            "Restored model from best epoch 3 with val_loss: 0.072249\n",
            "NFL direct model 186/201 completed\n",
            "Original data shape: (2135, 11)\n",
            "Flattened data shape: (2135, 11)\n",
            "Split training data: 1921 train, 214 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.93, 0.935]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8824, Train Loss: 0.0825, Val Acc: 0.8692, Val Loss: 0.0809\n",
            "Restored model from best epoch 4 with val_loss: 0.080906\n",
            "NFL direct model 187/201 completed\n",
            "Original data shape: (2280, 11)\n",
            "Flattened data shape: (2280, 11)\n",
            "Split training data: 2052 train, 228 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.935, 0.94]\n",
            "Early stopping at epoch 24\n",
            "Best epoch: 14, Train Acc: 0.8947, Train Loss: 0.0744, Val Acc: 0.8860, Val Loss: 0.0760\n",
            "Restored model from best epoch 14 with val_loss: 0.076007\n",
            "NFL direct model 188/201 completed\n",
            "Original data shape: (2447, 11)\n",
            "Flattened data shape: (2447, 11)\n",
            "Split training data: 2202 train, 245 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.94, 0.945]\n",
            "Early stopping at epoch 19\n",
            "Best epoch: 9, Train Acc: 0.9074, Train Loss: 0.0687, Val Acc: 0.9061, Val Loss: 0.0714\n",
            "Restored model from best epoch 9 with val_loss: 0.071415\n",
            "NFL direct model 189/201 completed\n",
            "Original data shape: (2443, 11)\n",
            "Flattened data shape: (2443, 11)\n",
            "Split training data: 2198 train, 245 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.945, 0.95]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8881, Train Loss: 0.0806, Val Acc: 0.8980, Val Loss: 0.0712\n",
            "Restored model from best epoch 4 with val_loss: 0.071210\n",
            "NFL direct model 190/201 completed\n",
            "Original data shape: (2496, 11)\n",
            "Flattened data shape: (2496, 11)\n",
            "Split training data: 2246 train, 250 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.95, 0.955]\n",
            "Early stopping at epoch 13\n",
            "Best epoch: 3, Train Acc: 0.8682, Train Loss: 0.0962, Val Acc: 0.9040, Val Loss: 0.0632\n",
            "Restored model from best epoch 3 with val_loss: 0.063234\n",
            "NFL direct model 191/201 completed\n",
            "Original data shape: (2360, 11)\n",
            "Flattened data shape: (2360, 11)\n",
            "Split training data: 2124 train, 236 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.955, 0.96]\n",
            "Early stopping at epoch 14\n",
            "Best epoch: 4, Train Acc: 0.8719, Train Loss: 0.0892, Val Acc: 0.9025, Val Loss: 0.0745\n",
            "Restored model from best epoch 4 with val_loss: 0.074477\n",
            "NFL direct model 192/201 completed\n",
            "Original data shape: (2509, 11)\n",
            "Flattened data shape: (2509, 11)\n",
            "Split training data: 2258 train, 251 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.96, 0.965]\n",
            "Early stopping at epoch 17\n",
            "Best epoch: 7, Train Acc: 0.9004, Train Loss: 0.0741, Val Acc: 0.8765, Val Loss: 0.0798\n",
            "Restored model from best epoch 7 with val_loss: 0.079777\n",
            "NFL direct model 193/201 completed\n",
            "Original data shape: (2284, 11)\n",
            "Flattened data shape: (2284, 11)\n",
            "Split training data: 2055 train, 229 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.965, 0.97]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.9178, Train Loss: 0.0586, Val Acc: 0.9214, Val Loss: 0.0683\n",
            "Restored model from best epoch 17 with val_loss: 0.068342\n",
            "NFL direct model 194/201 completed\n",
            "Original data shape: (5301, 11)\n",
            "Flattened data shape: (5301, 11)\n",
            "Split training data: 4770 train, 531 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.97, 0.975]\n",
            "Early stopping at epoch 16\n",
            "Best epoch: 6, Train Acc: 0.9004, Train Loss: 0.0742, Val Acc: 0.8983, Val Loss: 0.0819\n",
            "Restored model from best epoch 6 with val_loss: 0.081931\n",
            "NFL direct model 195/201 completed\n",
            "Original data shape: (2461, 11)\n",
            "Flattened data shape: (2461, 11)\n",
            "Split training data: 2214 train, 247 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.975, 0.98]\n",
            "Early stopping at epoch 27\n",
            "Best epoch: 17, Train Acc: 0.9178, Train Loss: 0.0659, Val Acc: 0.9069, Val Loss: 0.0709\n",
            "Restored model from best epoch 17 with val_loss: 0.070863\n",
            "NFL direct model 196/201 completed\n",
            "Original data shape: (2569, 11)\n",
            "Flattened data shape: (2569, 11)\n",
            "Split training data: 2312 train, 257 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.98, 0.985]\n",
            "Early stopping at epoch 28\n",
            "Best epoch: 18, Train Acc: 0.9234, Train Loss: 0.0536, Val Acc: 0.8872, Val Loss: 0.0628\n",
            "Restored model from best epoch 18 with val_loss: 0.062811\n",
            "NFL direct model 197/201 completed\n",
            "Original data shape: (2483, 11)\n",
            "Flattened data shape: (2483, 11)\n",
            "Split training data: 2234 train, 249 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.985, 0.99]\n",
            "Early stopping at epoch 47\n",
            "Best epoch: 37, Train Acc: 0.9463, Train Loss: 0.0421, Val Acc: 0.9157, Val Loss: 0.0642\n",
            "Restored model from best epoch 37 with val_loss: 0.064200\n",
            "NFL direct model 198/201 completed\n",
            "Original data shape: (2548, 11)\n",
            "Flattened data shape: (2548, 11)\n",
            "Split training data: 2293 train, 255 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.99, 0.995]\n",
            "Early stopping at epoch 44\n",
            "Best epoch: 34, Train Acc: 0.9328, Train Loss: 0.0501, Val Acc: 0.8902, Val Loss: 0.0802\n",
            "Restored model from best epoch 34 with val_loss: 0.080231\n",
            "NFL direct model 199/201 completed\n",
            "Original data shape: (2520, 11)\n",
            "Flattened data shape: (2520, 11)\n",
            "Split training data: 2268 train, 252 validation\n",
            "\n",
            "Training direct prediction model for timestep range [0.995, 1.0]\n",
            "Early stopping at epoch 37\n",
            "Best epoch: 27, Train Acc: 0.9334, Train Loss: 0.0491, Val Acc: 0.9206, Val Loss: 0.0603\n",
            "Restored model from best epoch 27 with val_loss: 0.060257\n",
            "NFL direct model 200/201 completed\n",
            "Original data shape: (3111, 11)\n",
            "Flattened data shape: (3111, 11)\n",
            "Split training data: 2799 train, 312 validation\n",
            "\n",
            "Training direct prediction model for timestep range [1.0, 1.005]\n",
            "Early stopping at epoch 23\n",
            "Best epoch: 13, Train Acc: 0.9482, Train Loss: 0.0384, Val Acc: 0.9423, Val Loss: 0.0387\n",
            "Restored model from best epoch 13 with val_loss: 0.038687\n",
            "NFL direct model 201/201 completed\n"
          ]
        }
      ],
      "source": [
        "all_models[\"nn\"] = setup_direct_models(training_data, None, num_models=201)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing timestep: 0.0\n",
            "Timestep 0.00% : Training Loss = 0.6127, Accuracy = 0.6620, Test Loss = 0.6229, Test Accuracy = 0.6554\n",
            "Processing timestep: 0.005\n",
            "Timestep 0.50% : Training Loss = 0.6122, Accuracy = 0.6611, Test Loss = 0.6818, Test Accuracy = 0.5918\n",
            "Processing timestep: 0.01\n",
            "Timestep 1.00% : Training Loss = 0.6109, Accuracy = 0.6559, Test Loss = 0.6020, Test Accuracy = 0.6923\n",
            "Processing timestep: 0.015\n",
            "Timestep 1.50% : Training Loss = 0.6298, Accuracy = 0.6444, Test Loss = 0.6012, Test Accuracy = 0.7013\n",
            "Processing timestep: 0.02\n",
            "Timestep 2.00% : Training Loss = 0.6221, Accuracy = 0.6616, Test Loss = 0.6487, Test Accuracy = 0.6124\n",
            "Processing timestep: 0.025\n",
            "Timestep 2.50% : Training Loss = 0.6254, Accuracy = 0.6521, Test Loss = 0.6254, Test Accuracy = 0.6579\n",
            "Processing timestep: 0.03\n",
            "Timestep 3.00% : Training Loss = 0.6115, Accuracy = 0.6697, Test Loss = 0.6171, Test Accuracy = 0.6759\n",
            "Processing timestep: 0.035\n",
            "Timestep 3.50% : Training Loss = 0.6238, Accuracy = 0.6499, Test Loss = 0.6202, Test Accuracy = 0.6327\n",
            "Processing timestep: 0.04\n",
            "Timestep 4.00% : Training Loss = 0.6110, Accuracy = 0.6583, Test Loss = 0.6053, Test Accuracy = 0.6832\n",
            "Processing timestep: 0.045\n",
            "Timestep 4.50% : Training Loss = 0.6158, Accuracy = 0.6562, Test Loss = 0.6178, Test Accuracy = 0.6643\n",
            "Processing timestep: 0.05\n",
            "Timestep 5.00% : Training Loss = 0.6149, Accuracy = 0.6701, Test Loss = 0.6102, Test Accuracy = 0.6765\n",
            "Processing timestep: 0.055\n",
            "Timestep 5.50% : Training Loss = 0.6057, Accuracy = 0.6828, Test Loss = 0.6447, Test Accuracy = 0.6236\n",
            "Processing timestep: 0.06\n",
            "Timestep 6.00% : Training Loss = 0.5931, Accuracy = 0.6807, Test Loss = 0.6930, Test Accuracy = 0.6035\n",
            "Processing timestep: 0.065\n",
            "Timestep 6.50% : Training Loss = 0.6189, Accuracy = 0.6589, Test Loss = 0.5994, Test Accuracy = 0.6825\n",
            "Processing timestep: 0.07\n",
            "Timestep 7.00% : Training Loss = 0.6131, Accuracy = 0.6761, Test Loss = 0.5559, Test Accuracy = 0.7000\n",
            "Processing timestep: 0.075\n",
            "Timestep 7.50% : Training Loss = 0.6082, Accuracy = 0.6678, Test Loss = 0.6254, Test Accuracy = 0.6397\n",
            "Processing timestep: 0.08\n",
            "Timestep 8.00% : Training Loss = 0.5962, Accuracy = 0.6852, Test Loss = 0.5894, Test Accuracy = 0.6915\n",
            "Processing timestep: 0.085\n",
            "Timestep 8.50% : Training Loss = 0.6077, Accuracy = 0.6708, Test Loss = 0.6122, Test Accuracy = 0.6608\n",
            "Processing timestep: 0.09\n",
            "Timestep 9.00% : Training Loss = 0.6142, Accuracy = 0.6662, Test Loss = 0.5949, Test Accuracy = 0.6852\n",
            "Processing timestep: 0.095\n",
            "Timestep 9.50% : Training Loss = 0.6074, Accuracy = 0.6677, Test Loss = 0.6394, Test Accuracy = 0.6219\n",
            "Processing timestep: 0.1\n",
            "Timestep 10.00% : Training Loss = 0.5902, Accuracy = 0.6898, Test Loss = 0.6095, Test Accuracy = 0.6655\n",
            "Processing timestep: 0.105\n",
            "Timestep 10.50% : Training Loss = 0.5927, Accuracy = 0.6780, Test Loss = 0.5994, Test Accuracy = 0.6582\n",
            "Processing timestep: 0.11\n",
            "Timestep 11.00% : Training Loss = 0.6064, Accuracy = 0.6729, Test Loss = 0.6084, Test Accuracy = 0.6866\n",
            "Processing timestep: 0.115\n",
            "Timestep 11.50% : Training Loss = 0.5872, Accuracy = 0.6908, Test Loss = 0.5567, Test Accuracy = 0.7143\n",
            "Processing timestep: 0.12\n",
            "Timestep 12.00% : Training Loss = 0.5953, Accuracy = 0.6860, Test Loss = 0.6010, Test Accuracy = 0.6572\n",
            "Processing timestep: 0.125\n",
            "Timestep 12.50% : Training Loss = 0.5753, Accuracy = 0.6884, Test Loss = 0.5942, Test Accuracy = 0.6947\n",
            "Processing timestep: 0.13\n",
            "Timestep 13.00% : Training Loss = 0.5788, Accuracy = 0.6939, Test Loss = 0.6120, Test Accuracy = 0.6738\n",
            "Processing timestep: 0.135\n",
            "Timestep 13.50% : Training Loss = 0.5985, Accuracy = 0.6754, Test Loss = 0.6021, Test Accuracy = 0.6823\n",
            "Processing timestep: 0.14\n",
            "Timestep 14.00% : Training Loss = 0.5821, Accuracy = 0.6941, Test Loss = 0.5856, Test Accuracy = 0.7212\n",
            "Processing timestep: 0.145\n",
            "Timestep 14.50% : Training Loss = 0.5861, Accuracy = 0.6869, Test Loss = 0.5315, Test Accuracy = 0.7319\n",
            "Processing timestep: 0.15\n",
            "Timestep 15.00% : Training Loss = 0.5797, Accuracy = 0.7074, Test Loss = 0.5833, Test Accuracy = 0.6958\n",
            "Processing timestep: 0.155\n",
            "Timestep 15.50% : Training Loss = 0.5931, Accuracy = 0.6938, Test Loss = 0.5652, Test Accuracy = 0.6958\n",
            "Processing timestep: 0.16\n",
            "Timestep 16.00% : Training Loss = 0.5785, Accuracy = 0.6949, Test Loss = 0.5799, Test Accuracy = 0.6803\n",
            "Processing timestep: 0.165\n",
            "Timestep 16.50% : Training Loss = 0.5898, Accuracy = 0.6939, Test Loss = 0.6029, Test Accuracy = 0.6418\n",
            "Processing timestep: 0.17\n",
            "Timestep 17.00% : Training Loss = 0.5718, Accuracy = 0.7122, Test Loss = 0.6186, Test Accuracy = 0.6534\n",
            "Processing timestep: 0.175\n",
            "Timestep 17.50% : Training Loss = 0.5960, Accuracy = 0.6810, Test Loss = 0.5677, Test Accuracy = 0.7118\n",
            "Processing timestep: 0.18\n",
            "Timestep 18.00% : Training Loss = 0.5759, Accuracy = 0.6955, Test Loss = 0.5377, Test Accuracy = 0.7296\n",
            "Processing timestep: 0.185\n",
            "Timestep 18.50% : Training Loss = 0.5932, Accuracy = 0.6808, Test Loss = 0.5131, Test Accuracy = 0.7491\n",
            "Processing timestep: 0.19\n",
            "Timestep 19.00% : Training Loss = 0.5568, Accuracy = 0.7102, Test Loss = 0.6004, Test Accuracy = 0.6889\n",
            "Processing timestep: 0.195\n",
            "Timestep 19.50% : Training Loss = 0.5694, Accuracy = 0.6994, Test Loss = 0.5389, Test Accuracy = 0.7117\n",
            "Processing timestep: 0.2\n",
            "Timestep 20.00% : Training Loss = 0.5783, Accuracy = 0.6986, Test Loss = 0.5654, Test Accuracy = 0.6886\n",
            "Processing timestep: 0.205\n",
            "Timestep 20.50% : Training Loss = 0.5627, Accuracy = 0.7015, Test Loss = 0.5778, Test Accuracy = 0.6739\n",
            "Processing timestep: 0.21\n",
            "Timestep 21.00% : Training Loss = 0.5583, Accuracy = 0.7125, Test Loss = 0.5662, Test Accuracy = 0.7158\n",
            "Processing timestep: 0.215\n",
            "Timestep 21.50% : Training Loss = 0.5586, Accuracy = 0.7087, Test Loss = 0.5856, Test Accuracy = 0.7159\n",
            "Processing timestep: 0.22\n",
            "Timestep 22.00% : Training Loss = 0.5521, Accuracy = 0.7254, Test Loss = 0.5748, Test Accuracy = 0.7021\n",
            "Processing timestep: 0.225\n",
            "Timestep 22.50% : Training Loss = 0.5547, Accuracy = 0.7136, Test Loss = 0.5558, Test Accuracy = 0.6982\n",
            "Processing timestep: 0.23\n",
            "Timestep 23.00% : Training Loss = 0.5464, Accuracy = 0.7175, Test Loss = 0.5368, Test Accuracy = 0.7116\n",
            "Processing timestep: 0.235\n",
            "Timestep 23.50% : Training Loss = 0.5421, Accuracy = 0.7228, Test Loss = 0.5749, Test Accuracy = 0.6923\n",
            "Processing timestep: 0.24\n",
            "Timestep 24.00% : Training Loss = 0.5733, Accuracy = 0.6941, Test Loss = 0.5688, Test Accuracy = 0.7000\n",
            "Processing timestep: 0.245\n",
            "Timestep 24.50% : Training Loss = 0.5537, Accuracy = 0.7190, Test Loss = 0.5801, Test Accuracy = 0.6850\n",
            "Processing timestep: 0.25\n",
            "Timestep 25.00% : Training Loss = 0.5479, Accuracy = 0.7118, Test Loss = 0.5435, Test Accuracy = 0.7203\n",
            "Processing timestep: 0.255\n",
            "Timestep 25.50% : Training Loss = 0.5505, Accuracy = 0.6997, Test Loss = 0.5217, Test Accuracy = 0.7256\n",
            "Processing timestep: 0.26\n",
            "Timestep 26.00% : Training Loss = 0.5659, Accuracy = 0.7034, Test Loss = 0.5081, Test Accuracy = 0.7639\n",
            "Processing timestep: 0.265\n",
            "Timestep 26.50% : Training Loss = 0.5366, Accuracy = 0.7227, Test Loss = 0.5708, Test Accuracy = 0.7010\n",
            "Processing timestep: 0.27\n",
            "Timestep 27.00% : Training Loss = 0.5330, Accuracy = 0.7379, Test Loss = 0.5920, Test Accuracy = 0.6808\n",
            "Processing timestep: 0.275\n",
            "Timestep 27.50% : Training Loss = 0.5457, Accuracy = 0.7005, Test Loss = 0.5843, Test Accuracy = 0.6554\n",
            "Processing timestep: 0.28\n",
            "Timestep 28.00% : Training Loss = 0.5438, Accuracy = 0.7178, Test Loss = 0.5598, Test Accuracy = 0.7082\n",
            "Processing timestep: 0.285\n",
            "Timestep 28.50% : Training Loss = 0.5371, Accuracy = 0.7344, Test Loss = 0.5478, Test Accuracy = 0.6980\n",
            "Processing timestep: 0.29\n",
            "Timestep 29.00% : Training Loss = 0.5430, Accuracy = 0.7114, Test Loss = 0.5636, Test Accuracy = 0.6875\n",
            "Processing timestep: 0.295\n",
            "Timestep 29.50% : Training Loss = 0.5412, Accuracy = 0.7171, Test Loss = 0.5437, Test Accuracy = 0.6868\n",
            "Processing timestep: 0.3\n",
            "Timestep 30.00% : Training Loss = 0.5447, Accuracy = 0.7153, Test Loss = 0.5640, Test Accuracy = 0.7148\n",
            "Processing timestep: 0.305\n",
            "Timestep 30.50% : Training Loss = 0.5363, Accuracy = 0.7181, Test Loss = 0.5264, Test Accuracy = 0.7208\n",
            "Processing timestep: 0.31\n",
            "Timestep 31.00% : Training Loss = 0.5242, Accuracy = 0.7260, Test Loss = 0.5998, Test Accuracy = 0.6561\n",
            "Processing timestep: 0.315\n",
            "Timestep 31.50% : Training Loss = 0.5692, Accuracy = 0.6905, Test Loss = 0.5353, Test Accuracy = 0.7107\n",
            "Processing timestep: 0.32\n",
            "Timestep 32.00% : Training Loss = 0.5129, Accuracy = 0.7403, Test Loss = 0.5283, Test Accuracy = 0.7032\n",
            "Processing timestep: 0.325\n",
            "Timestep 32.50% : Training Loss = 0.5272, Accuracy = 0.7309, Test Loss = 0.5385, Test Accuracy = 0.7085\n",
            "Processing timestep: 0.33\n",
            "Timestep 33.00% : Training Loss = 0.5231, Accuracy = 0.7312, Test Loss = 0.5122, Test Accuracy = 0.7286\n",
            "Processing timestep: 0.335\n",
            "Timestep 33.50% : Training Loss = 0.5208, Accuracy = 0.7458, Test Loss = 0.5230, Test Accuracy = 0.7123\n",
            "Processing timestep: 0.34\n",
            "Timestep 34.00% : Training Loss = 0.5146, Accuracy = 0.7362, Test Loss = 0.5522, Test Accuracy = 0.6920\n",
            "Processing timestep: 0.345\n",
            "Timestep 34.50% : Training Loss = 0.4953, Accuracy = 0.7520, Test Loss = 0.5503, Test Accuracy = 0.7345\n",
            "Processing timestep: 0.35\n",
            "Timestep 35.00% : Training Loss = 0.5163, Accuracy = 0.7413, Test Loss = 0.5186, Test Accuracy = 0.7289\n",
            "Processing timestep: 0.355\n",
            "Timestep 35.50% : Training Loss = 0.5270, Accuracy = 0.7258, Test Loss = 0.4870, Test Accuracy = 0.7589\n",
            "Processing timestep: 0.36\n",
            "Timestep 36.00% : Training Loss = 0.5430, Accuracy = 0.7297, Test Loss = 0.5270, Test Accuracy = 0.7138\n",
            "Processing timestep: 0.365\n",
            "Timestep 36.50% : Training Loss = 0.5056, Accuracy = 0.7480, Test Loss = 0.5009, Test Accuracy = 0.7456\n",
            "Processing timestep: 0.37\n",
            "Timestep 37.00% : Training Loss = 0.5300, Accuracy = 0.7424, Test Loss = 0.5624, Test Accuracy = 0.7426\n",
            "Processing timestep: 0.375\n",
            "Timestep 37.50% : Training Loss = 0.5076, Accuracy = 0.7378, Test Loss = 0.5342, Test Accuracy = 0.7279\n",
            "Processing timestep: 0.38\n",
            "Timestep 38.00% : Training Loss = 0.5034, Accuracy = 0.7519, Test Loss = 0.5018, Test Accuracy = 0.7428\n",
            "Processing timestep: 0.385\n",
            "Timestep 38.50% : Training Loss = 0.5060, Accuracy = 0.7430, Test Loss = 0.5504, Test Accuracy = 0.7004\n",
            "Processing timestep: 0.39\n",
            "Timestep 39.00% : Training Loss = 0.5111, Accuracy = 0.7358, Test Loss = 0.5233, Test Accuracy = 0.7338\n",
            "Processing timestep: 0.395\n",
            "Timestep 39.50% : Training Loss = 0.5127, Accuracy = 0.7387, Test Loss = 0.4965, Test Accuracy = 0.7368\n",
            "Processing timestep: 0.4\n",
            "Timestep 40.00% : Training Loss = 0.5196, Accuracy = 0.7381, Test Loss = 0.5558, Test Accuracy = 0.7097\n",
            "Processing timestep: 0.405\n",
            "Timestep 40.50% : Training Loss = 0.4817, Accuracy = 0.7765, Test Loss = 0.5335, Test Accuracy = 0.6981\n",
            "Processing timestep: 0.41\n",
            "Timestep 41.00% : Training Loss = 0.5174, Accuracy = 0.7329, Test Loss = 0.4777, Test Accuracy = 0.7924\n",
            "Processing timestep: 0.415\n",
            "Timestep 41.50% : Training Loss = 0.5095, Accuracy = 0.7468, Test Loss = 0.4955, Test Accuracy = 0.7563\n",
            "Processing timestep: 0.42\n",
            "Timestep 42.00% : Training Loss = 0.5164, Accuracy = 0.7422, Test Loss = 0.5130, Test Accuracy = 0.7456\n",
            "Processing timestep: 0.425\n",
            "Timestep 42.50% : Training Loss = 0.4906, Accuracy = 0.7697, Test Loss = 0.5538, Test Accuracy = 0.7071\n",
            "Processing timestep: 0.43\n",
            "Timestep 43.00% : Training Loss = 0.4895, Accuracy = 0.7636, Test Loss = 0.5144, Test Accuracy = 0.7133\n",
            "Processing timestep: 0.435\n",
            "Timestep 43.50% : Training Loss = 0.4928, Accuracy = 0.7645, Test Loss = 0.5149, Test Accuracy = 0.7163\n",
            "Processing timestep: 0.44\n",
            "Timestep 44.00% : Training Loss = 0.4918, Accuracy = 0.7510, Test Loss = 0.5204, Test Accuracy = 0.7179\n",
            "Processing timestep: 0.445\n",
            "Timestep 44.50% : Training Loss = 0.4925, Accuracy = 0.7627, Test Loss = 0.4706, Test Accuracy = 0.7474\n",
            "Processing timestep: 0.45\n",
            "Timestep 45.00% : Training Loss = 0.4947, Accuracy = 0.7594, Test Loss = 0.5183, Test Accuracy = 0.7589\n",
            "Processing timestep: 0.455\n",
            "Timestep 45.50% : Training Loss = 0.4984, Accuracy = 0.7627, Test Loss = 0.5476, Test Accuracy = 0.6915\n",
            "Processing timestep: 0.46\n",
            "Timestep 46.00% : Training Loss = 0.4866, Accuracy = 0.7695, Test Loss = 0.4592, Test Accuracy = 0.7951\n",
            "Processing timestep: 0.465\n",
            "Timestep 46.50% : Training Loss = 0.4702, Accuracy = 0.7798, Test Loss = 0.4818, Test Accuracy = 0.7704\n",
            "Processing timestep: 0.47\n",
            "Timestep 47.00% : Training Loss = 0.4789, Accuracy = 0.7700, Test Loss = 0.4792, Test Accuracy = 0.7609\n",
            "Processing timestep: 0.475\n",
            "Timestep 47.50% : Training Loss = 0.4606, Accuracy = 0.7757, Test Loss = 0.4346, Test Accuracy = 0.8000\n",
            "Processing timestep: 0.48\n",
            "Timestep 48.00% : Training Loss = 0.4672, Accuracy = 0.7743, Test Loss = 0.4521, Test Accuracy = 0.7718\n",
            "Processing timestep: 0.485\n",
            "Timestep 48.50% : Training Loss = 0.4592, Accuracy = 0.7814, Test Loss = 0.4917, Test Accuracy = 0.7646\n",
            "Processing timestep: 0.49\n",
            "Timestep 49.00% : Training Loss = 0.4641, Accuracy = 0.7831, Test Loss = 0.4672, Test Accuracy = 0.7910\n",
            "Processing timestep: 0.495\n",
            "Timestep 49.50% : Training Loss = 0.4536, Accuracy = 0.7941, Test Loss = 0.4522, Test Accuracy = 0.7945\n",
            "Processing timestep: 0.5\n",
            "Timestep 50.00% : Training Loss = 0.4395, Accuracy = 0.7999, Test Loss = 0.4573, Test Accuracy = 0.7787\n",
            "Processing timestep: 0.505\n",
            "Timestep 50.50% : Training Loss = 0.4368, Accuracy = 0.7901, Test Loss = 0.4358, Test Accuracy = 0.7988\n",
            "Processing timestep: 0.51\n",
            "Timestep 51.00% : Training Loss = 0.4317, Accuracy = 0.8069, Test Loss = 0.4728, Test Accuracy = 0.7707\n",
            "Processing timestep: 0.515\n",
            "Timestep 51.50% : Training Loss = 0.4224, Accuracy = 0.8127, Test Loss = 0.4507, Test Accuracy = 0.8130\n",
            "Processing timestep: 0.52\n",
            "Timestep 52.00% : Training Loss = 0.4488, Accuracy = 0.7963, Test Loss = 0.3883, Test Accuracy = 0.8115\n",
            "Processing timestep: 0.525\n",
            "Timestep 52.50% : Training Loss = 0.4174, Accuracy = 0.8098, Test Loss = 0.4148, Test Accuracy = 0.8073\n",
            "Processing timestep: 0.53\n",
            "Timestep 53.00% : Training Loss = 0.4395, Accuracy = 0.7981, Test Loss = 0.5140, Test Accuracy = 0.7547\n",
            "Processing timestep: 0.535\n",
            "Timestep 53.50% : Training Loss = 0.4035, Accuracy = 0.8252, Test Loss = 0.4087, Test Accuracy = 0.8345\n",
            "Processing timestep: 0.54\n",
            "Timestep 54.00% : Training Loss = 0.4420, Accuracy = 0.8000, Test Loss = 0.4711, Test Accuracy = 0.7865\n",
            "Processing timestep: 0.545\n",
            "Timestep 54.50% : Training Loss = 0.4328, Accuracy = 0.8096, Test Loss = 0.4580, Test Accuracy = 0.7932\n",
            "Processing timestep: 0.55\n",
            "Timestep 55.00% : Training Loss = 0.4089, Accuracy = 0.8199, Test Loss = 0.4085, Test Accuracy = 0.8212\n",
            "Processing timestep: 0.555\n",
            "Timestep 55.50% : Training Loss = 0.4298, Accuracy = 0.8106, Test Loss = 0.4217, Test Accuracy = 0.8007\n",
            "Processing timestep: 0.56\n",
            "Timestep 56.00% : Training Loss = 0.4140, Accuracy = 0.8224, Test Loss = 0.4377, Test Accuracy = 0.7956\n",
            "Processing timestep: 0.565\n",
            "Timestep 56.50% : Training Loss = 0.4170, Accuracy = 0.8087, Test Loss = 0.4394, Test Accuracy = 0.7786\n",
            "Processing timestep: 0.57\n",
            "Timestep 57.00% : Training Loss = 0.4148, Accuracy = 0.8149, Test Loss = 0.4392, Test Accuracy = 0.8281\n",
            "Processing timestep: 0.575\n",
            "Timestep 57.50% : Training Loss = 0.4249, Accuracy = 0.8090, Test Loss = 0.4394, Test Accuracy = 0.7852\n",
            "Processing timestep: 0.58\n",
            "Timestep 58.00% : Training Loss = 0.4306, Accuracy = 0.7974, Test Loss = 0.3662, Test Accuracy = 0.8296\n",
            "Processing timestep: 0.585\n",
            "Timestep 58.50% : Training Loss = 0.4248, Accuracy = 0.8125, Test Loss = 0.3735, Test Accuracy = 0.8535\n",
            "Processing timestep: 0.59\n",
            "Timestep 59.00% : Training Loss = 0.4246, Accuracy = 0.8057, Test Loss = 0.4662, Test Accuracy = 0.8069\n",
            "Processing timestep: 0.595\n",
            "Timestep 59.50% : Training Loss = 0.4090, Accuracy = 0.8106, Test Loss = 0.4702, Test Accuracy = 0.7825\n",
            "Processing timestep: 0.6\n",
            "Timestep 60.00% : Training Loss = 0.4146, Accuracy = 0.8063, Test Loss = 0.4188, Test Accuracy = 0.7878\n",
            "Processing timestep: 0.605\n",
            "Timestep 60.50% : Training Loss = 0.4155, Accuracy = 0.8080, Test Loss = 0.4601, Test Accuracy = 0.7692\n",
            "Processing timestep: 0.61\n",
            "Timestep 61.00% : Training Loss = 0.4168, Accuracy = 0.8130, Test Loss = 0.4024, Test Accuracy = 0.8021\n",
            "Processing timestep: 0.615\n",
            "Timestep 61.50% : Training Loss = 0.3897, Accuracy = 0.8188, Test Loss = 0.4174, Test Accuracy = 0.7972\n",
            "Processing timestep: 0.62\n",
            "Timestep 62.00% : Training Loss = 0.4117, Accuracy = 0.8186, Test Loss = 0.4428, Test Accuracy = 0.7814\n",
            "Processing timestep: 0.625\n",
            "Timestep 62.50% : Training Loss = 0.4198, Accuracy = 0.8070, Test Loss = 0.4124, Test Accuracy = 0.8103\n",
            "Processing timestep: 0.63\n",
            "Timestep 63.00% : Training Loss = 0.3995, Accuracy = 0.8172, Test Loss = 0.4277, Test Accuracy = 0.8071\n",
            "Processing timestep: 0.635\n",
            "Timestep 63.50% : Training Loss = 0.4148, Accuracy = 0.8067, Test Loss = 0.4043, Test Accuracy = 0.7993\n",
            "Processing timestep: 0.64\n",
            "Timestep 64.00% : Training Loss = 0.4095, Accuracy = 0.8187, Test Loss = 0.4000, Test Accuracy = 0.8202\n",
            "Processing timestep: 0.645\n",
            "Timestep 64.50% : Training Loss = 0.4175, Accuracy = 0.8082, Test Loss = 0.4044, Test Accuracy = 0.8207\n",
            "Processing timestep: 0.65\n",
            "Timestep 65.00% : Training Loss = 0.4096, Accuracy = 0.8084, Test Loss = 0.4435, Test Accuracy = 0.8078\n",
            "Processing timestep: 0.655\n",
            "Timestep 65.50% : Training Loss = 0.3812, Accuracy = 0.8319, Test Loss = 0.4312, Test Accuracy = 0.7867\n",
            "Processing timestep: 0.66\n",
            "Timestep 66.00% : Training Loss = 0.3815, Accuracy = 0.8285, Test Loss = 0.4095, Test Accuracy = 0.7909\n",
            "Processing timestep: 0.665\n",
            "Timestep 66.50% : Training Loss = 0.3791, Accuracy = 0.8310, Test Loss = 0.4318, Test Accuracy = 0.7786\n",
            "Processing timestep: 0.67\n",
            "Timestep 67.00% : Training Loss = 0.3889, Accuracy = 0.8276, Test Loss = 0.3809, Test Accuracy = 0.8404\n",
            "Processing timestep: 0.675\n",
            "Timestep 67.50% : Training Loss = 0.3809, Accuracy = 0.8314, Test Loss = 0.4300, Test Accuracy = 0.7810\n",
            "Processing timestep: 0.68\n",
            "Timestep 68.00% : Training Loss = 0.3935, Accuracy = 0.8187, Test Loss = 0.3699, Test Accuracy = 0.8381\n",
            "Processing timestep: 0.685\n",
            "Timestep 68.50% : Training Loss = 0.3942, Accuracy = 0.8235, Test Loss = 0.3519, Test Accuracy = 0.8235\n",
            "Processing timestep: 0.69\n",
            "Timestep 69.00% : Training Loss = 0.3930, Accuracy = 0.8198, Test Loss = 0.3644, Test Accuracy = 0.8546\n",
            "Processing timestep: 0.695\n",
            "Timestep 69.50% : Training Loss = 0.3639, Accuracy = 0.8396, Test Loss = 0.4142, Test Accuracy = 0.8094\n",
            "Processing timestep: 0.7\n",
            "Timestep 70.00% : Training Loss = 0.3781, Accuracy = 0.8335, Test Loss = 0.4024, Test Accuracy = 0.8182\n",
            "Processing timestep: 0.705\n",
            "Timestep 70.50% : Training Loss = 0.3688, Accuracy = 0.8313, Test Loss = 0.3553, Test Accuracy = 0.8388\n",
            "Processing timestep: 0.71\n",
            "Timestep 71.00% : Training Loss = 0.3617, Accuracy = 0.8372, Test Loss = 0.4166, Test Accuracy = 0.7847\n",
            "Processing timestep: 0.715\n",
            "Timestep 71.50% : Training Loss = 0.3758, Accuracy = 0.8293, Test Loss = 0.3476, Test Accuracy = 0.8351\n",
            "Processing timestep: 0.72\n",
            "Timestep 72.00% : Training Loss = 0.3564, Accuracy = 0.8482, Test Loss = 0.3459, Test Accuracy = 0.8191\n",
            "Processing timestep: 0.725\n",
            "Timestep 72.50% : Training Loss = 0.3713, Accuracy = 0.8291, Test Loss = 0.3344, Test Accuracy = 0.8345\n",
            "Processing timestep: 0.73\n",
            "Timestep 73.00% : Training Loss = 0.3595, Accuracy = 0.8407, Test Loss = 0.3826, Test Accuracy = 0.8049\n",
            "Processing timestep: 0.735\n",
            "Timestep 73.50% : Training Loss = 0.3555, Accuracy = 0.8367, Test Loss = 0.3689, Test Accuracy = 0.8561\n",
            "Processing timestep: 0.74\n",
            "Timestep 74.00% : Training Loss = 0.3517, Accuracy = 0.8444, Test Loss = 0.3512, Test Accuracy = 0.8263\n",
            "Processing timestep: 0.745\n",
            "Timestep 74.50% : Training Loss = 0.3518, Accuracy = 0.8397, Test Loss = 0.3649, Test Accuracy = 0.8514\n",
            "Processing timestep: 0.75\n",
            "Timestep 75.00% : Training Loss = 0.3497, Accuracy = 0.8389, Test Loss = 0.3303, Test Accuracy = 0.8522\n",
            "Processing timestep: 0.755\n",
            "Timestep 75.50% : Training Loss = 0.3299, Accuracy = 0.8550, Test Loss = 0.2874, Test Accuracy = 0.8976\n",
            "Processing timestep: 0.76\n",
            "Timestep 76.00% : Training Loss = 0.3370, Accuracy = 0.8506, Test Loss = 0.3333, Test Accuracy = 0.8914\n",
            "Processing timestep: 0.765\n",
            "Timestep 76.50% : Training Loss = 0.3388, Accuracy = 0.8557, Test Loss = 0.2871, Test Accuracy = 0.8625\n",
            "Processing timestep: 0.77\n",
            "Timestep 77.00% : Training Loss = 0.3495, Accuracy = 0.8395, Test Loss = 0.3239, Test Accuracy = 0.8651\n",
            "Processing timestep: 0.775\n",
            "Timestep 77.50% : Training Loss = 0.3176, Accuracy = 0.8576, Test Loss = 0.3734, Test Accuracy = 0.8516\n",
            "Processing timestep: 0.78\n",
            "Timestep 78.00% : Training Loss = 0.3355, Accuracy = 0.8550, Test Loss = 0.3730, Test Accuracy = 0.8120\n",
            "Processing timestep: 0.785\n",
            "Timestep 78.50% : Training Loss = 0.3415, Accuracy = 0.8445, Test Loss = 0.3178, Test Accuracy = 0.8690\n",
            "Processing timestep: 0.79\n",
            "Timestep 79.00% : Training Loss = 0.2996, Accuracy = 0.8747, Test Loss = 0.3203, Test Accuracy = 0.8640\n",
            "Processing timestep: 0.795\n",
            "Timestep 79.50% : Training Loss = 0.3382, Accuracy = 0.8468, Test Loss = 0.3774, Test Accuracy = 0.8290\n",
            "Processing timestep: 0.8\n",
            "Timestep 80.00% : Training Loss = 0.3165, Accuracy = 0.8647, Test Loss = 0.3305, Test Accuracy = 0.8600\n",
            "Processing timestep: 0.805\n",
            "Timestep 80.50% : Training Loss = 0.3175, Accuracy = 0.8652, Test Loss = 0.3366, Test Accuracy = 0.8443\n",
            "Processing timestep: 0.81\n",
            "Timestep 81.00% : Training Loss = 0.3212, Accuracy = 0.8610, Test Loss = 0.3043, Test Accuracy = 0.8632\n",
            "Processing timestep: 0.815\n",
            "Timestep 81.50% : Training Loss = 0.3226, Accuracy = 0.8551, Test Loss = 0.3039, Test Accuracy = 0.8683\n",
            "Processing timestep: 0.82\n",
            "Timestep 82.00% : Training Loss = 0.3201, Accuracy = 0.8564, Test Loss = 0.2554, Test Accuracy = 0.9044\n",
            "Processing timestep: 0.825\n",
            "Timestep 82.50% : Training Loss = 0.3195, Accuracy = 0.8522, Test Loss = 0.3305, Test Accuracy = 0.8296\n",
            "Processing timestep: 0.83\n",
            "Timestep 83.00% : Training Loss = 0.3030, Accuracy = 0.8613, Test Loss = 0.3013, Test Accuracy = 0.8652\n",
            "Processing timestep: 0.835\n",
            "Timestep 83.50% : Training Loss = 0.2813, Accuracy = 0.8844, Test Loss = 0.2639, Test Accuracy = 0.8834\n",
            "Processing timestep: 0.84\n",
            "Timestep 84.00% : Training Loss = 0.2740, Accuracy = 0.8795, Test Loss = 0.3253, Test Accuracy = 0.8333\n",
            "Processing timestep: 0.845\n",
            "Timestep 84.50% : Training Loss = 0.3150, Accuracy = 0.8600, Test Loss = 0.3423, Test Accuracy = 0.8258\n",
            "Processing timestep: 0.85\n",
            "Timestep 85.00% : Training Loss = 0.2988, Accuracy = 0.8649, Test Loss = 0.2575, Test Accuracy = 0.8737\n",
            "Processing timestep: 0.855\n",
            "Timestep 85.50% : Training Loss = 0.2863, Accuracy = 0.8690, Test Loss = 0.3073, Test Accuracy = 0.8741\n",
            "Processing timestep: 0.86\n",
            "Timestep 86.00% : Training Loss = 0.2844, Accuracy = 0.8668, Test Loss = 0.2888, Test Accuracy = 0.8561\n",
            "Processing timestep: 0.865\n",
            "Timestep 86.50% : Training Loss = 0.2960, Accuracy = 0.8682, Test Loss = 0.2325, Test Accuracy = 0.8964\n",
            "Processing timestep: 0.87\n",
            "Timestep 87.00% : Training Loss = 0.3011, Accuracy = 0.8574, Test Loss = 0.3079, Test Accuracy = 0.8369\n",
            "Processing timestep: 0.875\n",
            "Timestep 87.50% : Training Loss = 0.2848, Accuracy = 0.8704, Test Loss = 0.2613, Test Accuracy = 0.8862\n",
            "Processing timestep: 0.88\n",
            "Timestep 88.00% : Training Loss = 0.2794, Accuracy = 0.8751, Test Loss = 0.2928, Test Accuracy = 0.8702\n",
            "Processing timestep: 0.885\n",
            "Timestep 88.50% : Training Loss = 0.2932, Accuracy = 0.8657, Test Loss = 0.2940, Test Accuracy = 0.8671\n",
            "Processing timestep: 0.89\n",
            "Timestep 89.00% : Training Loss = 0.2982, Accuracy = 0.8707, Test Loss = 0.2839, Test Accuracy = 0.8690\n",
            "Processing timestep: 0.895\n",
            "Timestep 89.50% : Training Loss = 0.2537, Accuracy = 0.8912, Test Loss = 0.2617, Test Accuracy = 0.8669\n",
            "Processing timestep: 0.9\n",
            "Timestep 90.00% : Training Loss = 0.2959, Accuracy = 0.8590, Test Loss = 0.3445, Test Accuracy = 0.8415\n",
            "Processing timestep: 0.905\n",
            "Timestep 90.50% : Training Loss = 0.2620, Accuracy = 0.8832, Test Loss = 0.2538, Test Accuracy = 0.8832\n",
            "Processing timestep: 0.91\n",
            "Timestep 91.00% : Training Loss = 0.2599, Accuracy = 0.8905, Test Loss = 0.2873, Test Accuracy = 0.8542\n",
            "Processing timestep: 0.915\n",
            "Timestep 91.50% : Training Loss = 0.2525, Accuracy = 0.8901, Test Loss = 0.2502, Test Accuracy = 0.8912\n",
            "Processing timestep: 0.92\n",
            "Timestep 92.00% : Training Loss = 0.2676, Accuracy = 0.8829, Test Loss = 0.3028, Test Accuracy = 0.8365\n",
            "Processing timestep: 0.925\n",
            "Timestep 92.50% : Training Loss = 0.2441, Accuracy = 0.8944, Test Loss = 0.2681, Test Accuracy = 0.8615\n",
            "Processing timestep: 0.93\n",
            "Timestep 93.00% : Training Loss = 0.2434, Accuracy = 0.9030, Test Loss = 0.2835, Test Accuracy = 0.8660\n",
            "Processing timestep: 0.935\n",
            "Timestep 93.50% : Training Loss = 0.2552, Accuracy = 0.8937, Test Loss = 0.2405, Test Accuracy = 0.8947\n",
            "Processing timestep: 0.94\n",
            "Timestep 94.00% : Training Loss = 0.2275, Accuracy = 0.9048, Test Loss = 0.2214, Test Accuracy = 0.8859\n",
            "Processing timestep: 0.945\n",
            "Timestep 94.50% : Training Loss = 0.2298, Accuracy = 0.9017, Test Loss = 0.2474, Test Accuracy = 0.8937\n",
            "Processing timestep: 0.95\n",
            "Timestep 95.00% : Training Loss = 0.2550, Accuracy = 0.8859, Test Loss = 0.2394, Test Accuracy = 0.8880\n",
            "Processing timestep: 0.955\n",
            "Timestep 95.50% : Training Loss = 0.2372, Accuracy = 0.8948, Test Loss = 0.2601, Test Accuracy = 0.8785\n",
            "Processing timestep: 0.96\n",
            "Timestep 96.00% : Training Loss = 0.2548, Accuracy = 0.8884, Test Loss = 0.2560, Test Accuracy = 0.8886\n",
            "Processing timestep: 0.965\n",
            "Timestep 96.50% : Training Loss = 0.2293, Accuracy = 0.8949, Test Loss = 0.2499, Test Accuracy = 0.8892\n",
            "Processing timestep: 0.97\n",
            "Timestep 97.00% : Training Loss = 0.2413, Accuracy = 0.8954, Test Loss = 0.2392, Test Accuracy = 0.8982\n",
            "Processing timestep: 0.975\n",
            "Timestep 97.50% : Training Loss = 0.2473, Accuracy = 0.8938, Test Loss = 0.2512, Test Accuracy = 0.8838\n",
            "Processing timestep: 0.98\n",
            "Timestep 98.00% : Training Loss = 0.2097, Accuracy = 0.9116, Test Loss = 0.2284, Test Accuracy = 0.9093\n",
            "Processing timestep: 0.985\n",
            "Timestep 98.50% : Training Loss = 0.2101, Accuracy = 0.9128, Test Loss = 0.2690, Test Accuracy = 0.8847\n",
            "Processing timestep: 0.99\n",
            "Timestep 99.00% : Training Loss = 0.2321, Accuracy = 0.8979, Test Loss = 0.2840, Test Accuracy = 0.8773\n",
            "Processing timestep: 0.995\n",
            "Timestep 99.50% : Training Loss = 0.2366, Accuracy = 0.9080, Test Loss = 0.2319, Test Accuracy = 0.8968\n",
            "Processing timestep: 1.0\n",
            "Timestep 100.00% : Training Loss = 0.1771, Accuracy = 0.9440, Test Loss = 0.1742, Test Accuracy = 0.9186\n"
          ]
        }
      ],
      "source": [
        "other_features = [\n",
        "            \"type.id\",             # Play type (categorical)\n",
        "            \"home_has_possession\", # Binary indicator\n",
        "            \"end.down\",            # Down number (1-4, discrete)\n",
        "            \"home_timeouts_left\",  # Discrete count (0-3)\n",
        "            \"away_timeouts_left\",  # Discrete count (0-3)\n",
        "        ]\n",
        "numeric_features = [\n",
        "    \"score_difference\",\n",
        "    \"relative_strength\", \n",
        "    \"end.yardsToEndzone\", \n",
        "    \"end.distance\", \n",
        "    \"field_position_shift\"\n",
        "]\n",
        "all_models[\"logistic\"] = setup_logistic_regression_models(training_data, None, numeric_features, other_features, features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestep 0.00%(Calibrated): Training Loss = 0.3333, Accuracy = 0.6667, Validation Loss = 0.1998, Validation Accuracy = 0.6932\n",
            "Timestep 0.50%(Calibrated): Training Loss = 0.3324, Accuracy = 0.6676, Validation Loss = 0.2101, Validation Accuracy = 0.6776\n",
            "Timestep 1.00%(Calibrated): Training Loss = 0.3421, Accuracy = 0.6579, Validation Loss = 0.1988, Validation Accuracy = 0.6784\n",
            "Timestep 1.50%(Calibrated): Training Loss = 0.3420, Accuracy = 0.6580, Validation Loss = 0.2045, Validation Accuracy = 0.6771\n",
            "Timestep 2.00%(Calibrated): Training Loss = 0.3377, Accuracy = 0.6623, Validation Loss = 0.2160, Validation Accuracy = 0.6419\n",
            "Timestep 2.50%(Calibrated): Training Loss = 0.3371, Accuracy = 0.6629, Validation Loss = 0.2135, Validation Accuracy = 0.6614\n",
            "Timestep 3.00%(Calibrated): Training Loss = 0.3317, Accuracy = 0.6683, Validation Loss = 0.1992, Validation Accuracy = 0.6991\n",
            "Timestep 3.50%(Calibrated): Training Loss = 0.3479, Accuracy = 0.6521, Validation Loss = 0.2087, Validation Accuracy = 0.6616\n",
            "Timestep 4.00%(Calibrated): Training Loss = 0.3336, Accuracy = 0.6664, Validation Loss = 0.2074, Validation Accuracy = 0.6789\n",
            "Timestep 4.50%(Calibrated): Training Loss = 0.3290, Accuracy = 0.6710, Validation Loss = 0.2062, Validation Accuracy = 0.6767\n",
            "Timestep 5.00%(Calibrated): Training Loss = 0.3377, Accuracy = 0.6623, Validation Loss = 0.2127, Validation Accuracy = 0.6600\n",
            "Timestep 5.50%(Calibrated): Training Loss = 0.3166, Accuracy = 0.6834, Validation Loss = 0.2168, Validation Accuracy = 0.6430\n",
            "Timestep 6.00%(Calibrated): Training Loss = 0.3401, Accuracy = 0.6599, Validation Loss = 0.2247, Validation Accuracy = 0.6295\n",
            "Timestep 6.50%(Calibrated): Training Loss = 0.3297, Accuracy = 0.6703, Validation Loss = 0.2042, Validation Accuracy = 0.6565\n",
            "Timestep 7.00%(Calibrated): Training Loss = 0.3212, Accuracy = 0.6788, Validation Loss = 0.1893, Validation Accuracy = 0.7146\n",
            "Timestep 7.50%(Calibrated): Training Loss = 0.3206, Accuracy = 0.6794, Validation Loss = 0.2143, Validation Accuracy = 0.6468\n",
            "Timestep 8.00%(Calibrated): Training Loss = 0.3007, Accuracy = 0.6993, Validation Loss = 0.1883, Validation Accuracy = 0.7128\n",
            "Timestep 8.50%(Calibrated): Training Loss = 0.3282, Accuracy = 0.6718, Validation Loss = 0.2070, Validation Accuracy = 0.6786\n",
            "Timestep 9.00%(Calibrated): Training Loss = 0.3138, Accuracy = 0.6862, Validation Loss = 0.2096, Validation Accuracy = 0.6600\n",
            "Timestep 9.50%(Calibrated): Training Loss = 0.3199, Accuracy = 0.6801, Validation Loss = 0.2052, Validation Accuracy = 0.6858\n",
            "Timestep 10.00%(Calibrated): Training Loss = 0.2987, Accuracy = 0.7013, Validation Loss = 0.1997, Validation Accuracy = 0.6991\n",
            "Timestep 10.50%(Calibrated): Training Loss = 0.3081, Accuracy = 0.6919, Validation Loss = 0.2007, Validation Accuracy = 0.6878\n",
            "Timestep 11.00%(Calibrated): Training Loss = 0.3164, Accuracy = 0.6836, Validation Loss = 0.2089, Validation Accuracy = 0.6681\n",
            "Timestep 11.50%(Calibrated): Training Loss = 0.3130, Accuracy = 0.6870, Validation Loss = 0.1781, Validation Accuracy = 0.7385\n",
            "Timestep 12.00%(Calibrated): Training Loss = 0.3142, Accuracy = 0.6858, Validation Loss = 0.1960, Validation Accuracy = 0.6928\n",
            "Timestep 12.50%(Calibrated): Training Loss = 0.2928, Accuracy = 0.7072, Validation Loss = 0.1904, Validation Accuracy = 0.6968\n",
            "Timestep 13.00%(Calibrated): Training Loss = 0.3035, Accuracy = 0.6965, Validation Loss = 0.2025, Validation Accuracy = 0.6738\n",
            "Timestep 13.50%(Calibrated): Training Loss = 0.2979, Accuracy = 0.7021, Validation Loss = 0.2002, Validation Accuracy = 0.6818\n",
            "Timestep 14.00%(Calibrated): Training Loss = 0.2938, Accuracy = 0.7062, Validation Loss = 0.1920, Validation Accuracy = 0.7009\n",
            "Timestep 14.50%(Calibrated): Training Loss = 0.3091, Accuracy = 0.6909, Validation Loss = 0.1949, Validation Accuracy = 0.6848\n",
            "Timestep 15.00%(Calibrated): Training Loss = 0.2911, Accuracy = 0.7089, Validation Loss = 0.1873, Validation Accuracy = 0.6981\n",
            "Timestep 15.50%(Calibrated): Training Loss = 0.3044, Accuracy = 0.6956, Validation Loss = 0.1940, Validation Accuracy = 0.6963\n",
            "Timestep 16.00%(Calibrated): Training Loss = 0.3061, Accuracy = 0.6939, Validation Loss = 0.1890, Validation Accuracy = 0.7035\n",
            "Timestep 16.50%(Calibrated): Training Loss = 0.3206, Accuracy = 0.6794, Validation Loss = 0.1977, Validation Accuracy = 0.6659\n",
            "Timestep 17.00%(Calibrated): Training Loss = 0.2928, Accuracy = 0.7072, Validation Loss = 0.2021, Validation Accuracy = 0.6855\n",
            "Timestep 17.50%(Calibrated): Training Loss = 0.3134, Accuracy = 0.6866, Validation Loss = 0.1999, Validation Accuracy = 0.6827\n",
            "Timestep 18.00%(Calibrated): Training Loss = 0.2952, Accuracy = 0.7048, Validation Loss = 0.1829, Validation Accuracy = 0.7261\n",
            "Timestep 18.50%(Calibrated): Training Loss = 0.3063, Accuracy = 0.6937, Validation Loss = 0.1755, Validation Accuracy = 0.7371\n",
            "Timestep 19.00%(Calibrated): Training Loss = 0.2675, Accuracy = 0.7325, Validation Loss = 0.2056, Validation Accuracy = 0.6815\n",
            "Timestep 19.50%(Calibrated): Training Loss = 0.2919, Accuracy = 0.7081, Validation Loss = 0.1874, Validation Accuracy = 0.7024\n",
            "Timestep 20.00%(Calibrated): Training Loss = 0.2933, Accuracy = 0.7067, Validation Loss = 0.1847, Validation Accuracy = 0.7121\n",
            "Timestep 20.50%(Calibrated): Training Loss = 0.2752, Accuracy = 0.7248, Validation Loss = 0.1928, Validation Accuracy = 0.6913\n",
            "Timestep 21.00%(Calibrated): Training Loss = 0.2867, Accuracy = 0.7133, Validation Loss = 0.1814, Validation Accuracy = 0.7366\n",
            "Timestep 21.50%(Calibrated): Training Loss = 0.2897, Accuracy = 0.7103, Validation Loss = 0.1829, Validation Accuracy = 0.7312\n",
            "Timestep 22.00%(Calibrated): Training Loss = 0.2715, Accuracy = 0.7285, Validation Loss = 0.1871, Validation Accuracy = 0.7000\n",
            "Timestep 22.50%(Calibrated): Training Loss = 0.2790, Accuracy = 0.7210, Validation Loss = 0.1867, Validation Accuracy = 0.7158\n",
            "Timestep 23.00%(Calibrated): Training Loss = 0.2915, Accuracy = 0.7085, Validation Loss = 0.1784, Validation Accuracy = 0.7207\n",
            "Timestep 23.50%(Calibrated): Training Loss = 0.2589, Accuracy = 0.7411, Validation Loss = 0.1927, Validation Accuracy = 0.6960\n",
            "Timestep 24.00%(Calibrated): Training Loss = 0.3074, Accuracy = 0.6926, Validation Loss = 0.1852, Validation Accuracy = 0.7222\n",
            "Timestep 24.50%(Calibrated): Training Loss = 0.2662, Accuracy = 0.7338, Validation Loss = 0.1953, Validation Accuracy = 0.6872\n",
            "Completed 50/201 timesteps\n",
            "Timestep 25.00%(Calibrated): Training Loss = 0.2653, Accuracy = 0.7347, Validation Loss = 0.1738, Validation Accuracy = 0.7340\n",
            "Timestep 25.50%(Calibrated): Training Loss = 0.2869, Accuracy = 0.7131, Validation Loss = 0.1711, Validation Accuracy = 0.7044\n",
            "Timestep 26.00%(Calibrated): Training Loss = 0.3000, Accuracy = 0.7000, Validation Loss = 0.1625, Validation Accuracy = 0.7639\n",
            "Timestep 26.50%(Calibrated): Training Loss = 0.2772, Accuracy = 0.7228, Validation Loss = 0.1662, Validation Accuracy = 0.7526\n",
            "Timestep 27.00%(Calibrated): Training Loss = 0.2487, Accuracy = 0.7513, Validation Loss = 0.1923, Validation Accuracy = 0.6935\n",
            "Timestep 27.50%(Calibrated): Training Loss = 0.2806, Accuracy = 0.7194, Validation Loss = 0.1830, Validation Accuracy = 0.7120\n",
            "Timestep 28.00%(Calibrated): Training Loss = 0.2800, Accuracy = 0.7200, Validation Loss = 0.1817, Validation Accuracy = 0.7383\n",
            "Timestep 28.50%(Calibrated): Training Loss = 0.2667, Accuracy = 0.7333, Validation Loss = 0.1725, Validation Accuracy = 0.7399\n",
            "Timestep 29.00%(Calibrated): Training Loss = 0.2819, Accuracy = 0.7181, Validation Loss = 0.1864, Validation Accuracy = 0.7124\n",
            "Timestep 29.50%(Calibrated): Training Loss = 0.2719, Accuracy = 0.7281, Validation Loss = 0.1767, Validation Accuracy = 0.7109\n",
            "Timestep 30.00%(Calibrated): Training Loss = 0.2740, Accuracy = 0.7260, Validation Loss = 0.1840, Validation Accuracy = 0.7188\n",
            "Timestep 30.50%(Calibrated): Training Loss = 0.2831, Accuracy = 0.7169, Validation Loss = 0.1751, Validation Accuracy = 0.7370\n",
            "Timestep 31.00%(Calibrated): Training Loss = 0.2572, Accuracy = 0.7428, Validation Loss = 0.1817, Validation Accuracy = 0.7032\n",
            "Timestep 31.50%(Calibrated): Training Loss = 0.2885, Accuracy = 0.7115, Validation Loss = 0.1895, Validation Accuracy = 0.6974\n",
            "Timestep 32.00%(Calibrated): Training Loss = 0.2716, Accuracy = 0.7284, Validation Loss = 0.1615, Validation Accuracy = 0.7452\n",
            "Timestep 32.50%(Calibrated): Training Loss = 0.2435, Accuracy = 0.7565, Validation Loss = 0.1791, Validation Accuracy = 0.7273\n",
            "Timestep 33.00%(Calibrated): Training Loss = 0.2511, Accuracy = 0.7489, Validation Loss = 0.1707, Validation Accuracy = 0.7345\n",
            "Timestep 33.50%(Calibrated): Training Loss = 0.2474, Accuracy = 0.7526, Validation Loss = 0.1803, Validation Accuracy = 0.7200\n",
            "Timestep 34.00%(Calibrated): Training Loss = 0.2480, Accuracy = 0.7520, Validation Loss = 0.1721, Validation Accuracy = 0.7342\n",
            "Timestep 34.50%(Calibrated): Training Loss = 0.2408, Accuracy = 0.7592, Validation Loss = 0.1576, Validation Accuracy = 0.7739\n",
            "Timestep 35.00%(Calibrated): Training Loss = 0.2408, Accuracy = 0.7592, Validation Loss = 0.1678, Validation Accuracy = 0.7288\n",
            "Timestep 35.50%(Calibrated): Training Loss = 0.2374, Accuracy = 0.7626, Validation Loss = 0.1699, Validation Accuracy = 0.7484\n",
            "Timestep 36.00%(Calibrated): Training Loss = 0.2600, Accuracy = 0.7400, Validation Loss = 0.1698, Validation Accuracy = 0.7543\n",
            "Timestep 36.50%(Calibrated): Training Loss = 0.2270, Accuracy = 0.7730, Validation Loss = 0.1642, Validation Accuracy = 0.7531\n",
            "Timestep 37.00%(Calibrated): Training Loss = 0.2465, Accuracy = 0.7535, Validation Loss = 0.1685, Validation Accuracy = 0.7467\n",
            "Timestep 37.50%(Calibrated): Training Loss = 0.2456, Accuracy = 0.7544, Validation Loss = 0.1678, Validation Accuracy = 0.7458\n",
            "Timestep 38.00%(Calibrated): Training Loss = 0.2484, Accuracy = 0.7516, Validation Loss = 0.1558, Validation Accuracy = 0.7625\n",
            "Timestep 38.50%(Calibrated): Training Loss = 0.2480, Accuracy = 0.7520, Validation Loss = 0.1648, Validation Accuracy = 0.7511\n",
            "Timestep 39.00%(Calibrated): Training Loss = 0.2511, Accuracy = 0.7489, Validation Loss = 0.1727, Validation Accuracy = 0.7408\n",
            "Timestep 39.50%(Calibrated): Training Loss = 0.2309, Accuracy = 0.7691, Validation Loss = 0.1596, Validation Accuracy = 0.7579\n",
            "Timestep 40.00%(Calibrated): Training Loss = 0.2579, Accuracy = 0.7421, Validation Loss = 0.1669, Validation Accuracy = 0.7457\n",
            "Timestep 40.50%(Calibrated): Training Loss = 0.2283, Accuracy = 0.7717, Validation Loss = 0.1547, Validation Accuracy = 0.7710\n",
            "Timestep 41.00%(Calibrated): Training Loss = 0.2488, Accuracy = 0.7512, Validation Loss = 0.1540, Validation Accuracy = 0.7801\n",
            "Timestep 41.50%(Calibrated): Training Loss = 0.2308, Accuracy = 0.7692, Validation Loss = 0.1542, Validation Accuracy = 0.7672\n",
            "Timestep 42.00%(Calibrated): Training Loss = 0.2483, Accuracy = 0.7517, Validation Loss = 0.1703, Validation Accuracy = 0.7448\n",
            "Timestep 42.50%(Calibrated): Training Loss = 0.2196, Accuracy = 0.7804, Validation Loss = 0.1729, Validation Accuracy = 0.7473\n",
            "Timestep 43.00%(Calibrated): Training Loss = 0.2198, Accuracy = 0.7802, Validation Loss = 0.1579, Validation Accuracy = 0.7548\n",
            "Timestep 43.50%(Calibrated): Training Loss = 0.2231, Accuracy = 0.7769, Validation Loss = 0.1613, Validation Accuracy = 0.7547\n",
            "Timestep 44.00%(Calibrated): Training Loss = 0.2427, Accuracy = 0.7573, Validation Loss = 0.1593, Validation Accuracy = 0.7670\n",
            "Timestep 44.50%(Calibrated): Training Loss = 0.2268, Accuracy = 0.7732, Validation Loss = 0.1477, Validation Accuracy = 0.7874\n",
            "Timestep 45.00%(Calibrated): Training Loss = 0.2408, Accuracy = 0.7592, Validation Loss = 0.1646, Validation Accuracy = 0.7638\n",
            "Timestep 45.50%(Calibrated): Training Loss = 0.2328, Accuracy = 0.7672, Validation Loss = 0.1624, Validation Accuracy = 0.7553\n",
            "Timestep 46.00%(Calibrated): Training Loss = 0.2298, Accuracy = 0.7702, Validation Loss = 0.1375, Validation Accuracy = 0.7954\n",
            "Timestep 46.50%(Calibrated): Training Loss = 0.2116, Accuracy = 0.7884, Validation Loss = 0.1500, Validation Accuracy = 0.7862\n",
            "Timestep 47.00%(Calibrated): Training Loss = 0.2120, Accuracy = 0.7880, Validation Loss = 0.1533, Validation Accuracy = 0.7708\n",
            "Timestep 47.50%(Calibrated): Training Loss = 0.2082, Accuracy = 0.7918, Validation Loss = 0.1368, Validation Accuracy = 0.7891\n",
            "Timestep 48.00%(Calibrated): Training Loss = 0.2007, Accuracy = 0.7993, Validation Loss = 0.1524, Validation Accuracy = 0.7740\n",
            "Timestep 48.50%(Calibrated): Training Loss = 0.2098, Accuracy = 0.7902, Validation Loss = 0.1462, Validation Accuracy = 0.7913\n",
            "Timestep 49.00%(Calibrated): Training Loss = 0.2082, Accuracy = 0.7918, Validation Loss = 0.1444, Validation Accuracy = 0.8010\n",
            "Timestep 49.50%(Calibrated): Training Loss = 0.2037, Accuracy = 0.7963, Validation Loss = 0.1378, Validation Accuracy = 0.7955\n",
            "Completed 100/201 timesteps\n",
            "Timestep 50.00%(Calibrated): Training Loss = 0.1827, Accuracy = 0.8173, Validation Loss = 0.1357, Validation Accuracy = 0.8050\n",
            "Timestep 50.50%(Calibrated): Training Loss = 0.2125, Accuracy = 0.7875, Validation Loss = 0.1277, Validation Accuracy = 0.8102\n",
            "Timestep 51.00%(Calibrated): Training Loss = 0.1991, Accuracy = 0.8009, Validation Loss = 0.1503, Validation Accuracy = 0.7851\n",
            "Timestep 51.50%(Calibrated): Training Loss = 0.1932, Accuracy = 0.8068, Validation Loss = 0.1346, Validation Accuracy = 0.8151\n",
            "Timestep 52.00%(Calibrated): Training Loss = 0.1955, Accuracy = 0.8045, Validation Loss = 0.1411, Validation Accuracy = 0.7834\n",
            "Timestep 52.50%(Calibrated): Training Loss = 0.1850, Accuracy = 0.8150, Validation Loss = 0.1256, Validation Accuracy = 0.8144\n",
            "Timestep 53.00%(Calibrated): Training Loss = 0.2098, Accuracy = 0.7902, Validation Loss = 0.1519, Validation Accuracy = 0.7846\n",
            "Timestep 53.50%(Calibrated): Training Loss = 0.1808, Accuracy = 0.8192, Validation Loss = 0.1237, Validation Accuracy = 0.8229\n",
            "Timestep 54.00%(Calibrated): Training Loss = 0.2027, Accuracy = 0.7973, Validation Loss = 0.1455, Validation Accuracy = 0.7910\n",
            "Timestep 54.50%(Calibrated): Training Loss = 0.1964, Accuracy = 0.8036, Validation Loss = 0.1480, Validation Accuracy = 0.7793\n",
            "Timestep 55.00%(Calibrated): Training Loss = 0.1807, Accuracy = 0.8193, Validation Loss = 0.1389, Validation Accuracy = 0.8092\n",
            "Timestep 55.50%(Calibrated): Training Loss = 0.2013, Accuracy = 0.7987, Validation Loss = 0.1279, Validation Accuracy = 0.8137\n",
            "Timestep 56.00%(Calibrated): Training Loss = 0.1830, Accuracy = 0.8170, Validation Loss = 0.1375, Validation Accuracy = 0.7939\n",
            "Timestep 56.50%(Calibrated): Training Loss = 0.2110, Accuracy = 0.7890, Validation Loss = 0.1390, Validation Accuracy = 0.7854\n",
            "Timestep 57.00%(Calibrated): Training Loss = 0.1914, Accuracy = 0.8086, Validation Loss = 0.1222, Validation Accuracy = 0.8354\n",
            "Timestep 57.50%(Calibrated): Training Loss = 0.1897, Accuracy = 0.8103, Validation Loss = 0.1327, Validation Accuracy = 0.8013\n",
            "Timestep 58.00%(Calibrated): Training Loss = 0.2028, Accuracy = 0.7972, Validation Loss = 0.1260, Validation Accuracy = 0.8040\n",
            "Timestep 58.50%(Calibrated): Training Loss = 0.2000, Accuracy = 0.8000, Validation Loss = 0.1188, Validation Accuracy = 0.8352\n",
            "Timestep 59.00%(Calibrated): Training Loss = 0.1834, Accuracy = 0.8166, Validation Loss = 0.1417, Validation Accuracy = 0.7967\n",
            "Timestep 59.50%(Calibrated): Training Loss = 0.1900, Accuracy = 0.8100, Validation Loss = 0.1341, Validation Accuracy = 0.8101\n",
            "Timestep 60.00%(Calibrated): Training Loss = 0.1893, Accuracy = 0.8107, Validation Loss = 0.1432, Validation Accuracy = 0.7802\n",
            "Timestep 60.50%(Calibrated): Training Loss = 0.1839, Accuracy = 0.8161, Validation Loss = 0.1354, Validation Accuracy = 0.8004\n",
            "Timestep 61.00%(Calibrated): Training Loss = 0.1812, Accuracy = 0.8188, Validation Loss = 0.1246, Validation Accuracy = 0.8225\n",
            "Timestep 61.50%(Calibrated): Training Loss = 0.1676, Accuracy = 0.8324, Validation Loss = 0.1294, Validation Accuracy = 0.8013\n",
            "Timestep 62.00%(Calibrated): Training Loss = 0.1889, Accuracy = 0.8111, Validation Loss = 0.1355, Validation Accuracy = 0.8039\n",
            "Timestep 62.50%(Calibrated): Training Loss = 0.1785, Accuracy = 0.8215, Validation Loss = 0.1272, Validation Accuracy = 0.8257\n",
            "Timestep 63.00%(Calibrated): Training Loss = 0.1773, Accuracy = 0.8227, Validation Loss = 0.1340, Validation Accuracy = 0.8094\n",
            "Timestep 63.50%(Calibrated): Training Loss = 0.1875, Accuracy = 0.8125, Validation Loss = 0.1350, Validation Accuracy = 0.8065\n",
            "Timestep 64.00%(Calibrated): Training Loss = 0.1815, Accuracy = 0.8185, Validation Loss = 0.1232, Validation Accuracy = 0.8202\n",
            "Timestep 64.50%(Calibrated): Training Loss = 0.1939, Accuracy = 0.8061, Validation Loss = 0.1349, Validation Accuracy = 0.8137\n",
            "Timestep 65.00%(Calibrated): Training Loss = 0.1842, Accuracy = 0.8158, Validation Loss = 0.1344, Validation Accuracy = 0.8116\n",
            "Timestep 65.50%(Calibrated): Training Loss = 0.1688, Accuracy = 0.8312, Validation Loss = 0.1301, Validation Accuracy = 0.8088\n",
            "Timestep 66.00%(Calibrated): Training Loss = 0.1747, Accuracy = 0.8253, Validation Loss = 0.1238, Validation Accuracy = 0.8323\n",
            "Timestep 66.50%(Calibrated): Training Loss = 0.1866, Accuracy = 0.8134, Validation Loss = 0.1338, Validation Accuracy = 0.7944\n",
            "Timestep 67.00%(Calibrated): Training Loss = 0.1699, Accuracy = 0.8301, Validation Loss = 0.1151, Validation Accuracy = 0.8404\n",
            "Timestep 67.50%(Calibrated): Training Loss = 0.1691, Accuracy = 0.8309, Validation Loss = 0.1208, Validation Accuracy = 0.8114\n",
            "Timestep 68.00%(Calibrated): Training Loss = 0.1860, Accuracy = 0.8140, Validation Loss = 0.1206, Validation Accuracy = 0.8251\n",
            "Timestep 68.50%(Calibrated): Training Loss = 0.2050, Accuracy = 0.7950, Validation Loss = 0.1044, Validation Accuracy = 0.8506\n",
            "Timestep 69.00%(Calibrated): Training Loss = 0.1750, Accuracy = 0.8250, Validation Loss = 0.1126, Validation Accuracy = 0.8294\n",
            "Timestep 69.50%(Calibrated): Training Loss = 0.1573, Accuracy = 0.8427, Validation Loss = 0.1346, Validation Accuracy = 0.7970\n",
            "Timestep 70.00%(Calibrated): Training Loss = 0.1641, Accuracy = 0.8359, Validation Loss = 0.1193, Validation Accuracy = 0.8277\n",
            "Timestep 70.50%(Calibrated): Training Loss = 0.1751, Accuracy = 0.8249, Validation Loss = 0.1047, Validation Accuracy = 0.8527\n",
            "Timestep 71.00%(Calibrated): Training Loss = 0.1662, Accuracy = 0.8338, Validation Loss = 0.1219, Validation Accuracy = 0.8180\n",
            "Timestep 71.50%(Calibrated): Training Loss = 0.1828, Accuracy = 0.8172, Validation Loss = 0.1129, Validation Accuracy = 0.8291\n",
            "Timestep 72.00%(Calibrated): Training Loss = 0.1500, Accuracy = 0.8500, Validation Loss = 0.1083, Validation Accuracy = 0.8443\n",
            "Timestep 72.50%(Calibrated): Training Loss = 0.1860, Accuracy = 0.8140, Validation Loss = 0.1069, Validation Accuracy = 0.8278\n",
            "Timestep 73.00%(Calibrated): Training Loss = 0.1534, Accuracy = 0.8466, Validation Loss = 0.1165, Validation Accuracy = 0.8330\n",
            "Timestep 73.50%(Calibrated): Training Loss = 0.1555, Accuracy = 0.8445, Validation Loss = 0.1061, Validation Accuracy = 0.8575\n",
            "Timestep 74.00%(Calibrated): Training Loss = 0.1555, Accuracy = 0.8445, Validation Loss = 0.1066, Validation Accuracy = 0.8422\n",
            "Timestep 74.50%(Calibrated): Training Loss = 0.1535, Accuracy = 0.8465, Validation Loss = 0.1043, Validation Accuracy = 0.8458\n",
            "Completed 150/201 timesteps\n",
            "Timestep 75.00%(Calibrated): Training Loss = 0.1505, Accuracy = 0.8495, Validation Loss = 0.1003, Validation Accuracy = 0.8583\n",
            "Timestep 75.50%(Calibrated): Training Loss = 0.1268, Accuracy = 0.8732, Validation Loss = 0.0914, Validation Accuracy = 0.8659\n",
            "Timestep 76.00%(Calibrated): Training Loss = 0.1504, Accuracy = 0.8496, Validation Loss = 0.0887, Validation Accuracy = 0.8835\n",
            "Timestep 76.50%(Calibrated): Training Loss = 0.1622, Accuracy = 0.8378, Validation Loss = 0.0909, Validation Accuracy = 0.8763\n",
            "Timestep 77.00%(Calibrated): Training Loss = 0.1604, Accuracy = 0.8396, Validation Loss = 0.0991, Validation Accuracy = 0.8619\n",
            "Timestep 77.50%(Calibrated): Training Loss = 0.1362, Accuracy = 0.8638, Validation Loss = 0.1136, Validation Accuracy = 0.8414\n",
            "Timestep 78.00%(Calibrated): Training Loss = 0.1523, Accuracy = 0.8477, Validation Loss = 0.1226, Validation Accuracy = 0.8172\n",
            "Timestep 78.50%(Calibrated): Training Loss = 0.1486, Accuracy = 0.8514, Validation Loss = 0.1113, Validation Accuracy = 0.8447\n",
            "Timestep 79.00%(Calibrated): Training Loss = 0.1295, Accuracy = 0.8705, Validation Loss = 0.0932, Validation Accuracy = 0.8653\n",
            "Timestep 79.50%(Calibrated): Training Loss = 0.1475, Accuracy = 0.8525, Validation Loss = 0.1099, Validation Accuracy = 0.8371\n",
            "Timestep 80.00%(Calibrated): Training Loss = 0.1293, Accuracy = 0.8707, Validation Loss = 0.0935, Validation Accuracy = 0.8700\n",
            "Timestep 80.50%(Calibrated): Training Loss = 0.1333, Accuracy = 0.8667, Validation Loss = 0.0961, Validation Accuracy = 0.8607\n",
            "Timestep 81.00%(Calibrated): Training Loss = 0.1428, Accuracy = 0.8572, Validation Loss = 0.0928, Validation Accuracy = 0.8734\n",
            "Timestep 81.50%(Calibrated): Training Loss = 0.1349, Accuracy = 0.8651, Validation Loss = 0.0913, Validation Accuracy = 0.8737\n",
            "Timestep 82.00%(Calibrated): Training Loss = 0.1601, Accuracy = 0.8399, Validation Loss = 0.0786, Validation Accuracy = 0.8873\n",
            "Timestep 82.50%(Calibrated): Training Loss = 0.1453, Accuracy = 0.8547, Validation Loss = 0.1041, Validation Accuracy = 0.8400\n",
            "Timestep 83.00%(Calibrated): Training Loss = 0.1380, Accuracy = 0.8620, Validation Loss = 0.0934, Validation Accuracy = 0.8678\n",
            "Timestep 83.50%(Calibrated): Training Loss = 0.1182, Accuracy = 0.8818, Validation Loss = 0.0795, Validation Accuracy = 0.8832\n",
            "Timestep 84.00%(Calibrated): Training Loss = 0.1223, Accuracy = 0.8777, Validation Loss = 0.0927, Validation Accuracy = 0.8635\n",
            "Timestep 84.50%(Calibrated): Training Loss = 0.1454, Accuracy = 0.8546, Validation Loss = 0.0973, Validation Accuracy = 0.8616\n",
            "Timestep 85.00%(Calibrated): Training Loss = 0.1264, Accuracy = 0.8736, Validation Loss = 0.0723, Validation Accuracy = 0.8905\n",
            "Timestep 85.50%(Calibrated): Training Loss = 0.1448, Accuracy = 0.8552, Validation Loss = 0.0924, Validation Accuracy = 0.8661\n",
            "Timestep 86.00%(Calibrated): Training Loss = 0.1236, Accuracy = 0.8764, Validation Loss = 0.0929, Validation Accuracy = 0.8674\n",
            "Timestep 86.50%(Calibrated): Training Loss = 0.1215, Accuracy = 0.8785, Validation Loss = 0.0733, Validation Accuracy = 0.9058\n",
            "Timestep 87.00%(Calibrated): Training Loss = 0.1416, Accuracy = 0.8584, Validation Loss = 0.0950, Validation Accuracy = 0.8614\n",
            "Timestep 87.50%(Calibrated): Training Loss = 0.1235, Accuracy = 0.8765, Validation Loss = 0.0757, Validation Accuracy = 0.8884\n",
            "Timestep 88.00%(Calibrated): Training Loss = 0.1275, Accuracy = 0.8725, Validation Loss = 0.0875, Validation Accuracy = 0.8713\n",
            "Timestep 88.50%(Calibrated): Training Loss = 0.1332, Accuracy = 0.8668, Validation Loss = 0.0939, Validation Accuracy = 0.8613\n",
            "Timestep 89.00%(Calibrated): Training Loss = 0.1355, Accuracy = 0.8645, Validation Loss = 0.0853, Validation Accuracy = 0.8737\n",
            "Timestep 89.50%(Calibrated): Training Loss = 0.1136, Accuracy = 0.8864, Validation Loss = 0.0772, Validation Accuracy = 0.8871\n",
            "Timestep 90.00%(Calibrated): Training Loss = 0.1251, Accuracy = 0.8749, Validation Loss = 0.0938, Validation Accuracy = 0.8644\n",
            "Timestep 90.50%(Calibrated): Training Loss = 0.1130, Accuracy = 0.8870, Validation Loss = 0.0769, Validation Accuracy = 0.8884\n",
            "Timestep 91.00%(Calibrated): Training Loss = 0.1384, Accuracy = 0.8616, Validation Loss = 0.0725, Validation Accuracy = 0.8963\n",
            "Timestep 91.50%(Calibrated): Training Loss = 0.1055, Accuracy = 0.8945, Validation Loss = 0.0800, Validation Accuracy = 0.8898\n",
            "Timestep 92.00%(Calibrated): Training Loss = 0.1269, Accuracy = 0.8731, Validation Loss = 0.0806, Validation Accuracy = 0.8865\n",
            "Timestep 92.50%(Calibrated): Training Loss = 0.1055, Accuracy = 0.8945, Validation Loss = 0.0747, Validation Accuracy = 0.8909\n",
            "Timestep 93.00%(Calibrated): Training Loss = 0.1037, Accuracy = 0.8963, Validation Loss = 0.0797, Validation Accuracy = 0.8876\n",
            "Timestep 93.50%(Calibrated): Training Loss = 0.1228, Accuracy = 0.8772, Validation Loss = 0.0737, Validation Accuracy = 0.8912\n",
            "Timestep 94.00%(Calibrated): Training Loss = 0.0894, Accuracy = 0.9106, Validation Loss = 0.0612, Validation Accuracy = 0.9069\n",
            "Timestep 94.50%(Calibrated): Training Loss = 0.0933, Accuracy = 0.9067, Validation Loss = 0.0662, Validation Accuracy = 0.9018\n",
            "Timestep 95.00%(Calibrated): Training Loss = 0.1106, Accuracy = 0.8894, Validation Loss = 0.0692, Validation Accuracy = 0.8942\n",
            "Timestep 95.50%(Calibrated): Training Loss = 0.1000, Accuracy = 0.9000, Validation Loss = 0.0762, Validation Accuracy = 0.8831\n",
            "Timestep 96.00%(Calibrated): Training Loss = 0.1095, Accuracy = 0.8905, Validation Loss = 0.0791, Validation Accuracy = 0.8885\n",
            "Timestep 96.50%(Calibrated): Training Loss = 0.1051, Accuracy = 0.8949, Validation Loss = 0.0667, Validation Accuracy = 0.9054\n",
            "Timestep 97.00%(Calibrated): Training Loss = 0.0969, Accuracy = 0.9031, Validation Loss = 0.0690, Validation Accuracy = 0.9057\n",
            "Timestep 97.50%(Calibrated): Training Loss = 0.1008, Accuracy = 0.8992, Validation Loss = 0.0782, Validation Accuracy = 0.8896\n",
            "Timestep 98.00%(Calibrated): Training Loss = 0.0852, Accuracy = 0.9148, Validation Loss = 0.0596, Validation Accuracy = 0.9253\n",
            "Timestep 98.50%(Calibrated): Training Loss = 0.0849, Accuracy = 0.9151, Validation Loss = 0.0606, Validation Accuracy = 0.9163\n",
            "Timestep 99.00%(Calibrated): Training Loss = 0.0916, Accuracy = 0.9084, Validation Loss = 0.0820, Validation Accuracy = 0.8854\n",
            "Timestep 99.50%(Calibrated): Training Loss = 0.0825, Accuracy = 0.9175, Validation Loss = 0.0740, Validation Accuracy = 0.8952\n",
            "Completed 200/201 timesteps\n",
            "Timestep 100.00%(Calibrated): Training Loss = 0.0622, Accuracy = 0.9378, Validation Loss = 0.0471, Validation Accuracy = 0.9344\n",
            "Completed 201/201 timesteps\n"
          ]
        }
      ],
      "source": [
        "all_models[\"xgboost\"] = setup_xgboost_models(training_data, None, numeric_features, other_features, features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['xgboost', 'nn', 'logistic', 'lstm'])\n",
            "Generating predictions for timestep 0.0\n",
            "Generating predictions for timestep 0.005\n",
            "Generating predictions for timestep 0.01\n",
            "Generating predictions for timestep 0.015\n",
            "Generating predictions for timestep 0.02\n",
            "Generating predictions for timestep 0.025\n",
            "Generating predictions for timestep 0.03\n",
            "Generating predictions for timestep 0.035\n",
            "Generating predictions for timestep 0.04\n",
            "Generating predictions for timestep 0.045\n",
            "Generating predictions for timestep 0.05\n",
            "Generating predictions for timestep 0.055\n",
            "Generating predictions for timestep 0.06\n",
            "Generating predictions for timestep 0.065\n",
            "Generating predictions for timestep 0.07\n",
            "Generating predictions for timestep 0.075\n",
            "Generating predictions for timestep 0.08\n",
            "Generating predictions for timestep 0.085\n",
            "Generating predictions for timestep 0.09\n",
            "Generating predictions for timestep 0.095\n",
            "Generating predictions for timestep 0.1\n",
            "Generating predictions for timestep 0.105\n",
            "Generating predictions for timestep 0.11\n",
            "Generating predictions for timestep 0.115\n",
            "Generating predictions for timestep 0.12\n",
            "Generating predictions for timestep 0.125\n",
            "Generating predictions for timestep 0.13\n",
            "Generating predictions for timestep 0.135\n",
            "Generating predictions for timestep 0.14\n",
            "Generating predictions for timestep 0.145\n",
            "Generating predictions for timestep 0.15\n",
            "Generating predictions for timestep 0.155\n",
            "Generating predictions for timestep 0.16\n",
            "Generating predictions for timestep 0.165\n",
            "Generating predictions for timestep 0.17\n",
            "Generating predictions for timestep 0.175\n",
            "Generating predictions for timestep 0.18\n",
            "Generating predictions for timestep 0.185\n",
            "Generating predictions for timestep 0.19\n",
            "Generating predictions for timestep 0.195\n",
            "Generating predictions for timestep 0.2\n",
            "Generating predictions for timestep 0.205\n",
            "Generating predictions for timestep 0.21\n",
            "Generating predictions for timestep 0.215\n",
            "Generating predictions for timestep 0.22\n",
            "Generating predictions for timestep 0.225\n",
            "Generating predictions for timestep 0.23\n",
            "Generating predictions for timestep 0.235\n",
            "Generating predictions for timestep 0.24\n",
            "Generating predictions for timestep 0.245\n",
            "Generating predictions for timestep 0.25\n",
            "Generating predictions for timestep 0.255\n",
            "Generating predictions for timestep 0.26\n",
            "Generating predictions for timestep 0.265\n",
            "Generating predictions for timestep 0.27\n",
            "Generating predictions for timestep 0.275\n",
            "Generating predictions for timestep 0.28\n",
            "Generating predictions for timestep 0.285\n",
            "Generating predictions for timestep 0.29\n",
            "Generating predictions for timestep 0.295\n",
            "Generating predictions for timestep 0.3\n",
            "Generating predictions for timestep 0.305\n",
            "Generating predictions for timestep 0.31\n",
            "Generating predictions for timestep 0.315\n",
            "Generating predictions for timestep 0.32\n",
            "Generating predictions for timestep 0.325\n",
            "Generating predictions for timestep 0.33\n",
            "Generating predictions for timestep 0.335\n",
            "Generating predictions for timestep 0.34\n",
            "Generating predictions for timestep 0.345\n",
            "Generating predictions for timestep 0.35\n",
            "Generating predictions for timestep 0.355\n",
            "Generating predictions for timestep 0.36\n",
            "Generating predictions for timestep 0.365\n",
            "Generating predictions for timestep 0.37\n",
            "Generating predictions for timestep 0.375\n",
            "Generating predictions for timestep 0.38\n",
            "Generating predictions for timestep 0.385\n",
            "Generating predictions for timestep 0.39\n",
            "Generating predictions for timestep 0.395\n",
            "Generating predictions for timestep 0.4\n",
            "Generating predictions for timestep 0.405\n",
            "Generating predictions for timestep 0.41\n",
            "Generating predictions for timestep 0.415\n",
            "Generating predictions for timestep 0.42\n",
            "Generating predictions for timestep 0.425\n",
            "Generating predictions for timestep 0.43\n",
            "Generating predictions for timestep 0.435\n",
            "Generating predictions for timestep 0.44\n",
            "Generating predictions for timestep 0.445\n",
            "Generating predictions for timestep 0.45\n",
            "Generating predictions for timestep 0.455\n",
            "Generating predictions for timestep 0.46\n",
            "Generating predictions for timestep 0.465\n",
            "Generating predictions for timestep 0.47\n",
            "Generating predictions for timestep 0.475\n",
            "Generating predictions for timestep 0.48\n",
            "Generating predictions for timestep 0.485\n",
            "Generating predictions for timestep 0.49\n",
            "Generating predictions for timestep 0.495\n",
            "Generating predictions for timestep 0.5\n",
            "Generating predictions for timestep 0.505\n",
            "Generating predictions for timestep 0.51\n",
            "Generating predictions for timestep 0.515\n",
            "Generating predictions for timestep 0.52\n",
            "Generating predictions for timestep 0.525\n",
            "Generating predictions for timestep 0.53\n",
            "Generating predictions for timestep 0.535\n",
            "Generating predictions for timestep 0.54\n",
            "Generating predictions for timestep 0.545\n",
            "Generating predictions for timestep 0.55\n",
            "Generating predictions for timestep 0.555\n",
            "Generating predictions for timestep 0.56\n",
            "Generating predictions for timestep 0.565\n",
            "Generating predictions for timestep 0.57\n",
            "Generating predictions for timestep 0.575\n",
            "Generating predictions for timestep 0.58\n",
            "Generating predictions for timestep 0.585\n",
            "Generating predictions for timestep 0.59\n",
            "Generating predictions for timestep 0.595\n",
            "Generating predictions for timestep 0.6\n",
            "Generating predictions for timestep 0.605\n",
            "Generating predictions for timestep 0.61\n",
            "Generating predictions for timestep 0.615\n",
            "Generating predictions for timestep 0.62\n",
            "Generating predictions for timestep 0.625\n",
            "Generating predictions for timestep 0.63\n",
            "Generating predictions for timestep 0.635\n",
            "Generating predictions for timestep 0.64\n",
            "Generating predictions for timestep 0.645\n",
            "Generating predictions for timestep 0.65\n",
            "Generating predictions for timestep 0.655\n",
            "Generating predictions for timestep 0.66\n",
            "Generating predictions for timestep 0.665\n",
            "Generating predictions for timestep 0.67\n",
            "Generating predictions for timestep 0.675\n",
            "Generating predictions for timestep 0.68\n",
            "Generating predictions for timestep 0.685\n",
            "Generating predictions for timestep 0.69\n",
            "Generating predictions for timestep 0.695\n",
            "Generating predictions for timestep 0.7\n",
            "Generating predictions for timestep 0.705\n",
            "Generating predictions for timestep 0.71\n",
            "Generating predictions for timestep 0.715\n",
            "Generating predictions for timestep 0.72\n",
            "Generating predictions for timestep 0.725\n",
            "Generating predictions for timestep 0.73\n",
            "Generating predictions for timestep 0.735\n",
            "Generating predictions for timestep 0.74\n",
            "Generating predictions for timestep 0.745\n",
            "Generating predictions for timestep 0.75\n",
            "Generating predictions for timestep 0.755\n",
            "Generating predictions for timestep 0.76\n",
            "Generating predictions for timestep 0.765\n",
            "Generating predictions for timestep 0.77\n",
            "Generating predictions for timestep 0.775\n",
            "Generating predictions for timestep 0.78\n",
            "Generating predictions for timestep 0.785\n",
            "Generating predictions for timestep 0.79\n",
            "Generating predictions for timestep 0.795\n",
            "Generating predictions for timestep 0.8\n",
            "Generating predictions for timestep 0.805\n",
            "Generating predictions for timestep 0.81\n",
            "Generating predictions for timestep 0.815\n",
            "Generating predictions for timestep 0.82\n",
            "Generating predictions for timestep 0.825\n",
            "Generating predictions for timestep 0.83\n",
            "Generating predictions for timestep 0.835\n",
            "Generating predictions for timestep 0.84\n",
            "Generating predictions for timestep 0.845\n",
            "Generating predictions for timestep 0.85\n",
            "Generating predictions for timestep 0.855\n",
            "Generating predictions for timestep 0.86\n",
            "Generating predictions for timestep 0.865\n",
            "Generating predictions for timestep 0.87\n",
            "Generating predictions for timestep 0.875\n",
            "Generating predictions for timestep 0.88\n",
            "Generating predictions for timestep 0.885\n",
            "Generating predictions for timestep 0.89\n",
            "Generating predictions for timestep 0.895\n",
            "Generating predictions for timestep 0.9\n",
            "Generating predictions for timestep 0.905\n",
            "Generating predictions for timestep 0.91\n",
            "Generating predictions for timestep 0.915\n",
            "Generating predictions for timestep 0.92\n",
            "Generating predictions for timestep 0.925\n",
            "Generating predictions for timestep 0.93\n",
            "Generating predictions for timestep 0.935\n",
            "Generating predictions for timestep 0.94\n",
            "Generating predictions for timestep 0.945\n",
            "Generating predictions for timestep 0.95\n",
            "Generating predictions for timestep 0.955\n",
            "Generating predictions for timestep 0.96\n",
            "Generating predictions for timestep 0.965\n",
            "Generating predictions for timestep 0.97\n",
            "Generating predictions for timestep 0.975\n",
            "Generating predictions for timestep 0.98\n",
            "Generating predictions for timestep 0.985\n",
            "Generating predictions for timestep 0.99\n",
            "Generating predictions for timestep 0.995\n",
            "Generating predictions for timestep 1.0\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "print(all_models.keys())\n",
        "all_models_order = [\"xgboost\", \"nn\", \"logistic\", \"lstm\"] # Strict ordering of models\n",
        "def generate_ensemble_matrix(models, all_models_order, data_dict_seq):\n",
        "    \"\"\"Generate predictions from a specific model type on given data\"\"\"\n",
        "    predictions = {}\n",
        "    # predictions:\n",
        "        # timestep:\n",
        "            # \"predictions\": [model_1_predictions, model_2_predictions, ..., model_n_predictions, \n",
        "            # model_1_oredictions_seq, model_2_oredictions_seq, ..., model_n_oredictions_seq],\n",
        "            # \"y_true\": y_true\n",
        "    for timestep in data_dict_seq:\n",
        "        print(f\"Generating predictions for timestep {timestep}\")\n",
        "        # For each entry, take the last array from the \"rows\" list and pair with its label\n",
        "        non_sequential_data_for_timestep = [{\"rows\": entry[\"rows\"][-1], \"label\": entry[\"label\"]} for entry in data_dict_seq[timestep]]\n",
        "        X = np.array([row[\"rows\"] for row in non_sequential_data_for_timestep])\n",
        "        y = np.array([row[\"label\"] for row in non_sequential_data_for_timestep])\n",
        "        X_seq = np.array([row[\"rows\"] for row in data_dict_seq[timestep]])\n",
        "        predictions[timestep] = {\"predictions\": [], \"y_true\": []}\n",
        "        for i in range(len(X)):\n",
        "            predictions[timestep][\"predictions\"].append(np.array([\n",
        "                models[model][timestep].predict_proba(np.expand_dims(X_seq[i], axis=0))[:, 1].item() if model == \"lstm\" else models[model][timestep].predict_proba(np.expand_dims(X[i], axis=0))[:, 1].item()\n",
        "                for model in all_models_order\n",
        "            ]))\n",
        "            predictions[timestep][\"y_true\"].append(y[i])\n",
        "        predictions[timestep][\"y_true\"] = np.array(predictions[timestep][\"y_true\"])\n",
        "        predictions[timestep][\"predictions\"] = np.array(predictions[timestep][\"predictions\"])\n",
        "    return predictions\n",
        "\n",
        "ensemble_matrices = generate_ensemble_matrix(all_models, all_models_order, ensemble_data_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegressionMetaModel:\n",
        "    def __init__(self):\n",
        "        self.meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        self.meta_model.fit(X, y)\n",
        "        if X_val is not None and y_val is not None:\n",
        "            self.meta_model.fit(X_val, y_val)\n",
        "    def predict(self, X):\n",
        "        return 1 if self.meta_model.predict_proba(X)[:, 1] > 0.5 else 0\n",
        "    def predict_proba(self, X):\n",
        "        return self.meta_model.predict_proba(X)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class SimpleMetaNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleMetaNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class NeuralNetworkMetaModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim=None, lr=0.01, epochs=30, batch_size=32, device=None):\n",
        "        self.input_dim = input_dim\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.input_dim is None:\n",
        "            self.input_dim = X.shape[1]\n",
        "        self.model = SimpleMetaNN(self.input_dim).to(self.device)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for xb, yb in loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = self.model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = self.model(X_tensor).cpu().numpy().flatten()\n",
        "        return (probs > 0.5).astype(int)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = self.model(X_tensor).cpu().numpy().flatten()\n",
        "        return np.column_stack([1 - probs, probs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training meta-model for timestep 0.0\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.253, 'nn': 0.0, 'logistic': 0.0, 'lstm': 0.747} (score: 0.230930)\n",
            "Training meta-model for timestep 0.005\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1021, 'nn': 0.5613, 'logistic': 0.0, 'lstm': 0.3365} (score: 0.239375)\n",
            "Training meta-model for timestep 0.01\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7646, 'lstm': 0.2354} (score: 0.228538)\n",
            "Training meta-model for timestep 0.015\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3112, 'logistic': 0.6888, 'lstm': 0.0} (score: 0.226704)\n",
            "Training meta-model for timestep 0.02\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3002, 'nn': 0.5008, 'logistic': 0.199, 'lstm': 0.0} (score: 0.230613)\n",
            "Training meta-model for timestep 0.025\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3946, 'nn': 0.2118, 'logistic': 0.0, 'lstm': 0.3936} (score: 0.230149)\n",
            "Training meta-model for timestep 0.03\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0381, 'nn': 0.0, 'logistic': 0.547, 'lstm': 0.4149} (score: 0.225601)\n",
            "Training meta-model for timestep 0.035\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6433, 'nn': 0.0, 'logistic': 0.3435, 'lstm': 0.0133} (score: 0.227181)\n",
            "Training meta-model for timestep 0.04\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5734, 'nn': 0.084, 'logistic': 0.248, 'lstm': 0.0945} (score: 0.235240)\n",
            "Training meta-model for timestep 0.045\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3772, 'logistic': 0.2865, 'lstm': 0.3363} (score: 0.236913)\n",
            "Training meta-model for timestep 0.05\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.252, 'nn': 0.0727, 'logistic': 0.5, 'lstm': 0.1753} (score: 0.228883)\n",
            "Training meta-model for timestep 0.055\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5541, 'nn': 0.0, 'logistic': 0.2717, 'lstm': 0.1743} (score: 0.218022)\n",
            "Training meta-model for timestep 0.06\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6858, 'nn': 0.3142, 'logistic': 0.0, 'lstm': 0.0} (score: 0.230031)\n",
            "Training meta-model for timestep 0.065\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0554, 'nn': 0.0347, 'logistic': 0.5782, 'lstm': 0.3317} (score: 0.231471)\n",
            "Training meta-model for timestep 0.07\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.226317)\n",
            "Training meta-model for timestep 0.075\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5076, 'nn': 0.1388, 'logistic': 0.3535, 'lstm': 0.0} (score: 0.226369)\n",
            "Training meta-model for timestep 0.08\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.9699, 'lstm': 0.0301} (score: 0.221233)\n",
            "Training meta-model for timestep 0.085\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3151, 'nn': 0.294, 'logistic': 0.3908, 'lstm': 0.0} (score: 0.213206)\n",
            "Training meta-model for timestep 0.09\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3044, 'logistic': 0.6956, 'lstm': 0.0} (score: 0.220260)\n",
            "Training meta-model for timestep 0.095\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0933, 'nn': 0.1267, 'logistic': 0.7801, 'lstm': 0.0} (score: 0.207558)\n",
            "Training meta-model for timestep 0.1\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3237, 'logistic': 0.4833, 'lstm': 0.193} (score: 0.216968)\n",
            "Training meta-model for timestep 0.105\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.5702, 'lstm': 0.4298} (score: 0.213420)\n",
            "Training meta-model for timestep 0.11\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4256, 'nn': 0.25, 'logistic': 0.1993, 'lstm': 0.1251} (score: 0.222236)\n",
            "Training meta-model for timestep 0.115\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.1834, 'logistic': 0.8166, 'lstm': 0.0} (score: 0.217879)\n",
            "Training meta-model for timestep 0.12\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1521, 'nn': 0.202, 'logistic': 0.6459, 'lstm': 0.0} (score: 0.216559)\n",
            "Training meta-model for timestep 0.125\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2145, 'nn': 0.0, 'logistic': 0.7855, 'lstm': 0.0} (score: 0.233405)\n",
            "Training meta-model for timestep 0.13\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4572, 'nn': 0.3371, 'logistic': 0.1988, 'lstm': 0.0068} (score: 0.212039)\n",
            "Training meta-model for timestep 0.135\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1274, 'nn': 0.0241, 'logistic': 0.7097, 'lstm': 0.1387} (score: 0.222300)\n",
            "Training meta-model for timestep 0.14\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.1836, 'logistic': 0.4327, 'lstm': 0.3836} (score: 0.218762)\n",
            "Training meta-model for timestep 0.145\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.1836, 'logistic': 0.7856, 'lstm': 0.0309} (score: 0.221920)\n",
            "Training meta-model for timestep 0.15\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.088, 'nn': 0.0, 'logistic': 0.912, 'lstm': 0.0} (score: 0.213878)\n",
            "Training meta-model for timestep 0.155\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0677, 'nn': 0.0, 'logistic': 0.4536, 'lstm': 0.4787} (score: 0.197417)\n",
            "Training meta-model for timestep 0.16\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1619, 'nn': 0.1038, 'logistic': 0.6972, 'lstm': 0.0371} (score: 0.212407)\n",
            "Training meta-model for timestep 0.165\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.177, 'nn': 0.1249, 'logistic': 0.4779, 'lstm': 0.2202} (score: 0.217471)\n",
            "Training meta-model for timestep 0.17\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7781, 'lstm': 0.2219} (score: 0.220814)\n",
            "Training meta-model for timestep 0.175\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.5643, 'logistic': 0.1834, 'lstm': 0.2524} (score: 0.219714)\n",
            "Training meta-model for timestep 0.18\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.206654)\n",
            "Training meta-model for timestep 0.185\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.4248, 'logistic': 0.5752, 'lstm': 0.0} (score: 0.211433)\n",
            "Training meta-model for timestep 0.19\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3807, 'nn': 0.0, 'logistic': 0.5196, 'lstm': 0.0996} (score: 0.221723)\n",
            "Training meta-model for timestep 0.195\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2275, 'nn': 0.0616, 'logistic': 0.5923, 'lstm': 0.1185} (score: 0.201215)\n",
            "Training meta-model for timestep 0.2\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7817, 'lstm': 0.2183} (score: 0.210467)\n",
            "Training meta-model for timestep 0.205\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.354, 'nn': 0.033, 'logistic': 0.4584, 'lstm': 0.1546} (score: 0.215698)\n",
            "Training meta-model for timestep 0.21\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0394, 'nn': 0.0, 'logistic': 0.9382, 'lstm': 0.0224} (score: 0.212875)\n",
            "Training meta-model for timestep 0.215\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0574, 'nn': 0.1241, 'logistic': 0.7644, 'lstm': 0.054} (score: 0.217642)\n",
            "Training meta-model for timestep 0.22\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0015, 'nn': 0.0, 'logistic': 0.7898, 'lstm': 0.2086} (score: 0.205972)\n",
            "Training meta-model for timestep 0.225\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2403, 'nn': 0.1467, 'logistic': 0.613, 'lstm': 0.0} (score: 0.201701)\n",
            "Training meta-model for timestep 0.23\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0813, 'nn': 0.2722, 'logistic': 0.3148, 'lstm': 0.3316} (score: 0.223292)\n",
            "Training meta-model for timestep 0.235\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0345, 'nn': 0.0, 'logistic': 0.9655, 'lstm': 0.0} (score: 0.200287)\n",
            "Training meta-model for timestep 0.24\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5742, 'nn': 0.2178, 'logistic': 0.1991, 'lstm': 0.0089} (score: 0.203558)\n",
            "Training meta-model for timestep 0.245\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3003, 'nn': 0.0, 'logistic': 0.6997, 'lstm': 0.0} (score: 0.214079)\n",
            "Training meta-model for timestep 0.25\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.211542)\n",
            "Training meta-model for timestep 0.255\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1464, 'nn': 0.0, 'logistic': 0.0, 'lstm': 0.8536} (score: 0.197663)\n",
            "Training meta-model for timestep 0.26\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.1474, 'logistic': 0.8526, 'lstm': 0.0} (score: 0.206209)\n",
            "Training meta-model for timestep 0.265\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6481, 'nn': 0.1009, 'logistic': 0.251, 'lstm': 0.0} (score: 0.213924)\n",
            "Training meta-model for timestep 0.27\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6639, 'nn': 0.0, 'logistic': 0.0212, 'lstm': 0.3149} (score: 0.208569)\n",
            "Training meta-model for timestep 0.275\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0339, 'nn': 0.0, 'logistic': 0.5443, 'lstm': 0.4218} (score: 0.201805)\n",
            "Training meta-model for timestep 0.28\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2277, 'nn': 0.0, 'logistic': 0.7016, 'lstm': 0.0708} (score: 0.206499)\n",
            "Training meta-model for timestep 0.285\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2115, 'nn': 0.0, 'logistic': 0.7885, 'lstm': 0.0} (score: 0.199234)\n",
            "Training meta-model for timestep 0.29\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4468, 'nn': 0.2575, 'logistic': 0.2957, 'lstm': 0.0} (score: 0.215954)\n",
            "Training meta-model for timestep 0.295\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.165, 'logistic': 0.835, 'lstm': 0.0} (score: 0.215243)\n",
            "Training meta-model for timestep 0.3\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1373, 'nn': 0.0758, 'logistic': 0.7534, 'lstm': 0.0335} (score: 0.198836)\n",
            "Training meta-model for timestep 0.305\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2032, 'nn': 0.5035, 'logistic': 0.2933, 'lstm': 0.0} (score: 0.211011)\n",
            "Training meta-model for timestep 0.31\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3505, 'nn': 0.3673, 'logistic': 0.2822, 'lstm': 0.0} (score: 0.192219)\n",
            "Training meta-model for timestep 0.315\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2234, 'nn': 0.0, 'logistic': 0.7766, 'lstm': 0.0} (score: 0.204516)\n",
            "Training meta-model for timestep 0.32\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1684, 'nn': 0.0, 'logistic': 0.5325, 'lstm': 0.2991} (score: 0.219661)\n",
            "Training meta-model for timestep 0.325\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3369, 'nn': 0.0827, 'logistic': 0.5707, 'lstm': 0.0097} (score: 0.213228)\n",
            "Training meta-model for timestep 0.33\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2568, 'nn': 0.0083, 'logistic': 0.511, 'lstm': 0.2239} (score: 0.211834)\n",
            "Training meta-model for timestep 0.335\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5462, 'nn': 0.0, 'logistic': 0.3915, 'lstm': 0.0623} (score: 0.190179)\n",
            "Training meta-model for timestep 0.34\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6168, 'nn': 0.3023, 'logistic': 0.0, 'lstm': 0.0809} (score: 0.186835)\n",
            "Training meta-model for timestep 0.345\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4224, 'nn': 0.0761, 'logistic': 0.3231, 'lstm': 0.1784} (score: 0.207599)\n",
            "Training meta-model for timestep 0.35\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3684, 'nn': 0.0, 'logistic': 0.5254, 'lstm': 0.1062} (score: 0.208832)\n",
            "Training meta-model for timestep 0.355\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1444, 'nn': 0.0326, 'logistic': 0.823, 'lstm': 0.0} (score: 0.201443)\n",
            "Training meta-model for timestep 0.36\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0674, 'logistic': 0.8938, 'lstm': 0.0388} (score: 0.194635)\n",
            "Training meta-model for timestep 0.365\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1235, 'nn': 0.0, 'logistic': 0.6118, 'lstm': 0.2647} (score: 0.206934)\n",
            "Training meta-model for timestep 0.37\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0416, 'logistic': 0.3127, 'lstm': 0.6457} (score: 0.204499)\n",
            "Training meta-model for timestep 0.375\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.6447, 'lstm': 0.3553} (score: 0.194827)\n",
            "Training meta-model for timestep 0.38\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0498, 'nn': 0.0, 'logistic': 0.9137, 'lstm': 0.0365} (score: 0.191937)\n",
            "Training meta-model for timestep 0.385\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.418, 'nn': 0.0, 'logistic': 0.4552, 'lstm': 0.1267} (score: 0.205242)\n",
            "Training meta-model for timestep 0.39\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.049, 'nn': 0.0, 'logistic': 0.951, 'lstm': 0.0} (score: 0.193592)\n",
            "Training meta-model for timestep 0.395\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4125, 'nn': 0.0, 'logistic': 0.5386, 'lstm': 0.0489} (score: 0.204933)\n",
            "Training meta-model for timestep 0.4\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3966, 'logistic': 0.3768, 'lstm': 0.2266} (score: 0.195977)\n",
            "Training meta-model for timestep 0.405\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0897, 'nn': 0.6503, 'logistic': 0.2433, 'lstm': 0.0167} (score: 0.192211)\n",
            "Training meta-model for timestep 0.41\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0527, 'nn': 0.8657, 'logistic': 0.0, 'lstm': 0.0815} (score: 0.211345)\n",
            "Training meta-model for timestep 0.415\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.199846)\n",
            "Training meta-model for timestep 0.42\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0312, 'nn': 0.3209, 'logistic': 0.3474, 'lstm': 0.3005} (score: 0.206882)\n",
            "Training meta-model for timestep 0.425\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0456, 'nn': 0.0, 'logistic': 0.5505, 'lstm': 0.4039} (score: 0.190570)\n",
            "Training meta-model for timestep 0.43\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.203, 'nn': 0.0, 'logistic': 0.797, 'lstm': 0.0} (score: 0.195934)\n",
            "Training meta-model for timestep 0.435\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0209, 'logistic': 0.9174, 'lstm': 0.0617} (score: 0.190614)\n",
            "Training meta-model for timestep 0.44\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0442, 'nn': 0.0, 'logistic': 0.3823, 'lstm': 0.5735} (score: 0.187615)\n",
            "Training meta-model for timestep 0.445\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1838, 'nn': 0.0684, 'logistic': 0.6503, 'lstm': 0.0974} (score: 0.205543)\n",
            "Training meta-model for timestep 0.45\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1563, 'nn': 0.2352, 'logistic': 0.6031, 'lstm': 0.0053} (score: 0.186251)\n",
            "Training meta-model for timestep 0.455\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4025, 'nn': 0.1343, 'logistic': 0.2957, 'lstm': 0.1676} (score: 0.195041)\n",
            "Training meta-model for timestep 0.46\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2605, 'nn': 0.0, 'logistic': 0.2991, 'lstm': 0.4404} (score: 0.183583)\n",
            "Training meta-model for timestep 0.465\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0001, 'logistic': 0.8074, 'lstm': 0.1925} (score: 0.183865)\n",
            "Training meta-model for timestep 0.47\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4365, 'nn': 0.0567, 'logistic': 0.5068, 'lstm': 0.0} (score: 0.190207)\n",
            "Training meta-model for timestep 0.475\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2461, 'nn': 0.2555, 'logistic': 0.4985, 'lstm': 0.0} (score: 0.198933)\n",
            "Training meta-model for timestep 0.48\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1669, 'nn': 0.0, 'logistic': 0.2459, 'lstm': 0.5872} (score: 0.175631)\n",
            "Training meta-model for timestep 0.485\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1864, 'nn': 0.0, 'logistic': 0.5725, 'lstm': 0.241} (score: 0.176758)\n",
            "Training meta-model for timestep 0.49\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2633, 'nn': 0.0335, 'logistic': 0.5935, 'lstm': 0.1097} (score: 0.177823)\n",
            "Training meta-model for timestep 0.495\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.174096)\n",
            "Training meta-model for timestep 0.5\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.178262)\n",
            "Training meta-model for timestep 0.505\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3154, 'nn': 0.4117, 'logistic': 0.273, 'lstm': 0.0} (score: 0.166811)\n",
            "Training meta-model for timestep 0.51\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5079, 'nn': 0.1879, 'logistic': 0.3043, 'lstm': 0.0} (score: 0.167101)\n",
            "Training meta-model for timestep 0.515\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4353, 'nn': 0.0, 'logistic': 0.5647, 'lstm': 0.0} (score: 0.183579)\n",
            "Training meta-model for timestep 0.52\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.714, 'lstm': 0.286} (score: 0.179991)\n",
            "Training meta-model for timestep 0.525\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 1.0, 'nn': 0.0, 'logistic': 0.0, 'lstm': 0.0} (score: 0.170990)\n",
            "Training meta-model for timestep 0.53\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3103, 'nn': 0.0, 'logistic': 0.6897, 'lstm': 0.0} (score: 0.172888)\n",
            "Training meta-model for timestep 0.535\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3224, 'nn': 0.0, 'logistic': 0.6776, 'lstm': 0.0} (score: 0.173487)\n",
            "Training meta-model for timestep 0.54\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1847, 'nn': 0.0789, 'logistic': 0.7364, 'lstm': 0.0} (score: 0.164522)\n",
            "Training meta-model for timestep 0.545\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1612, 'nn': 0.0, 'logistic': 0.8388, 'lstm': 0.0} (score: 0.182610)\n",
            "Training meta-model for timestep 0.55\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3262, 'nn': 0.0, 'logistic': 0.6662, 'lstm': 0.0077} (score: 0.176114)\n",
            "Training meta-model for timestep 0.555\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4074, 'nn': 0.0, 'logistic': 0.5849, 'lstm': 0.0077} (score: 0.173882)\n",
            "Training meta-model for timestep 0.56\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.9051, 'lstm': 0.0949} (score: 0.156565)\n",
            "Training meta-model for timestep 0.565\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2169, 'nn': 0.0026, 'logistic': 0.7805, 'lstm': 0.0} (score: 0.166307)\n",
            "Training meta-model for timestep 0.57\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.168325)\n",
            "Training meta-model for timestep 0.575\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3955, 'nn': 0.0, 'logistic': 0.3501, 'lstm': 0.2543} (score: 0.179282)\n",
            "Training meta-model for timestep 0.58\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4303, 'nn': 0.0763, 'logistic': 0.4142, 'lstm': 0.0792} (score: 0.166669)\n",
            "Training meta-model for timestep 0.585\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1348, 'nn': 0.0, 'logistic': 0.8652, 'lstm': 0.0} (score: 0.164886)\n",
            "Training meta-model for timestep 0.59\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.8584, 'nn': 0.0, 'logistic': 0.1362, 'lstm': 0.0054} (score: 0.177285)\n",
            "Training meta-model for timestep 0.595\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1148, 'nn': 0.0, 'logistic': 0.8852, 'lstm': 0.0} (score: 0.159682)\n",
            "Training meta-model for timestep 0.6\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4434, 'nn': 0.1552, 'logistic': 0.2945, 'lstm': 0.1069} (score: 0.174242)\n",
            "Training meta-model for timestep 0.605\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2985, 'nn': 0.1347, 'logistic': 0.5256, 'lstm': 0.0412} (score: 0.170463)\n",
            "Training meta-model for timestep 0.61\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7653, 'lstm': 0.2347} (score: 0.165454)\n",
            "Training meta-model for timestep 0.615\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1209, 'nn': 0.0547, 'logistic': 0.5608, 'lstm': 0.2636} (score: 0.169094)\n",
            "Training meta-model for timestep 0.62\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3114, 'logistic': 0.5615, 'lstm': 0.1271} (score: 0.170304)\n",
            "Training meta-model for timestep 0.625\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5207, 'nn': 0.0, 'logistic': 0.4793, 'lstm': 0.0} (score: 0.158338)\n",
            "Training meta-model for timestep 0.63\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0309, 'nn': 0.1941, 'logistic': 0.4152, 'lstm': 0.3598} (score: 0.156136)\n",
            "Training meta-model for timestep 0.635\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1311, 'nn': 0.2155, 'logistic': 0.6534, 'lstm': 0.0} (score: 0.162816)\n",
            "Training meta-model for timestep 0.64\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0475, 'nn': 0.3563, 'logistic': 0.5524, 'lstm': 0.0438} (score: 0.158169)\n",
            "Training meta-model for timestep 0.645\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0329, 'nn': 0.0444, 'logistic': 0.213, 'lstm': 0.7097} (score: 0.163944)\n",
            "Training meta-model for timestep 0.65\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3771, 'nn': 0.1808, 'logistic': 0.3394, 'lstm': 0.1027} (score: 0.159770)\n",
            "Training meta-model for timestep 0.655\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6151, 'nn': 0.0055, 'logistic': 0.3794, 'lstm': 0.0} (score: 0.152872)\n",
            "Training meta-model for timestep 0.66\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.2781, 'logistic': 0.7219, 'lstm': 0.0} (score: 0.166591)\n",
            "Training meta-model for timestep 0.665\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.9382, 'lstm': 0.0618} (score: 0.143534)\n",
            "Training meta-model for timestep 0.67\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1984, 'nn': 0.3082, 'logistic': 0.2703, 'lstm': 0.2231} (score: 0.155055)\n",
            "Training meta-model for timestep 0.675\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1499, 'nn': 0.5075, 'logistic': 0.3426, 'lstm': 0.0} (score: 0.171595)\n",
            "Training meta-model for timestep 0.68\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1464, 'nn': 0.0, 'logistic': 0.7303, 'lstm': 0.1233} (score: 0.168669)\n",
            "Training meta-model for timestep 0.685\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4381, 'nn': 0.0, 'logistic': 0.5619, 'lstm': 0.0} (score: 0.152257)\n",
            "Training meta-model for timestep 0.69\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.147, 'nn': 0.2575, 'logistic': 0.5955, 'lstm': 0.0} (score: 0.146540)\n",
            "Training meta-model for timestep 0.695\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3943, 'nn': 0.2406, 'logistic': 0.307, 'lstm': 0.0582} (score: 0.149546)\n",
            "Training meta-model for timestep 0.7\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.2496, 'logistic': 0.7504, 'lstm': 0.0} (score: 0.154844)\n",
            "Training meta-model for timestep 0.705\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3231, 'logistic': 0.637, 'lstm': 0.0399} (score: 0.152141)\n",
            "Training meta-model for timestep 0.71\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3553, 'nn': 0.0, 'logistic': 0.4256, 'lstm': 0.219} (score: 0.153420)\n",
            "Training meta-model for timestep 0.715\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2695, 'nn': 0.255, 'logistic': 0.4755, 'lstm': 0.0} (score: 0.170351)\n",
            "Training meta-model for timestep 0.72\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0877, 'nn': 0.3403, 'logistic': 0.0, 'lstm': 0.572} (score: 0.149888)\n",
            "Training meta-model for timestep 0.725\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3833, 'nn': 0.1955, 'logistic': 0.2421, 'lstm': 0.1791} (score: 0.150253)\n",
            "Training meta-model for timestep 0.73\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2536, 'nn': 0.0468, 'logistic': 0.2025, 'lstm': 0.4971} (score: 0.146546)\n",
            "Training meta-model for timestep 0.735\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3045, 'nn': 0.1874, 'logistic': 0.2495, 'lstm': 0.2585} (score: 0.152912)\n",
            "Training meta-model for timestep 0.74\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1488, 'nn': 0.0826, 'logistic': 0.4329, 'lstm': 0.3357} (score: 0.139297)\n",
            "Training meta-model for timestep 0.745\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3422, 'nn': 0.452, 'logistic': 0.2058, 'lstm': 0.0} (score: 0.153969)\n",
            "Training meta-model for timestep 0.75\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.107, 'nn': 0.1179, 'logistic': 0.7066, 'lstm': 0.0685} (score: 0.155131)\n",
            "Training meta-model for timestep 0.755\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.123, 'nn': 0.1172, 'logistic': 0.7598, 'lstm': 0.0} (score: 0.146958)\n",
            "Training meta-model for timestep 0.76\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5115, 'nn': 0.2319, 'logistic': 0.2566, 'lstm': 0.0} (score: 0.146973)\n",
            "Training meta-model for timestep 0.765\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0418, 'logistic': 0.5059, 'lstm': 0.4523} (score: 0.123851)\n",
            "Training meta-model for timestep 0.77\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.6112, 'lstm': 0.3888} (score: 0.148954)\n",
            "Training meta-model for timestep 0.775\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3714, 'nn': 0.5391, 'logistic': 0.0802, 'lstm': 0.0092} (score: 0.146985)\n",
            "Training meta-model for timestep 0.78\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4874, 'nn': 0.0, 'logistic': 0.3041, 'lstm': 0.2085} (score: 0.140277)\n",
            "Training meta-model for timestep 0.785\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0375, 'logistic': 0.5011, 'lstm': 0.4614} (score: 0.129726)\n",
            "Training meta-model for timestep 0.79\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3029, 'nn': 0.0431, 'logistic': 0.4966, 'lstm': 0.1574} (score: 0.132051)\n",
            "Training meta-model for timestep 0.795\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4973, 'nn': 0.192, 'logistic': 0.0, 'lstm': 0.3107} (score: 0.145459)\n",
            "Training meta-model for timestep 0.8\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.5082, 'logistic': 0.4199, 'lstm': 0.0719} (score: 0.128894)\n",
            "Training meta-model for timestep 0.805\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0491, 'logistic': 0.7453, 'lstm': 0.2056} (score: 0.133684)\n",
            "Training meta-model for timestep 0.81\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5911, 'nn': 0.395, 'logistic': 0.014, 'lstm': 0.0} (score: 0.134967)\n",
            "Training meta-model for timestep 0.815\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3281, 'nn': 0.0, 'logistic': 0.375, 'lstm': 0.2969} (score: 0.130151)\n",
            "Training meta-model for timestep 0.82\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7624, 'lstm': 0.2376} (score: 0.151309)\n",
            "Training meta-model for timestep 0.825\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0439, 'nn': 0.0, 'logistic': 0.8472, 'lstm': 0.1089} (score: 0.135092)\n",
            "Training meta-model for timestep 0.83\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3119, 'logistic': 0.5909, 'lstm': 0.0972} (score: 0.129890)\n",
            "Training meta-model for timestep 0.835\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.2723, 'logistic': 0.4695, 'lstm': 0.2582} (score: 0.139334)\n",
            "Training meta-model for timestep 0.84\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4531, 'nn': 0.0557, 'logistic': 0.4912, 'lstm': 0.0} (score: 0.129269)\n",
            "Training meta-model for timestep 0.845\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.2004, 'logistic': 0.6297, 'lstm': 0.1698} (score: 0.117129)\n",
            "Training meta-model for timestep 0.85\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.7045, 'lstm': 0.2955} (score: 0.132602)\n",
            "Training meta-model for timestep 0.855\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.4136, 'logistic': 0.3066, 'lstm': 0.2798} (score: 0.136935)\n",
            "Training meta-model for timestep 0.86\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0653, 'nn': 0.1676, 'logistic': 0.74, 'lstm': 0.0272} (score: 0.123702)\n",
            "Training meta-model for timestep 0.865\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 1.0, 'lstm': 0.0} (score: 0.125463)\n",
            "Training meta-model for timestep 0.87\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0, 'logistic': 0.9937, 'lstm': 0.0063} (score: 0.124521)\n",
            "Training meta-model for timestep 0.875\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0291, 'nn': 0.1596, 'logistic': 0.7934, 'lstm': 0.0179} (score: 0.127114)\n",
            "Training meta-model for timestep 0.88\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0012, 'nn': 0.0, 'logistic': 0.9988, 'lstm': 0.0} (score: 0.110412)\n",
            "Training meta-model for timestep 0.885\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.317, 'nn': 0.0, 'logistic': 0.683, 'lstm': 0.0} (score: 0.126082)\n",
            "Training meta-model for timestep 0.89\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5959, 'nn': 0.018, 'logistic': 0.3861, 'lstm': 0.0} (score: 0.101474)\n",
            "Training meta-model for timestep 0.895\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4913, 'nn': 0.0, 'logistic': 0.4057, 'lstm': 0.103} (score: 0.091789)\n",
            "Training meta-model for timestep 0.9\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6779, 'nn': 0.0, 'logistic': 0.1046, 'lstm': 0.2175} (score: 0.108000)\n",
            "Training meta-model for timestep 0.905\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2871, 'nn': 0.2079, 'logistic': 0.5049, 'lstm': 0.0} (score: 0.112040)\n",
            "Training meta-model for timestep 0.91\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3333, 'nn': 0.0, 'logistic': 0.6287, 'lstm': 0.038} (score: 0.120304)\n",
            "Training meta-model for timestep 0.915\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.6427, 'nn': 0.0417, 'logistic': 0.3156, 'lstm': 0.0} (score: 0.088124)\n",
            "Training meta-model for timestep 0.92\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.4698, 'nn': 0.043, 'logistic': 0.3109, 'lstm': 0.1763} (score: 0.098115)\n",
            "Training meta-model for timestep 0.925\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1927, 'nn': 0.0, 'logistic': 0.4085, 'lstm': 0.3988} (score: 0.104260)\n",
            "Training meta-model for timestep 0.93\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0375, 'nn': 0.0512, 'logistic': 0.6828, 'lstm': 0.2285} (score: 0.097482)\n",
            "Training meta-model for timestep 0.935\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1283, 'nn': 0.0185, 'logistic': 0.7393, 'lstm': 0.114} (score: 0.096972)\n",
            "Training meta-model for timestep 0.94\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1044, 'nn': 0.2292, 'logistic': 0.5708, 'lstm': 0.0956} (score: 0.097311)\n",
            "Training meta-model for timestep 0.945\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.1723, 'nn': 0.0927, 'logistic': 0.1993, 'lstm': 0.5357} (score: 0.101003)\n",
            "Training meta-model for timestep 0.95\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.1964, 'logistic': 0.3813, 'lstm': 0.4223} (score: 0.099255)\n",
            "Training meta-model for timestep 0.955\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3885, 'nn': 0.1419, 'logistic': 0.3581, 'lstm': 0.1115} (score: 0.099193)\n",
            "Training meta-model for timestep 0.96\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.3187, 'nn': 0.4099, 'logistic': 0.0, 'lstm': 0.2713} (score: 0.105425)\n",
            "Training meta-model for timestep 0.965\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.0928, 'logistic': 0.9072, 'lstm': 0.0} (score: 0.105571)\n",
            "Training meta-model for timestep 0.97\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3207, 'logistic': 0.3884, 'lstm': 0.2909} (score: 0.091196)\n",
            "Training meta-model for timestep 0.975\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.2682, 'nn': 0.0, 'logistic': 0.5003, 'lstm': 0.2315} (score: 0.091876)\n",
            "Training meta-model for timestep 0.98\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.3652, 'logistic': 0.3123, 'lstm': 0.3225} (score: 0.094623)\n",
            "Training meta-model for timestep 0.985\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0, 'nn': 0.29, 'logistic': 0.4784, 'lstm': 0.2315} (score: 0.077851)\n",
            "Training meta-model for timestep 0.99\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0948, 'nn': 0.5728, 'logistic': 0.177, 'lstm': 0.1555} (score: 0.081792)\n",
            "Training meta-model for timestep 0.995\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.0633, 'nn': 0.479, 'logistic': 0.1565, 'lstm': 0.3011} (score: 0.080102)\n",
            "Training meta-model for timestep 1.0\n",
            "Training ensemble for this timestep...\n",
            "Optimizing ensemble weights for this timestep...\n",
            "  Optimized weights: {'xgboost': 0.5749, 'nn': 0.2094, 'logistic': 0.0, 'lstm': 0.2157} (score: 0.052229)\n"
          ]
        }
      ],
      "source": [
        "def setup_meta_models(ensemble_matrices, all_models, all_models_order, strategy='meta_model', meta_model=None):\n",
        "    models = {}\n",
        "    for timestep in ensemble_matrices:\n",
        "        print(f\"Training meta-model for timestep {timestep}\")\n",
        "        ensemble_matrix = ensemble_matrices[timestep]\n",
        "        x_train = ensemble_matrix[\"predictions\"]\n",
        "        y_train = ensemble_matrix[\"y_true\"]\n",
        "        all_models_for_timestep = {model_name: all_models[model_name][timestep] for model_name in all_models_order}\n",
        "        models[timestep] = EnsemblePredictor(all_models_for_timestep, all_models_order, strategy, meta_model)\n",
        "        models[timestep].train_ensemble(x_train, y_train, objective='brier')\n",
        "    return models\n",
        "\n",
        "ensemble_models = setup_meta_models(ensemble_matrices, all_models, all_models_order, strategy='weighted_average', meta_model=LogisticRegressionMetaModel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestep 0.00%: Accuracy = 0.7259, Brier Score = 0.2083\n",
            "Timestep 0.50%: Accuracy = 0.6643, Brier Score = 0.2163\n",
            "Timestep 1.00%: Accuracy = 0.6917, Brier Score = 0.2104\n",
            "Timestep 1.50%: Accuracy = 0.6765, Brier Score = 0.2139\n",
            "Timestep 2.00%: Accuracy = 0.6587, Brier Score = 0.2247\n",
            "Timestep 2.50%: Accuracy = 0.6736, Brier Score = 0.2153\n",
            "Timestep 3.00%: Accuracy = 0.6770, Brier Score = 0.2146\n",
            "Timestep 3.50%: Accuracy = 0.6549, Brier Score = 0.2206\n",
            "Timestep 4.00%: Accuracy = 0.6618, Brier Score = 0.2224\n",
            "Timestep 4.50%: Accuracy = 0.6837, Brier Score = 0.2107\n",
            "Timestep 5.00%: Accuracy = 0.7094, Brier Score = 0.2008\n",
            "Timestep 5.50%: Accuracy = 0.7106, Brier Score = 0.2235\n",
            "Timestep 6.00%: Accuracy = 0.7361, Brier Score = 0.2068\n",
            "Timestep 6.50%: Accuracy = 0.6851, Brier Score = 0.2196\n",
            "Timestep 7.00%: Accuracy = 0.6797, Brier Score = 0.2215\n",
            "Timestep 7.50%: Accuracy = 0.6920, Brier Score = 0.2164\n",
            "Timestep 8.00%: Accuracy = 0.7044, Brier Score = 0.2037\n",
            "Timestep 8.50%: Accuracy = 0.7368, Brier Score = 0.1920\n",
            "Timestep 9.00%: Accuracy = 0.6418, Brier Score = 0.2198\n",
            "Timestep 9.50%: Accuracy = 0.6703, Brier Score = 0.2099\n",
            "Timestep 10.00%: Accuracy = 0.6660, Brier Score = 0.2186\n",
            "Timestep 10.50%: Accuracy = 0.6646, Brier Score = 0.2127\n",
            "Timestep 11.00%: Accuracy = 0.7023, Brier Score = 0.2020\n",
            "Timestep 11.50%: Accuracy = 0.6965, Brier Score = 0.2091\n",
            "Timestep 12.00%: Accuracy = 0.7119, Brier Score = 0.2049\n",
            "Timestep 12.50%: Accuracy = 0.6444, Brier Score = 0.2204\n",
            "Timestep 13.00%: Accuracy = 0.7146, Brier Score = 0.1978\n",
            "Timestep 13.50%: Accuracy = 0.6419, Brier Score = 0.2268\n",
            "Timestep 14.00%: Accuracy = 0.6885, Brier Score = 0.1917\n",
            "Timestep 14.50%: Accuracy = 0.6740, Brier Score = 0.2002\n",
            "Timestep 15.00%: Accuracy = 0.6561, Brier Score = 0.2103\n",
            "Timestep 15.50%: Accuracy = 0.6813, Brier Score = 0.2010\n",
            "Timestep 16.00%: Accuracy = 0.6638, Brier Score = 0.2020\n",
            "Timestep 16.50%: Accuracy = 0.6614, Brier Score = 0.2139\n",
            "Timestep 17.00%: Accuracy = 0.6823, Brier Score = 0.1970\n",
            "Timestep 17.50%: Accuracy = 0.6318, Brier Score = 0.2153\n",
            "Timestep 18.00%: Accuracy = 0.6905, Brier Score = 0.2044\n",
            "Timestep 18.50%: Accuracy = 0.6771, Brier Score = 0.2045\n",
            "Timestep 19.00%: Accuracy = 0.6419, Brier Score = 0.2198\n",
            "Timestep 19.50%: Accuracy = 0.7007, Brier Score = 0.1971\n",
            "Timestep 20.00%: Accuracy = 0.6998, Brier Score = 0.2037\n",
            "Timestep 20.50%: Accuracy = 0.6723, Brier Score = 0.2081\n",
            "Timestep 21.00%: Accuracy = 0.6596, Brier Score = 0.2003\n",
            "Timestep 21.50%: Accuracy = 0.6329, Brier Score = 0.2153\n",
            "Timestep 22.00%: Accuracy = 0.6312, Brier Score = 0.2078\n",
            "Timestep 22.50%: Accuracy = 0.6687, Brier Score = 0.2111\n",
            "Timestep 23.00%: Accuracy = 0.6367, Brier Score = 0.2106\n",
            "Timestep 23.50%: Accuracy = 0.6514, Brier Score = 0.2078\n",
            "Timestep 24.00%: Accuracy = 0.6180, Brier Score = 0.2182\n",
            "Timestep 24.50%: Accuracy = 0.6329, Brier Score = 0.2040\n",
            "Timestep 25.00%: Accuracy = 0.6446, Brier Score = 0.2058\n",
            "Timestep 25.50%: Accuracy = 0.6330, Brier Score = 0.2211\n",
            "Timestep 26.00%: Accuracy = 0.6345, Brier Score = 0.2000\n",
            "Timestep 26.50%: Accuracy = 0.6199, Brier Score = 0.2175\n",
            "Timestep 27.00%: Accuracy = 0.7153, Brier Score = 0.1913\n",
            "Timestep 27.50%: Accuracy = 0.6402, Brier Score = 0.2066\n",
            "Timestep 28.00%: Accuracy = 0.6644, Brier Score = 0.1987\n",
            "Timestep 28.50%: Accuracy = 0.6287, Brier Score = 0.2072\n",
            "Timestep 29.00%: Accuracy = 0.6681, Brier Score = 0.1901\n",
            "Timestep 29.50%: Accuracy = 0.6923, Brier Score = 0.1909\n",
            "Timestep 30.00%: Accuracy = 0.6722, Brier Score = 0.1967\n",
            "Timestep 30.50%: Accuracy = 0.6712, Brier Score = 0.1959\n",
            "Timestep 31.00%: Accuracy = 0.7079, Brier Score = 0.1926\n",
            "Timestep 31.50%: Accuracy = 0.6817, Brier Score = 0.1989\n",
            "Timestep 32.00%: Accuracy = 0.6673, Brier Score = 0.1938\n",
            "Timestep 32.50%: Accuracy = 0.6528, Brier Score = 0.2053\n",
            "Timestep 33.00%: Accuracy = 0.6971, Brier Score = 0.1917\n",
            "Timestep 33.50%: Accuracy = 0.6360, Brier Score = 0.1972\n",
            "Timestep 34.00%: Accuracy = 0.6807, Brier Score = 0.1942\n",
            "Timestep 34.50%: Accuracy = 0.6843, Brier Score = 0.1949\n",
            "Timestep 35.00%: Accuracy = 0.7008, Brier Score = 0.1761\n",
            "Timestep 35.50%: Accuracy = 0.6687, Brier Score = 0.1895\n",
            "Timestep 36.00%: Accuracy = 0.7565, Brier Score = 0.1643\n",
            "Timestep 36.50%: Accuracy = 0.7333, Brier Score = 0.1692\n",
            "Timestep 37.00%: Accuracy = 0.7198, Brier Score = 0.1746\n",
            "Timestep 37.50%: Accuracy = 0.7313, Brier Score = 0.1813\n",
            "Timestep 38.00%: Accuracy = 0.7520, Brier Score = 0.1622\n",
            "Timestep 38.50%: Accuracy = 0.7372, Brier Score = 0.1669\n",
            "Timestep 39.00%: Accuracy = 0.7505, Brier Score = 0.1626\n",
            "Timestep 39.50%: Accuracy = 0.7206, Brier Score = 0.1723\n",
            "Timestep 40.00%: Accuracy = 0.7449, Brier Score = 0.1677\n",
            "Timestep 40.50%: Accuracy = 0.7799, Brier Score = 0.1524\n",
            "Timestep 41.00%: Accuracy = 0.7262, Brier Score = 0.1773\n",
            "Timestep 41.50%: Accuracy = 0.7599, Brier Score = 0.1627\n",
            "Timestep 42.00%: Accuracy = 0.7424, Brier Score = 0.1731\n",
            "Timestep 42.50%: Accuracy = 0.7684, Brier Score = 0.1546\n",
            "Timestep 43.00%: Accuracy = 0.7572, Brier Score = 0.1644\n",
            "Timestep 43.50%: Accuracy = 0.7596, Brier Score = 0.1589\n",
            "Timestep 44.00%: Accuracy = 0.7562, Brier Score = 0.1622\n",
            "Timestep 44.50%: Accuracy = 0.7638, Brier Score = 0.1518\n",
            "Timestep 45.00%: Accuracy = 0.7984, Brier Score = 0.1526\n",
            "Timestep 45.50%: Accuracy = 0.7642, Brier Score = 0.1611\n",
            "Timestep 46.00%: Accuracy = 0.7587, Brier Score = 0.1635\n",
            "Timestep 46.50%: Accuracy = 0.7893, Brier Score = 0.1421\n",
            "Timestep 47.00%: Accuracy = 0.7586, Brier Score = 0.1507\n",
            "Timestep 47.50%: Accuracy = 0.7743, Brier Score = 0.1460\n",
            "Timestep 48.00%: Accuracy = 0.7839, Brier Score = 0.1482\n",
            "Timestep 48.50%: Accuracy = 0.7695, Brier Score = 0.1485\n",
            "Timestep 49.00%: Accuracy = 0.8000, Brier Score = 0.1397\n",
            "Timestep 49.50%: Accuracy = 0.7846, Brier Score = 0.1473\n",
            "Timestep 50.00%: Accuracy = 0.7708, Brier Score = 0.1508\n",
            "Timestep 50.50%: Accuracy = 0.7799, Brier Score = 0.1526\n",
            "Timestep 51.00%: Accuracy = 0.7619, Brier Score = 0.1556\n",
            "Timestep 51.50%: Accuracy = 0.7470, Brier Score = 0.1579\n",
            "Timestep 52.00%: Accuracy = 0.7903, Brier Score = 0.1357\n",
            "Timestep 52.50%: Accuracy = 0.7734, Brier Score = 0.1525\n",
            "Timestep 53.00%: Accuracy = 0.7417, Brier Score = 0.1616\n",
            "Timestep 53.50%: Accuracy = 0.7605, Brier Score = 0.1570\n",
            "Timestep 54.00%: Accuracy = 0.7426, Brier Score = 0.1569\n",
            "Timestep 54.50%: Accuracy = 0.7736, Brier Score = 0.1487\n",
            "Timestep 55.00%: Accuracy = 0.7799, Brier Score = 0.1428\n",
            "Timestep 55.50%: Accuracy = 0.7290, Brier Score = 0.1647\n",
            "Timestep 56.00%: Accuracy = 0.7577, Brier Score = 0.1559\n",
            "Timestep 56.50%: Accuracy = 0.7618, Brier Score = 0.1519\n",
            "Timestep 57.00%: Accuracy = 0.7244, Brier Score = 0.1662\n",
            "Timestep 57.50%: Accuracy = 0.7957, Brier Score = 0.1373\n",
            "Timestep 58.00%: Accuracy = 0.8099, Brier Score = 0.1367\n",
            "Timestep 58.50%: Accuracy = 0.7735, Brier Score = 0.1468\n",
            "Timestep 59.00%: Accuracy = 0.8230, Brier Score = 0.1339\n",
            "Timestep 59.50%: Accuracy = 0.7992, Brier Score = 0.1513\n",
            "Timestep 60.00%: Accuracy = 0.8224, Brier Score = 0.1282\n",
            "Timestep 60.50%: Accuracy = 0.7679, Brier Score = 0.1459\n",
            "Timestep 61.00%: Accuracy = 0.7960, Brier Score = 0.1449\n",
            "Timestep 61.50%: Accuracy = 0.7865, Brier Score = 0.1373\n",
            "Timestep 62.00%: Accuracy = 0.7886, Brier Score = 0.1495\n",
            "Timestep 62.50%: Accuracy = 0.8269, Brier Score = 0.1295\n",
            "Timestep 63.00%: Accuracy = 0.7812, Brier Score = 0.1392\n",
            "Timestep 63.50%: Accuracy = 0.8133, Brier Score = 0.1293\n",
            "Timestep 64.00%: Accuracy = 0.8259, Brier Score = 0.1228\n",
            "Timestep 64.50%: Accuracy = 0.7794, Brier Score = 0.1414\n",
            "Timestep 65.00%: Accuracy = 0.7874, Brier Score = 0.1402\n",
            "Timestep 65.50%: Accuracy = 0.8080, Brier Score = 0.1236\n",
            "Timestep 66.00%: Accuracy = 0.8323, Brier Score = 0.1261\n",
            "Timestep 66.50%: Accuracy = 0.7975, Brier Score = 0.1351\n",
            "Timestep 67.00%: Accuracy = 0.8373, Brier Score = 0.1138\n",
            "Timestep 67.50%: Accuracy = 0.8018, Brier Score = 0.1352\n",
            "Timestep 68.00%: Accuracy = 0.8377, Brier Score = 0.1148\n",
            "Timestep 68.50%: Accuracy = 0.7844, Brier Score = 0.1402\n",
            "Timestep 69.00%: Accuracy = 0.8180, Brier Score = 0.1213\n",
            "Timestep 69.50%: Accuracy = 0.7887, Brier Score = 0.1265\n",
            "Timestep 70.00%: Accuracy = 0.8315, Brier Score = 0.1188\n",
            "Timestep 70.50%: Accuracy = 0.8280, Brier Score = 0.1141\n",
            "Timestep 71.00%: Accuracy = 0.8330, Brier Score = 0.1079\n",
            "Timestep 71.50%: Accuracy = 0.8577, Brier Score = 0.1037\n",
            "Timestep 72.00%: Accuracy = 0.8059, Brier Score = 0.1228\n",
            "Timestep 72.50%: Accuracy = 0.8450, Brier Score = 0.1037\n",
            "Timestep 73.00%: Accuracy = 0.8568, Brier Score = 0.1118\n",
            "Timestep 73.50%: Accuracy = 0.8348, Brier Score = 0.1143\n",
            "Timestep 74.00%: Accuracy = 0.8326, Brier Score = 0.1118\n",
            "Timestep 74.50%: Accuracy = 0.8160, Brier Score = 0.1191\n",
            "Timestep 75.00%: Accuracy = 0.8327, Brier Score = 0.1082\n",
            "Timestep 75.50%: Accuracy = 0.8351, Brier Score = 0.1058\n",
            "Timestep 76.00%: Accuracy = 0.8497, Brier Score = 0.1062\n",
            "Timestep 76.50%: Accuracy = 0.8268, Brier Score = 0.1152\n",
            "Timestep 77.00%: Accuracy = 0.8662, Brier Score = 0.0971\n",
            "Timestep 77.50%: Accuracy = 0.8528, Brier Score = 0.1021\n",
            "Timestep 78.00%: Accuracy = 0.8399, Brier Score = 0.1115\n",
            "Timestep 78.50%: Accuracy = 0.8580, Brier Score = 0.1033\n",
            "Timestep 79.00%: Accuracy = 0.8418, Brier Score = 0.1128\n",
            "Timestep 79.50%: Accuracy = 0.8664, Brier Score = 0.1050\n",
            "Timestep 80.00%: Accuracy = 0.8439, Brier Score = 0.1011\n",
            "Timestep 80.50%: Accuracy = 0.8698, Brier Score = 0.1055\n",
            "Timestep 81.00%: Accuracy = 0.8609, Brier Score = 0.0962\n",
            "Timestep 81.50%: Accuracy = 0.8439, Brier Score = 0.1064\n",
            "Timestep 82.00%: Accuracy = 0.8702, Brier Score = 0.0881\n",
            "Timestep 82.50%: Accuracy = 0.8708, Brier Score = 0.0912\n",
            "Timestep 83.00%: Accuracy = 0.8597, Brier Score = 0.0990\n",
            "Timestep 83.50%: Accuracy = 0.8857, Brier Score = 0.0846\n",
            "Timestep 84.00%: Accuracy = 0.8606, Brier Score = 0.0952\n",
            "Timestep 84.50%: Accuracy = 0.8402, Brier Score = 0.1007\n",
            "Timestep 85.00%: Accuracy = 0.8726, Brier Score = 0.0826\n",
            "Timestep 85.50%: Accuracy = 0.8422, Brier Score = 0.1126\n",
            "Timestep 86.00%: Accuracy = 0.8788, Brier Score = 0.0812\n",
            "Timestep 86.50%: Accuracy = 0.8889, Brier Score = 0.0819\n",
            "Timestep 87.00%: Accuracy = 0.8976, Brier Score = 0.0825\n",
            "Timestep 87.50%: Accuracy = 0.8832, Brier Score = 0.0860\n",
            "Timestep 88.00%: Accuracy = 0.8915, Brier Score = 0.0880\n",
            "Timestep 88.50%: Accuracy = 0.8875, Brier Score = 0.0846\n",
            "Timestep 89.00%: Accuracy = 0.8990, Brier Score = 0.0792\n",
            "Timestep 89.50%: Accuracy = 0.9020, Brier Score = 0.0889\n",
            "Timestep 90.00%: Accuracy = 0.8931, Brier Score = 0.0807\n",
            "Timestep 90.50%: Accuracy = 0.8731, Brier Score = 0.0896\n",
            "Timestep 91.00%: Accuracy = 0.8773, Brier Score = 0.0853\n",
            "Timestep 91.50%: Accuracy = 0.9089, Brier Score = 0.0735\n",
            "Timestep 92.00%: Accuracy = 0.8905, Brier Score = 0.0772\n",
            "Timestep 92.50%: Accuracy = 0.8878, Brier Score = 0.0777\n",
            "Timestep 93.00%: Accuracy = 0.8667, Brier Score = 0.0880\n",
            "Timestep 93.50%: Accuracy = 0.8777, Brier Score = 0.0833\n",
            "Timestep 94.00%: Accuracy = 0.8770, Brier Score = 0.0821\n",
            "Timestep 94.50%: Accuracy = 0.8817, Brier Score = 0.0821\n",
            "Timestep 95.00%: Accuracy = 0.8917, Brier Score = 0.0836\n",
            "Timestep 95.50%: Accuracy = 0.8819, Brier Score = 0.0838\n",
            "Timestep 96.00%: Accuracy = 0.8777, Brier Score = 0.0801\n",
            "Timestep 96.50%: Accuracy = 0.9123, Brier Score = 0.0675\n",
            "Timestep 97.00%: Accuracy = 0.9093, Brier Score = 0.0672\n",
            "Timestep 97.50%: Accuracy = 0.8791, Brier Score = 0.0844\n",
            "Timestep 98.00%: Accuracy = 0.9028, Brier Score = 0.0710\n",
            "Timestep 98.50%: Accuracy = 0.9075, Brier Score = 0.0669\n",
            "Timestep 99.00%: Accuracy = 0.9215, Brier Score = 0.0612\n",
            "Timestep 99.50%: Accuracy = 0.8717, Brier Score = 0.0882\n",
            "Timestep 100.00%: Accuracy = 0.8898, Brier Score = 0.0766\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary Statistics:\n",
            "Average Accuracy: 0.7664  0.0844\n",
            "Average Brier Score: 0.1527  0.0480\n",
            "Best Accuracy: 0.9215 at timestep 99.00%\n",
            "Best Brier Score: 0.0612 at timestep 99.00%\n"
          ]
        }
      ],
      "source": [
        "# Test accuracy and Brier score of model for each timestep on test data and plot\n",
        "accuracies = []\n",
        "brier_scores = []\n",
        "timesteps = []\n",
        "def brier_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "for timestep, i in zip(ensemble_models, test_data.keys()):\n",
        "    model = ensemble_models[timestep]\n",
        "    # Convert test data to array\n",
        "    y_test = np.array([row[\"label\"] for row in test_data_seq[i]])\n",
        "    X_test = np.array([row[\"rows\"] for row in test_data_seq[i]])\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    \n",
        "    # Calculate Brier score\n",
        "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probability predictions\n",
        "    brier_score = brier_loss(y_test, y_test_pred_proba)\n",
        "    \n",
        "    print(f\"Timestep {timestep:.2%}: Accuracy = {accuracy:.4f}, Brier Score = {brier_score:.4f}\")\n",
        "    accuracies.append(accuracy)\n",
        "    brier_scores.append(brier_score)\n",
        "    timesteps.append(timestep)\n",
        "\n",
        "# Create subplots for both metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(timesteps, accuracies, 'b-', linewidth=2, marker='o', markersize=3)\n",
        "ax1.set_xlabel(\"Timestep\")\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_title(\"Test Accuracy vs Timestep\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([min(accuracies) * 0.95, max(accuracies) * 1.05])\n",
        "\n",
        "# Plot Brier score\n",
        "ax2.plot(timesteps, brier_scores, 'r-', linewidth=2, marker='s', markersize=3)\n",
        "ax2.set_xlabel(\"Timestep\")\n",
        "ax2.set_ylabel(\"Brier Score\")\n",
        "ax2.set_title(\"Test Brier Score vs Timestep\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim([min(brier_scores) * 0.95, max(brier_scores) * 1.05])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.4f}  {np.std(accuracies):.4f}\")\n",
        "print(f\"Average Brier Score: {np.mean(brier_scores):.4f}  {np.std(brier_scores):.4f}\")\n",
        "print(f\"Best Accuracy: {max(accuracies):.4f} at timestep {timesteps[np.argmax(accuracies)]:.2%}\")\n",
        "print(f\"Best Brier Score: {min(brier_scores):.4f} at timestep {timesteps[np.argmin(brier_scores)]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for 2024\n",
            "Processed file:  game_401671629.csv\n",
            "Processed file:  game_401671601.csv\n",
            "Processed file:  game_401671826.csv\n",
            "Processed file:  game_401671832.csv\n",
            "Processed file:  game_401671749.csv\n",
            "Processed file:  game_401671775.csv\n",
            "Processed file:  game_401671761.csv\n",
            "Processed file:  game_401671760.csv\n",
            "Processed file:  game_401671774.csv\n",
            "Processed file:  game_401671748.csv\n",
            "Processed file:  game_401671833.csv\n",
            "Processed file:  game_401671827.csv\n",
            "Processed file:  game_401671600.csv\n",
            "Processed file:  game_401671628.csv\n",
            "Processed file:  game_401671616.csv\n",
            "Processed file:  game_401671831.csv\n",
            "Processed file:  game_401671825.csv\n",
            "Processed file:  game_401671819.csv\n",
            "Processed file:  game_401671762.csv\n",
            "Processed file:  game_401671776.csv\n",
            "Processed file:  game_401671789.csv\n",
            "Processed file:  game_401671788.csv\n",
            "Processed file:  game_401671777.csv\n",
            "Processed file:  game_401671763.csv\n",
            "Processed file:  game_401671818.csv\n",
            "Processed file:  game_401671824.csv\n",
            "Processed file:  game_401671830.csv\n",
            "Processed file:  game_401671617.csv\n",
            "Processed file:  game_401671808.csv\n",
            "Processed file:  game_401671834.csv\n",
            "Processed file:  game_401671820.csv\n",
            "Processed file:  game_401671767.csv\n",
            "Processed file:  game_401671773.csv\n",
            "Processed file:  game_401671798.csv\n",
            "Processed file:  game_401671799.csv\n",
            "Processed file:  game_401671772.csv\n",
            "Processed file:  game_401671766.csv\n",
            "Processed file:  game_401671821.csv\n",
            "Processed file:  game_401671835.csv\n",
            "Processed file:  game_401671809.csv\n",
            "Processed file:  game_401671638.csv\n",
            "Processed file:  game_401671823.csv\n",
            "Processed file:  game_401671837.csv\n",
            "Processed file:  game_401671599.csv\n",
            "Processed file:  game_401671770.csv\n",
            "Processed file:  game_401671764.csv\n",
            "Processed file:  game_401671758.csv\n",
            "Processed file:  game_401671759.csv\n",
            "Processed file:  game_401671765.csv\n",
            "Processed file:  game_401671771.csv\n",
            "Processed file:  game_401671836.csv\n",
            "Processed file:  game_401671822.csv\n",
            "Processed file:  game_401671639.csv\n",
            "Processed file:  game_401671662.csv\n",
            "Processed file:  game_401671676.csv\n",
            "Processed file:  game_401671845.csv\n",
            "Processed file:  game_401671851.csv\n",
            "Processed file:  game_401671689.csv\n",
            "Processed file:  game_401671716.csv\n",
            "Processed file:  game_401671702.csv\n",
            "Processed file:  game_401671703.csv\n",
            "Processed file:  game_401671717.csv\n",
            "Processed file:  game_401671688.csv\n",
            "Processed file:  game_401671850.csv\n",
            "Processed file:  game_401671844.csv\n",
            "Processed file:  game_401671677.csv\n",
            "Processed file:  game_401671663.csv\n",
            "Processed file:  game_401671649.csv\n",
            "Processed file:  game_401671675.csv\n",
            "Processed file:  game_401671661.csv\n",
            "Processed file:  game_401671852.csv\n",
            "Processed file:  game_401671846.csv\n",
            "Processed file:  game_401671729.csv\n",
            "Processed file:  game_401671701.csv\n",
            "Processed file:  game_401671715.csv\n",
            "Processed file:  game_401671714.csv\n",
            "Processed file:  game_401671700.csv\n",
            "Processed file:  game_401671728.csv\n",
            "Processed file:  game_401671489.csv\n",
            "Processed file:  game_401671847.csv\n",
            "Processed file:  game_401671853.csv\n",
            "Processed file:  game_401671660.csv\n",
            "Processed file:  game_401671674.csv\n",
            "Processed file:  game_401671648.csv\n",
            "Processed file:  game_401671670.csv\n",
            "Processed file:  game_401671664.csv\n",
            "Processed file:  game_401671658.csv\n",
            "Processed file:  game_401671857.csv\n",
            "Processed file:  game_401671843.csv\n",
            "Processed file:  game_401671704.csv\n",
            "Processed file:  game_401671710.csv\n",
            "Processed file:  game_401671738.csv\n",
            "Processed file:  game_401671739.csv\n",
            "Processed file:  game_401671711.csv\n",
            "Processed file:  game_401671705.csv\n",
            "Processed file:  game_401671842.csv\n",
            "Processed file:  game_401671856.csv\n",
            "Processed file:  game_401671659.csv\n",
            "Processed file:  game_401671665.csv\n",
            "Processed file:  game_401671671.csv\n",
            "Processed file:  game_401671667.csv\n",
            "Processed file:  game_401671673.csv\n",
            "Processed file:  game_401671868.csv\n",
            "Processed file:  game_401671840.csv\n",
            "Processed file:  game_401671698.csv\n",
            "Processed file:  game_401671854.csv\n",
            "Processed file:  game_401671713.csv\n",
            "Processed file:  game_401671707.csv\n",
            "Processed file:  game_401671706.csv\n",
            "Processed file:  game_401671712.csv\n",
            "Processed file:  game_401671855.csv\n",
            "Processed file:  game_401671699.csv\n",
            "Processed file:  game_401671841.csv\n",
            "Processed file:  game_401671869.csv\n",
            "Processed file:  game_401671672.csv\n",
            "Processed file:  game_401671666.csv\n",
            "Processed file:  game_401671643.csv\n",
            "Processed file:  game_401671657.csv\n",
            "Processed file:  game_401671864.csv\n",
            "Processed file:  game_401671870.csv\n",
            "Processed file:  game_401671858.csv\n",
            "Processed file:  game_401671680.csv\n",
            "Processed file:  game_401671694.csv\n",
            "Processed file:  game_401671737.csv\n",
            "Processed file:  game_401671723.csv\n",
            "Processed file:  game_401671722.csv\n",
            "Processed file:  game_401671736.csv\n",
            "Processed file:  game_401671695.csv\n",
            "Processed file:  game_401671681.csv\n",
            "Processed file:  game_401671859.csv\n",
            "Processed file:  game_401671871.csv\n",
            "Processed file:  game_401671865.csv\n",
            "Processed file:  game_401671656.csv\n",
            "Processed file:  game_401671642.csv\n",
            "Processed file:  game_401671668.csv\n",
            "Processed file:  game_401671654.csv\n",
            "Processed file:  game_401671640.csv\n",
            "Processed file:  game_401671873.csv\n",
            "Processed file:  game_401671867.csv\n",
            "Processed file:  game_401671697.csv\n",
            "Processed file:  game_401671683.csv\n",
            "Processed file:  game_401671495.csv\n",
            "Processed file:  game_401671708.csv\n",
            "Processed file:  game_401671720.csv\n",
            "Processed file:  game_401671734.csv\n",
            "Processed file:  game_401671735.csv\n",
            "Processed file:  game_401671721.csv\n",
            "Processed file:  game_401671709.csv\n",
            "Processed file:  game_401671494.csv\n",
            "Processed file:  game_401671682.csv\n",
            "Processed file:  game_401671696.csv\n",
            "Processed file:  game_401671866.csv\n",
            "Processed file:  game_401671872.csv\n",
            "Processed file:  game_401671641.csv\n",
            "Processed file:  game_401671655.csv\n",
            "Processed file:  game_401671669.csv\n",
            "Processed file:  game_401671651.csv\n",
            "Processed file:  game_401671645.csv\n",
            "Processed file:  game_401671679.csv\n",
            "Processed file:  game_401671692.csv\n",
            "Processed file:  game_401671686.csv\n",
            "Processed file:  game_401671876.csv\n",
            "Processed file:  game_401671862.csv\n",
            "Processed file:  game_401671490.csv\n",
            "Processed file:  game_401671725.csv\n",
            "Processed file:  game_401671731.csv\n",
            "Processed file:  game_401671719.csv\n",
            "Processed file:  game_401671718.csv\n",
            "Processed file:  game_401671730.csv\n",
            "Processed file:  game_401671724.csv\n",
            "Processed file:  game_401671491.csv\n",
            "Processed file:  game_401671863.csv\n",
            "Processed file:  game_401671877.csv\n",
            "Processed file:  game_401671687.csv\n",
            "Processed file:  game_401671693.csv\n",
            "Processed file:  game_401671678.csv\n",
            "Processed file:  game_401671644.csv\n",
            "Processed file:  game_401671650.csv\n",
            "Processed file:  game_401671646.csv\n",
            "Processed file:  game_401671652.csv\n",
            "Processed file:  game_401671685.csv\n",
            "Processed file:  game_401671849.csv\n",
            "Processed file:  game_401671691.csv\n",
            "Processed file:  game_401671861.csv\n",
            "Processed file:  game_401671875.csv\n",
            "Processed file:  game_401671493.csv\n",
            "Processed file:  game_401671732.csv\n",
            "Processed file:  game_401671726.csv\n",
            "Processed file:  game_401671727.csv\n",
            "Processed file:  game_401671733.csv\n",
            "Processed file:  game_401671492.csv\n",
            "Processed file:  game_401671874.csv\n",
            "Processed file:  game_401671860.csv\n",
            "Processed file:  game_401671690.csv\n",
            "Processed file:  game_401671848.csv\n",
            "Processed file:  game_401671684.csv\n",
            "Processed file:  game_401671653.csv\n",
            "Processed file:  game_401671647.csv\n",
            "Processed file:  game_401671620.csv\n",
            "Processed file:  game_401671634.csv\n",
            "Processed file:  game_401671807.csv\n",
            "Processed file:  game_401671813.csv\n",
            "Processed file:  game_401671768.csv\n",
            "Processed file:  game_401671754.csv\n",
            "Processed file:  game_401671740.csv\n",
            "Processed file:  game_401671797.csv\n",
            "Processed file:  game_401671783.csv\n",
            "Processed file:  game_401671782.csv\n",
            "Processed file:  game_401671796.csv\n",
            "Processed file:  game_401671741.csv\n",
            "Processed file:  game_401671755.csv\n",
            "Processed file:  game_401671769.csv\n",
            "Processed file:  game_401671812.csv\n",
            "Processed file:  game_401671806.csv\n",
            "Processed file:  game_401671635.csv\n",
            "Processed file:  game_401671621.csv\n",
            "Processed file:  game_401671637.csv\n",
            "Processed file:  game_401671623.csv\n",
            "Processed file:  game_401671810.csv\n",
            "Processed file:  game_401671804.csv\n",
            "Processed file:  game_401671838.csv\n",
            "Processed file:  game_401671743.csv\n",
            "Processed file:  game_401671757.csv\n",
            "Processed file:  game_401671780.csv\n",
            "Processed file:  game_401671794.csv\n",
            "Processed file:  game_401671795.csv\n",
            "Processed file:  game_401671781.csv\n",
            "Processed file:  game_401671756.csv\n",
            "Processed file:  game_401671742.csv\n",
            "Processed file:  game_401671839.csv\n",
            "Processed file:  game_401671805.csv\n",
            "Processed file:  game_401671811.csv\n",
            "Processed file:  game_401671622.csv\n",
            "Processed file:  game_401671636.csv\n",
            "Processed file:  game_401671632.csv\n",
            "Processed file:  game_401671626.csv\n",
            "Processed file:  game_401671829.csv\n",
            "Processed file:  game_401671815.csv\n",
            "Processed file:  game_401671801.csv\n",
            "Processed file:  game_401671746.csv\n",
            "Processed file:  game_401671752.csv\n",
            "Processed file:  game_401671785.csv\n",
            "Processed file:  game_401671791.csv\n",
            "Processed file:  game_401671790.csv\n",
            "Processed file:  game_401671784.csv\n",
            "Processed file:  game_401671753.csv\n",
            "Processed file:  game_401671747.csv\n",
            "Processed file:  game_401671800.csv\n",
            "Processed file:  game_401671814.csv\n",
            "Processed file:  game_401671828.csv\n",
            "Processed file:  game_401671627.csv\n",
            "Processed file:  game_401671633.csv\n",
            "Processed file:  game_401671625.csv\n",
            "Processed file:  game_401671631.csv\n",
            "Processed file:  game_401671619.csv\n",
            "Processed file:  game_401671802.csv\n",
            "Processed file:  game_401671816.csv\n",
            "Processed file:  game_401671751.csv\n",
            "Processed file:  game_401671745.csv\n",
            "Processed file:  game_401671779.csv\n",
            "Processed file:  game_401671792.csv\n",
            "Processed file:  game_401671786.csv\n",
            "Processed file:  game_401671787.csv\n",
            "Processed file:  game_401671793.csv\n",
            "Processed file:  game_401671778.csv\n",
            "Processed file:  game_401671744.csv\n",
            "Processed file:  game_401671750.csv\n",
            "Processed file:  game_401671817.csv\n",
            "Processed file:  game_401671803.csv\n",
            "Processed file:  game_401671618.csv\n",
            "Processed file:  game_401671630.csv\n",
            "Processed file:  game_401671624.csv\n"
          ]
        }
      ],
      "source": [
        "# Write predictions to csv file\n",
        "from process_data import write_predictions\n",
        "write_predictions(ensemble_models, interpolated_dir, [2024], 4, features, replace_nan_val = 0, phat_b = \"ensemble_phat_b_model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NFL_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
